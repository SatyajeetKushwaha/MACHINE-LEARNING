{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "                                ### Theoretical Questions\n",
        "\n",
        "#### 1. What is unsupervised learning in the context of machine learning?\n",
        "[cite_start]Unsupervised learning is a type of machine learning where the algorithm learns from a dataset without any pre-labeled responses or outcomes[cite: 9]. [cite_start]The goal is to discover hidden patterns, structures, or relationships within the data[cite: 9]. Unlike supervised learning, there's no \"correct\" answer to guide the algorithm.\n",
        "#### 2. How does K-Means clustering algorithm work?\n",
        "[cite_start]The K-Means algorithm partitions data into K distinct, non-overlapping subgroups or clusters[cite: 10]. It works by:\n",
        "1.  Randomly initializing K cluster centroids.\n",
        "2.  Iteratively assigning each data point to the nearest centroid.\n",
        "3.  Recalculating the centroids based on the mean of all points assigned to that cluster.\n",
        "4.  [cite_start]Repeating steps 2 and 3 until the cluster assignments no longer change or a maximum number of iterations is reached[cite: 10].\n",
        "\n",
        "#### 3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "[cite_start]A dendrogram is a tree-like diagram used to visualize the results of hierarchical clustering[cite: 11]. [cite_start]It shows the sequence of merges or splits of clusters and the distance at which they occurred[cite: 11]. The vertical axis represents the distance between clusters, while the horizontal axis represents the data points. [cite_start]By cutting the dendrogram at a specific height, you can determine the clusters[cite: 11].\n",
        "\n",
        "#### 4. What is the main difference between K-Means and Hierarchical Clustering?\n",
        "[cite_start]The main difference is their approach to clustering[cite: 12]. [cite_start]K-Means is a **partitional** algorithm that requires the number of clusters (K) to be specified beforehand, and it aims to partition data into K groups[cite: 12]. [cite_start]Hierarchical clustering, on the other hand, is an **agglomerative** or **divisive** algorithm that creates a hierarchy of clusters, which can be visualized in a dendrogram, and does not require the number of clusters to be pre-specified[cite: 12].\n",
        "\n",
        "#### 5. What are the advantages of DBSCAN over K-Means?\n",
        "[cite_start]DBSCAN (Density-Based Spatial Clustering of Applications with Noise) has several advantages over K-Means[cite: 13]:\n",
        "* It does not require specifying the number of clusters beforehand.\n",
        "* It can discover clusters of arbitrary shapes.\n",
        "* [cite_start]It is robust to outliers, as it can identify and label them as \"noise\"[cite: 13].\n",
        "\n",
        "#### 6. When would you use Silhouette Score in clustering?\n",
        "[cite_start]You would use the Silhouette Score to evaluate the quality of a clustering result[cite: 14]. [cite_start]It measures how well each data point fits within its assigned cluster and how poorly it fits into neighboring clusters[cite: 14]. A higher Silhouette Score indicates a better-defined and more separated clustering. [cite_start]It is particularly useful for comparing the performance of clustering with different numbers of clusters[cite: 14].\n",
        "\n",
        "#### 7. What are the limitations of Hierarchical Clustering?\n",
        "[cite_start]Some limitations of hierarchical clustering include[cite: 15]:\n",
        "* It can be computationally expensive, especially for large datasets, with a time complexity often around $O(n^3)$ or $O(n^2)$ depending on the linkage criteria.\n",
        "* It is sensitive to the choice of distance metric and linkage criteria.\n",
        "* [cite_start]It doesn't provide a single set of clusters but a hierarchy, which can make it difficult to determine the \"correct\" number of clusters without a dendrogram[cite: 15].\n",
        "\n",
        "#### 8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "[cite_start]Feature scaling is important because K-Means and other distance-based clustering algorithms rely on calculating distances between data points[cite: 16]. [cite_start]If features have different scales or units, those with a larger range will disproportionately influence the distance calculation, potentially leading to inaccurate clustering[cite: 16]. [cite_start]Scaling ensures all features contribute equally to the distance measurements[cite: 16].\n",
        "\n",
        "#### 9. How does DBSCAN identify noise points?\n",
        "[cite_start]DBSCAN identifies noise points as data points that are not part of any cluster[cite: 17]. [cite_start]A point is considered noise if it's neither a core point nor a border point[cite: 17]. [cite_start]A core point has a minimum number of other points (MinPts) within its neighborhood (epsilon distance), while a border point is within the epsilon distance of a core point but does not have enough points in its own neighborhood to be a core point[cite: 17].\n",
        "#### 10. Define inertia in the context of K-Means.\n",
        "[cite_start]Inertia is the sum of squared distances of samples to their closest cluster center[cite: 18]. [cite_start]It measures the compactness of the clusters[cite: 18]. [cite_start]The lower the inertia, the better the clusters are considered to be, as the data points are closer to their respective centroids[cite: 18].\n",
        "\n",
        "#### 11. What is the elbow method in K-Means clustering?\n",
        "[cite_start]The elbow method is a heuristic used to find the optimal number of clusters (K) for K-Means[cite: 19]. [cite_start]It involves plotting the inertia (sum of squared distances) against different values of K[cite: 19]. [cite_start]The plot often resembles an arm, and the \"elbow\" is the point where the rate of decrease in inertia slows down significantly[cite: 19]. This point is considered a good candidate for the optimal K.\n",
        "\n",
        "#### 12. Describe the concept of \"density\" in DBSCAN.\n",
        "[cite_start]In DBSCAN, density refers to the concentration of data points in a given region[cite: 20]. [cite_start]The algorithm defines a cluster as a high-density region separated from other high-density regions by low-density regions[cite: 20]. [cite_start]This is determined by two main parameters: epsilon (the radius of a neighborhood) and MinPts (the minimum number of points required to form a high-density region)[cite: 20].\n",
        "\n",
        "#### 13. Can hierarchical clustering be used on categorical data?\n",
        "[cite_start]Yes, hierarchical clustering can be used on categorical data, but it requires using a specific dissimilarity metric suitable for categorical variables, such as the Jaccard distance or the Hamming distance[cite: 21]. [cite_start]You cannot use standard Euclidean distance, which is designed for numerical data[cite: 21].\n",
        "\n",
        "#### 14. What does a negative Silhouette Score indicate?\n",
        "[cite_start]A negative Silhouette Score for a data point indicates that the point might have been assigned to the wrong cluster[cite: 22]. [cite_start]Specifically, it means the average distance of that point to all other points in its own cluster is greater than its average distance to points in a neighboring cluster[cite: 22].\n",
        "\n",
        "#### 15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "[cite_start]Linkage criteria define the distance between two clusters in hierarchical clustering[cite: 23]. [cite_start]It determines which clusters to merge at each step of the agglomerative process[cite: 23]. Common linkage criteria include:\n",
        "* **Single linkage**: The distance between the closest points in the two clusters.\n",
        "* **Complete linkage**: The distance between the farthest points in the two clusters.\n",
        "* **Average linkage**: The average distance between all points in the two clusters.\n",
        "\n",
        "#### 16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "[cite_start]K-Means assumes that clusters are spherical and of similar size and density[cite: 24]. [cite_start]Since it assigns points based on proximity to the centroid, it can struggle with clusters that are not compact or have different densities[cite: 24]. [cite_start]For example, a large, low-density cluster might be split into multiple smaller clusters, while a small, dense cluster might be swallowed by a larger one[cite: 24].\n",
        "\n",
        "#### 17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "[cite_start]The core parameters in DBSCAN are[cite: 25]:\n",
        "* [cite_start]**Epsilon ($\\epsilon$)**: The maximum distance between two samples for one to be considered in the neighborhood of the other[cite: 25]. A larger $\\epsilon$ can lead to larger clusters and fewer noise points.\n",
        "* [cite_start]**MinPts**: The minimum number of points required to form a dense region or a cluster[cite: 25]. A higher MinPts makes the algorithm more strict in defining a cluster, leading to fewer but denser clusters and more noise points.\n",
        "\n",
        "#### 18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "[cite_start]K-Means++ improves upon standard K-Means by selecting the initial cluster centroids in a smarter way[cite: 26]. [cite_start]Instead of random initialization, K-Means++ selects the first centroid randomly, and then each subsequent centroid is chosen from the remaining data points with a probability proportional to its squared distance from the nearest existing centroid[cite: 26]. [cite_start]This ensures that the initial centroids are well-separated, leading to faster convergence and better final clusterings[cite: 26].\n",
        "\n",
        "#### 19. What is agglomerative clustering?\n",
        "[cite_start]Agglomerative clustering is a \"bottom-up\" approach to hierarchical clustering[cite: 27]. [cite_start]It starts with each data point as its own individual cluster[cite: 27]. [cite_start]Then, it iteratively merges the two closest clusters at each step until only one cluster remains or a stopping criterion is met[cite: 27].\n",
        "\n",
        "#### 20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "[cite_start]While inertia measures the compactness of clusters, it has a significant limitation: it always decreases as the number of clusters (K) increases[cite: 28]. [cite_start]This makes it difficult to use inertia alone to find the optimal number of clusters[cite: 28]. [cite_start]The Silhouette Score, on the other hand, considers both the compactness of clusters and their separation from each other[cite: 28]. [cite_start]It provides a more balanced evaluation of clustering quality and is therefore a better metric for comparing models with different numbers of clusters[cite: 28].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                          ### Practical Questions\n",
        "\n",
        "\n",
        "#### **21. Generate synthetic data with 4 centers using `make_blobs` and apply K-Means clustering. Visualize using a scatter plot.**\n",
        "* **Step 1: Import necessary libraries.** You'll need `numpy`, `matplotlib.pyplot`, `sklearn.datasets.make_blobs`, and `sklearn.cluster.KMeans`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` with `n_samples` and `centers=4`.\n",
        "* **Step 3: Apply K-Means.** Initialize `KMeans(n_clusters=4)` and fit it to the data.\n",
        "* **Step 4: Visualize.** Create a scatter plot of the generated data. Use the cluster labels from K-Means to color the points and visualize the four distinct clusters.\n",
        "\n",
        "#### **22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_iris` and `sklearn.cluster.AgglomerativeClustering`.\n",
        "* **Step 2: Load data.** Use `load_iris()` to get the dataset.\n",
        "* **Step 3: Apply Agglomerative Clustering.** Initialize `AgglomerativeClustering(n_clusters=3)` and fit it to the data.\n",
        "* **Step 4: Display results.** Print the first 10 predicted cluster labels.\n",
        "\n",
        "#### **23. Generate synthetic data using `make_moons` and apply DBSCAN. Highlight outliers in the plot.**\n",
        "* **Step 1: Import libraries.** Import `matplotlib.pyplot`, `sklearn.datasets.make_moons`, and `sklearn.cluster.DBSCAN`.\n",
        "* **Step 2: Generate data.** Use `make_moons` to create the crescent-shaped data.\n",
        "* **Step 3: Apply DBSCAN.** Initialize and fit `DBSCAN` to the data.\n",
        "* **Step 4: Visualize.** Create a scatter plot. Use the DBSCAN labels to color the clusters. Points with a label of -1 are outliers and should be highlighted differently (e.g., in a black color).\n",
        "\n",
        "#### **24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_wine`, `sklearn.preprocessing.StandardScaler`, and `sklearn.cluster.KMeans`.\n",
        "* **Step 2: Load and scale data.** Load the Wine dataset and then use `StandardScaler` to scale the features.\n",
        "* **Step 3: Apply K-Means.** Initialize `KMeans` with a suitable number of clusters and fit it to the scaled data.\n",
        "* **Step 4: Print cluster sizes.** Count the number of samples belonging to each cluster using the `np.bincount` function on the predicted labels and print the counts.\n",
        "\n",
        "#### **25. Use `make_circles` to generate synthetic data and cluster it using DBSCAN. Plot the result.**\n",
        "* **Step 1: Import libraries.** Import `matplotlib.pyplot`, `sklearn.datasets.make_circles`, and `sklearn.cluster.DBSCAN`.\n",
        "* **Step 2: Generate data.** Use `make_circles` to create two concentric circles.\n",
        "* **Step 3: Apply DBSCAN.** Fit `DBSCAN` to the data. DBSCAN is ideal for this shape, unlike K-Means.\n",
        "* **Step 4: Plot.** Create a scatter plot, coloring points according to their cluster label.\n",
        "\n",
        "#### **26. Load the Breast Cancer dataset, apply `MinMaxScaler`, and use K-Means with 2 clusters. Output the cluster centroids.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_breast_cancer`, `sklearn.preprocessing.MinMaxScaler`, and `sklearn.cluster.KMeans`.\n",
        "* **Step 2: Load and scale data.** Load the dataset and then use `MinMaxScaler` to scale the features.\n",
        "* **Step 3: Apply K-Means.** Initialize `KMeans(n_clusters=2)` and fit it to the scaled data.\n",
        "* **Step 4: Output centroids.** Print the `cluster_centers_` attribute of the fitted K-Means model.\n",
        "\n",
        "#### **27. Generate synthetic data using `make_blobs` with varying cluster standard deviations and cluster with DBSCAN.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs` and `sklearn.cluster.DBSCAN`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` with the `cluster_std` parameter set to a list of different values (e.g., `[0.5, 1.5, 0.75]`).\n",
        "* **Step 3: Apply DBSCAN.** Fit `DBSCAN` to the data. Note that DBSCAN may not perform well on data with highly varying densities without tuning parameters.\n",
        "\n",
        "#### **28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_digits`, `sklearn.decomposition.PCA`, `sklearn.cluster.KMeans`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load and reduce data.** Load the digits dataset and use `PCA(n_components=2)` to reduce its dimensionality.\n",
        "* **Step 3: Apply K-Means.** Fit `KMeans(n_clusters=10)` (since there are 10 digits) to the reduced 2D data.\n",
        "* **Step 4: Visualize.** Create a scatter plot of the 2D data, coloring the points based on the K-Means cluster labels.\n",
        "\n",
        "#### **29. Create synthetic data using `make_blobs` and evaluate silhouette scores for $k=2$ to 5. Display as a bar chart.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs`, `sklearn.cluster.KMeans`, `sklearn.metrics.silhouette_score`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` to create synthetic data.\n",
        "* **Step 3: Calculate silhouette scores.** Loop through K from 2 to 5. Inside the loop, fit `KMeans(n_clusters=k)` and calculate the `silhouette_score`. Store the scores.\n",
        "* **Step 4: Create a bar chart.** Use `matplotlib.pyplot.bar` to plot the scores against the K values.\n",
        "\n",
        "#### **30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_iris` and `scipy.cluster.hierarchy`.\n",
        "* **Step 2: Load data.** Load the Iris dataset.\n",
        "* **Step 3: Perform linkage.** Use `scipy.cluster.hierarchy.linkage` with `method='average'` on the dataset.\n",
        "* **Step 4: Plot dendrogram.** Use `scipy.cluster.hierarchy.dendrogram` on the linkage result.\n",
        "\n",
        "#### **31. Generate synthetic data with overlapping clusters using `make_blobs`, then apply K-Means and visualize with decision boundaries.**\n",
        "* **Step 1: Import libraries.** Import `numpy`, `matplotlib.pyplot`, `sklearn.datasets.make_blobs`, and `sklearn.cluster.KMeans`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` with a high `cluster_std` to create overlapping clusters.\n",
        "* **Step 3: Apply K-Means.** Fit `KMeans` to the data.\n",
        "* **Step 4: Visualize.** Plot the data points. To visualize decision boundaries, create a grid of points, predict the cluster for each point on the grid, and then use `plt.contourf` to color the background according to the predicted cluster.\n",
        "\n",
        "#### **32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_digits`, `sklearn.manifold.TSNE`, `sklearn.cluster.DBSCAN`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load and reduce data.** Load the digits dataset and use `TSNE(n_components=2)` to reduce its dimensionality.\n",
        "* **Step 3: Apply DBSCAN.** Fit `DBSCAN` to the 2D data.\n",
        "* **Step 4: Visualize.** Create a scatter plot of the 2D data, coloring the points based on the DBSCAN cluster labels.\n",
        "\n",
        "#### **33. Generate synthetic data using `make_blobs` and apply Agglomerative Clustering with complete linkage. Plot the result.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs`, `sklearn.cluster.AgglomerativeClustering`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` to create synthetic data.\n",
        "* **Step 3: Apply Agglomerative Clustering.** Initialize `AgglomerativeClustering(n_clusters=some_number, linkage='complete')` and fit it to the data.\n",
        "* **Step 4: Visualize.** Create a scatter plot, coloring the points according to the cluster labels.\n",
        "\n",
        "#### **34. Load the Breast Cancer dataset and compare inertia values for $K=2$ to 6 using K-Means. Show results in a line plot.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_breast_cancer`, `sklearn.cluster.KMeans`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load data.** Load the breast cancer dataset.\n",
        "* **Step 3: Calculate inertia.** Loop through K from 2 to 6. Inside the loop, fit `KMeans(n_clusters=k)` and store the `inertia_` attribute.\n",
        "* **Step 4: Plot.** Create a line plot of the inertia values against the K values. This is an application of the elbow method.\n",
        "\n",
        "#### **35. Generate synthetic concentric circles using `make_circles` and cluster using Agglomerative Clustering with single linkage.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_circles` and `sklearn.cluster.AgglomerativeClustering`.\n",
        "* **Step 2: Generate data.** Use `make_circles` to create the data.\n",
        "* **Step 3: Apply Agglomerative Clustering.** Initialize `AgglomerativeClustering(n_clusters=2, linkage='single')` and fit it to the data. Single linkage works well for non-spherical clusters like circles.\n",
        "\n",
        "#### **36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_wine`, `sklearn.preprocessing.StandardScaler`, and `sklearn.cluster.DBSCAN`.\n",
        "* **Step 2: Load and scale data.** Load the dataset and scale it using `StandardScaler`.\n",
        "* **Step 3: Apply DBSCAN.** Fit `DBSCAN` to the scaled data.\n",
        "* **Step 4: Count clusters.** Get the unique labels from the fitted model. The number of clusters is the number of unique labels, excluding the label for noise (-1).\n",
        "\n",
        "#### **37. Generate synthetic data with `make_blobs` and apply KMeans. Then plot the cluster centers on top of the data points.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs`, `sklearn.cluster.KMeans`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Generate data and apply K-Means.** Generate data using `make_blobs` and fit a `KMeans` model.\n",
        "* **Step 3: Plot.** Create a scatter plot of the data points, colored by cluster. Then, use `plt.scatter` to plot the cluster centroids on top of the data points, using a distinct marker or color (e.g., a large red 'X').\n",
        "\n",
        "#### **38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_iris` and `sklearn.cluster.DBSCAN`.\n",
        "* **Step 2: Load and apply DBSCAN.** Load the Iris dataset and fit `DBSCAN` to it.\n",
        "* **Step 3: Count noise points.** Count the number of samples with a cluster label of -1 from the DBSCAN results.\n",
        "\n",
        "#### **39. Generate synthetic non-linearly separable data using `make_moons`, apply K-Means, and visualize the clustering result.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_moons`, `sklearn.cluster.KMeans`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Generate data.** Use `make_moons` to create the data.\n",
        "* **Step 3: Apply K-Means.** Fit `KMeans(n_clusters=2)` to the data. Note that K-Means will likely perform poorly on this non-linear data.\n",
        "* **Step 4: Visualize.** Create a scatter plot of the data, coloring points based on the K-Means cluster labels. The poor performance should be evident from the visualization.\n",
        "\n",
        "#### **40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_digits`, `sklearn.decomposition.PCA`, `sklearn.cluster.KMeans`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load and reduce data.** Load the digits dataset and use `PCA(n_components=3)` to reduce its dimensionality.\n",
        "* **Step 3: Apply K-Means.** Fit `KMeans(n_clusters=10)` to the 3D data.\n",
        "* **Step 4: Visualize.** Create a 3D scatter plot of the data, coloring points by their cluster labels.\n",
        "\n",
        "#### **41. Generate synthetic blobs with 5 centers and apply KMeans. Then use `silhouette_score` to evaluate the clustering.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs`, `sklearn.cluster.KMeans`, and `sklearn.metrics.silhouette_score`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` with `centers=5`.\n",
        "* **Step 3: Apply K-Means.** Fit `KMeans(n_clusters=5)` to the data.\n",
        "* **Step 4: Evaluate.** Use `silhouette_score` to calculate and print the score for the clustering result.\n",
        "\n",
        "#### **42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_breast_cancer`, `sklearn.decomposition.PCA`, `sklearn.cluster.AgglomerativeClustering`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load and reduce data.** Load the dataset and use `PCA(n_components=2)` to reduce the dimensionality.\n",
        "* **Step 3: Apply Agglomerative Clustering.** Fit `AgglomerativeClustering(n_clusters=2)` to the 2D data.\n",
        "* **Step 4: Visualize.** Create a scatter plot of the 2D data, coloring points by cluster labels.\n",
        "\n",
        "#### **43. Generate noisy circular data using `make_circles` and visualize clustering results from KMeans and DBSCAN side-by-side.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_circles`, `sklearn.cluster.KMeans`, `sklearn.cluster.DBSCAN`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Generate data.** Use `make_circles` with `noise` to create noisy circular data.\n",
        "* **Step 3: Apply both algorithms.** Fit `KMeans(n_clusters=2)` and `DBSCAN` to the same data.\n",
        "* **Step 4: Visualize.** Create a figure with two subplots. In the first, plot the K-Means result. In the second, plot the DBSCAN result. This will visually demonstrate DBSCAN's advantage with non-linear shapes.\n",
        "\n",
        "#### **44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_iris`, `sklearn.cluster.KMeans`, `sklearn.metrics.silhouette_samples`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load and cluster data.** Load the Iris dataset and fit a `KMeans(n_clusters=3)` model.\n",
        "* **Step 3: Calculate silhouette samples.** Use `silhouette_samples` to get the coefficient for each sample.\n",
        "* **Step 4: Plot.** Create a horizontal bar plot (`plt.barh`) of the silhouette coefficients, grouped by cluster.\n",
        "\n",
        "#### **45. Generate synthetic data using `make_blobs` and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs`, `sklearn.cluster.AgglomerativeClustering`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Generate data and cluster.** Generate data with `make_blobs` and fit `AgglomerativeClustering(n_clusters=some_number, linkage='average')`.\n",
        "* **Step 3: Visualize.** Create a scatter plot of the data, colored according to the cluster labels.\n",
        "\n",
        "#### **46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features).**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_wine`, `sklearn.cluster.KMeans`, and `seaborn`.\n",
        "* **Step 2: Load and cluster data.** Load the Wine dataset and fit `KMeans` to it.\n",
        "* **Step 3: Prepare data for pairplot.** Create a Pandas DataFrame with the first 4 features and the K-Means cluster labels.\n",
        "* **Step 4: Visualize.** Use `seaborn.pairplot` with the hue parameter set to the cluster labels.\n",
        "\n",
        "#### **47. Generate noisy blobs using `make_blobs` and use DBSCAN to identify both clusters and noise points. Print the count.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.make_blobs` and `sklearn.cluster.DBSCAN`.\n",
        "* **Step 2: Generate data.** Use `make_blobs` with `cluster_std` and `random_state` to create noisy blobs.\n",
        "* **Step 3: Apply DBSCAN.** Fit `DBSCAN` to the data.\n",
        "* **Step 4: Print counts.** Get the unique labels. Count the number of noise points (label -1) and the number of clusters (positive labels). Print both counts.\n",
        "\n",
        "#### **48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.**\n",
        "* **Step 1: Import libraries.** Import `sklearn.datasets.load_digits`, `sklearn.manifold.TSNE`, `sklearn.cluster.AgglomerativeClustering`, and `matplotlib.pyplot`.\n",
        "* **Step 2: Load and reduce data.** Load the digits dataset and use `TSNE(n_components=2)` to reduce its dimensionality.\n",
        "* **Step 3: Apply Agglomerative Clustering.** Fit `AgglomerativeClustering(n_clusters=10)` to the 2D data.\n",
        "* **Step 4: Plot.** Create a scatter plot of the 2D data, coloring points by cluster labels."
      ],
      "metadata": {
        "id": "h-MuI1y4dhd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}