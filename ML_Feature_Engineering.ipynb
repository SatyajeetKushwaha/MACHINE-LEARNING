{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "ANS: Parameter is a variable that the model learns from the training data. Parameters define the model's behavior and are adjusted during training to minimize the difference between predictions and actual outcomes. Parameter is important because they define the structure and function of the model."
      ],
      "metadata": {
        "id": "G4y235DZzJzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n",
        "ANS: Correlation is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another. Correlation represented by a correlation coefficient , which ranges  from -1 to 1. Three types of correlation such as Positive correlation, Negative correlation, Zero correlation.\n",
        "\n",
        "**Negative correlation:**\n",
        "A negative correlation means that as one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "e.g: Speed and Travel Time\n"
      ],
      "metadata": {
        "id": "jFjIDdxt0XdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "ANS: The subfield of computer a sciences that gives computer the ability to learn without explicitly programmed. It is a files of artificial intelligence that enables computers to learn from data and make predictions or decision without being explicitly programmed.\n",
        "- Machine learns patterns from the data and the replicate the same in future.\n",
        "- The foundation of ML; includes training and testing datasets.\n",
        "- It measures the how well the model's predictions match the actual results.\n",
        "- Testing the model using unseen data to measure accuracy and performance."
      ],
      "metadata": {
        "id": "u03P45_x19JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "ANS: Th loss value is a numerical measure of how far the model's prediction are from the actual values.\n",
        "- A lower loss  indicates that the model is making better predictions, while a higher loss suggest poor performance.\n",
        "- It also helps in understanding how well the model fits the data, guides the training process, and provides insights into potential issues like overfitting or underfitting.\n",
        "- During training, the model adjust its parameters to minimize the loss value. By doing so, the model learns to make more accurate predictions.\n",
        "- A low loss means the model is learning well; a high loss suggest it needs improvement."
      ],
      "metadata": {
        "id": "UaCods7E3dA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "ANS: **Continuous Variables:**\n",
        "- A continuous variables is a numerical variable that can take an infinite number of values within a given range.\n",
        "- It is usually measured rather than counted.\n",
        "- In this variable values can be ordered.\n",
        "- e.g: Heigh, Weight, Temperature\n",
        "\n",
        "**Categorical Variables:**\n",
        "- A categorical variables represents the categorical data,\n",
        "- It represents a group or category.\n",
        "- It consist of discrete values that cannot be measured on a numerical scale but can be counted.\n",
        "- e.g: Gender, Car Brand"
      ],
      "metadata": {
        "id": "gFs_JixE5z-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "ANS: Categorical variables cannot be directly used in Machine Learning models that require numerical inputs. We need to convert categorical variables into numerical representations before feeding them into model.\n",
        "Common Techniques to handle categorical variables:\n",
        "1) Nominal \\ One Hot Encoding: It converts categorical data into numerical data.\n",
        "2) Ordinal \\ Label Encoding: Label Encoding assigns numerical data to each category. Ordinal data has categories with a meaningful order.\n",
        "3)Target Guided Ordinal Encoding: It is useful when we have large numbers of unique categories in categorical data. Categorical groups with mean / median of corresponding target variable."
      ],
      "metadata": {
        "id": "5xvjZxBc7SJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "ANS: **Train Dataset**:\n",
        "- It is the subset of the dataset used to train the model.\n",
        "- It is the largest portion of the dataset.\n",
        "- The model uses this to generalize and find patterns and relationships.\n",
        "- During training, the algorithm adjust its parameters based on this data to minimize error and improve its predictions.\n",
        "- It help the modern learn\n",
        "- e.g: If we have 10,000 data points, around 60-80% is used for training.\n",
        "\n",
        "**Test Dataset**\n",
        "- It represents the unseen data.\n",
        "- It is the subset of the dataset used t provide an unbiased evaluation of a final model fit on the training dataset.\n",
        "- It is used only after training is complete.\n",
        "- It measures the final performance oof the model on unseen data.\n",
        "- It helps to understand how well the model generalize to new inputs.\n",
        "- It evaluate the final model performance.\n",
        "-  e.g: 10-20% of data is set aside for testing."
      ],
      "metadata": {
        "id": "DpgKVdfU9VhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "ANS: sklearn.preprocessing is a module Scikit-Learn that provides various functions to transform and scale data before feeding it into machine learning models.\n",
        "- It helps improve model performance by normalizing, encoding, and standardizing data.\n",
        "- It ensures data is in a proper format before feeding it into machine learning models.\n",
        "- It convert categorical variables into numerical representations.\n",
        "- It reduce the impact of outliers."
      ],
      "metadata": {
        "id": "BhRlZOVJ_I3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is a Test set?\n",
        "ANS: A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model.\n",
        "- It helps assess how well the model generalizes to unseen data.\n",
        "- It is never used during training.\n",
        "- It also help to detect overfitting."
      ],
      "metadata": {
        "id": "Gzcw7n3iASAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "ANS: - To train a Machine Learning model, we need to split the dataset into:\n",
        "1) Training set(used to learn patterns)\n",
        "2) Validation set( Used hyperparameter tuning of model)\n",
        "3) Test data (Unseen data, test the model accuracy)\n",
        "- The common way to split data is using train_test_split from sklearn.model_selection.\n",
        "- For better model tuning, we add a validation set.\n",
        "- A structured approach ensures better model performance and efficiency.\n",
        "\n",
        "To approach a Machine Learning:\n",
        "1) We need to define the problem and need to understand business needs and constraints. Choose the model based on the problem type such as  regression and classification.\n",
        "2) Load the dataset and check for missing values, duplicates, and weird values.\n",
        "3) It visualize the data to spot patterns.\n",
        "4) It visualize distribution by using Seaborn / Matplotlib.\n",
        "5) It fill missing values by using mean , median for numerical data\n",
        "6) It encode categorical variables (OneHotEncoder, LabelEncoder)\n",
        "7) Use GridSearchCV or RandomizedSearchCV to find the best parameters.\n",
        "8) Save the trained model and used it in production."
      ],
      "metadata": {
        "id": "mHz1F8I0Bluh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "ANS: Exploratory Data Analysis (EDA) is crucial step in machine learning because it helps us understand the data before feeding it into a model.\n",
        "- If we skip this step , we might end up with wrong assumptions, poor model performance or misleading results.\n",
        "- If we have missing values in important features , or model may fail pr perform poorly.\n",
        "- Outliers can mislead the model and impact performance.\n",
        "- It improves model accuracy by selecting the right feature and transformation.\n",
        "- It saves times by avoiding unnecessary model training on bad data."
      ],
      "metadata": {
        "id": "BJpaLeD4G2rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "ANS: Correlation is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another. Correlation represented by a correlation coefficient , which ranges  from -1 to 1. Three types of correlation such as Positive correlation, Negative correlation, Zero correlation.\n"
      ],
      "metadata": {
        "id": "t9qTixNHIbcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "ANS: A negative correlation means that as one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "e.g: Speed and Travel Time"
      ],
      "metadata": {
        "id": "UKEzOwBeIite"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "ANS: In python, we can find correlation between variables using different methods, depending on whether we want a quick overview or a more detailed statistical analysis. It helps us to understand which features are important before building a machine learning model.\n",
        "- The easiest way to find correlation in Python is by using df.corr() and Seaborn heatmaps for visualization.\n",
        "- By default .corr() uses Pearson correlation, which measures linear relationships but monotonic relationships), Kendall (For ordinal data).\n",
        "- If we need both the correlation coefficient and p-value, we use pearsonr() from Scipy."
      ],
      "metadata": {
        "id": "fxWc0groIx6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "ANS: Causation means that a change in one variable leads to a change in another.\n",
        "**Correlation:**\n",
        "- Correlation it means two variables move together, but it doesn't mean one cause the other.\n",
        "- correlation does not imply causation.\n",
        "- e.g: Ice cream sales and drowning are correlated due to summer, but eating ice cream doesn't cause drowning.\n",
        "\n",
        "**Causation:**\n",
        "- Causation means one variable directly influences another.\n",
        "- It strong evidence needed to prove causation.\n",
        "- e.g: Smoking cause lung cancer, which is a true cause-effect relationship.\n"
      ],
      "metadata": {
        "id": "e93kkkWFLODt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "ANS: An optimizer is an algorithm that adjusts a machine learning model's parameters to minimize the loss function and improve predictions.\n",
        "- It helps the model learn faster and more efficiently by updating weights in the right direction.\n",
        "- The most common types are:\n",
        "1)  Gradient Descent Based Optimizers:\n",
        "-  These optimizers minimize the loss function using gradient descent, which calculates the direction and step size for weight updates.\n",
        "- It includes Batch Gradient Descent(GD), Stochastic Gradient Descent(SGD), Mini-Batch Gradient Descent.\n",
        "\n",
        "2) Adaptive Optimizers:\n",
        "- It is faster and adaptive.\n",
        "- For deep learning, Adam is the commonly used because it efficiently balances speed and accuracy.\n",
        "- It includes Momentum Optimizer, AdaGrad(Adaptive Gradient Descent), RMSprop(Root Mean Square Propagation), Adam(Adaptive Moment Estimation), AdamW(Adam with Wight Decay)\n"
      ],
      "metadata": {
        "id": "e4IxSXODbxTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Gradient Descent (GD)\n",
        "\n",
        "import numpy as np\n",
        "def gradient_descent(w, learinig_rate, gradient, iterations):\n",
        "    for i in range(iterations):\n",
        "        w = w - learning_rate * gradient(w)\n",
        "    return w\n",
        "\n",
        "gradient = lambda x: 2 * (x - 3)\n",
        "w_init = 10\n",
        "learning_rate = 0.1\n",
        "iterations = 100\n",
        "w_final = gradient_descent(w_init, learning_rate, gradient, iterations)\n",
        "print(\"Optimized Weight:\", w_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx8sd1lxIw2q",
        "outputId": "66be93c5-efca-4221-b7e1-bf53cd720e30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Weight: 3.000000001425925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic Gradient Descent (SGD)\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "sgd = SGDRegressor(learning_rate='constant', eta0=0.01, max_iter=1000)\n",
        "sgd.fit(x, y)\n",
        "\n",
        "print(\"SGD Coefficient:\", sgd.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkM-utidghsF",
        "outputId": "4ffcd3bc-dedc-4148-969c-fa07799fa7d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD Coefficient: [0.68961444]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini-Batch Gradient Descent\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(100, 1)\n",
        "y = 3 * x + 2 + np.random.rand(100, 1) * 0.1\n",
        "\n",
        "model = Sequential([Dense(1, input_dim=1,)])\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mean_squared_error')\n",
        "model.fit(x, y, epochs=100, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nHAyykKgmHN",
        "outputId": "36af7da8-be86-42a9-bd8f-23d6307a4720"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.5809   \n",
            "Epoch 2/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5059 \n",
            "Epoch 3/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7188 \n",
            "Epoch 4/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6624 \n",
            "Epoch 5/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9189 \n",
            "Epoch 6/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5985 \n",
            "Epoch 7/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3467 \n",
            "Epoch 8/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1886 \n",
            "Epoch 9/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1246 \n",
            "Epoch 10/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0827 \n",
            "Epoch 11/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0583 \n",
            "Epoch 12/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0321 \n",
            "Epoch 13/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0239 \n",
            "Epoch 14/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0186 \n",
            "Epoch 15/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0129 \n",
            "Epoch 16/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0123 \n",
            "Epoch 17/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0109 \n",
            "Epoch 18/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0110 \n",
            "Epoch 19/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0101 \n",
            "Epoch 20/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0097 \n",
            "Epoch 21/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0091 \n",
            "Epoch 22/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0090 \n",
            "Epoch 23/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0098 \n",
            "Epoch 24/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0089 \n",
            "Epoch 25/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0071 \n",
            "Epoch 26/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 \n",
            "Epoch 27/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0074 \n",
            "Epoch 28/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0080 \n",
            "Epoch 29/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0091 \n",
            "Epoch 30/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0078 \n",
            "Epoch 31/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0074 \n",
            "Epoch 32/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0083 \n",
            "Epoch 33/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0066  \n",
            "Epoch 34/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0070 \n",
            "Epoch 35/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0065 \n",
            "Epoch 36/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0061 \n",
            "Epoch 37/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0076 \n",
            "Epoch 38/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0057 \n",
            "Epoch 39/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0075 \n",
            "Epoch 40/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0067 \n",
            "Epoch 41/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0059 \n",
            "Epoch 42/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0061 \n",
            "Epoch 43/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0056 \n",
            "Epoch 44/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0053 \n",
            "Epoch 45/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 \n",
            "Epoch 46/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0063 \n",
            "Epoch 47/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0045  \n",
            "Epoch 48/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0055 \n",
            "Epoch 49/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0047 \n",
            "Epoch 50/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 \n",
            "Epoch 51/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0054 \n",
            "Epoch 52/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 \n",
            "Epoch 53/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 \n",
            "Epoch 54/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 \n",
            "Epoch 55/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042  \n",
            "Epoch 56/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 \n",
            "Epoch 57/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046  \n",
            "Epoch 58/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042 \n",
            "Epoch 59/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 \n",
            "Epoch 60/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038  \n",
            "Epoch 61/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 \n",
            "Epoch 62/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 \n",
            "Epoch 63/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0045 \n",
            "Epoch 64/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 \n",
            "Epoch 65/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 \n",
            "Epoch 66/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 \n",
            "Epoch 67/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 \n",
            "Epoch 68/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 \n",
            "Epoch 69/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 \n",
            "Epoch 70/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0033 \n",
            "Epoch 71/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 \n",
            "Epoch 72/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 \n",
            "Epoch 73/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 \n",
            "Epoch 74/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 \n",
            "Epoch 75/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032  \n",
            "Epoch 76/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 \n",
            "Epoch 77/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 \n",
            "Epoch 78/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 \n",
            "Epoch 79/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 \n",
            "Epoch 80/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0025 \n",
            "Epoch 81/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0027 \n",
            "Epoch 82/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0027 \n",
            "Epoch 83/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0025 \n",
            "Epoch 84/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0027  \n",
            "Epoch 85/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0023     \n",
            "Epoch 86/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0026 \n",
            "Epoch 87/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0024 \n",
            "Epoch 88/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0027 \n",
            "Epoch 89/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0025 \n",
            "Epoch 90/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 \n",
            "Epoch 91/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0021 \n",
            "Epoch 92/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0023 \n",
            "Epoch 93/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0020 \n",
            "Epoch 94/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024 \n",
            "Epoch 95/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0020 \n",
            "Epoch 96/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0026 \n",
            "Epoch 97/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023 \n",
            "Epoch 98/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0021 \n",
            "Epoch 99/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019     \n",
            "Epoch 100/100\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0020 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x788dca8d6050>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Momentum Optimizer\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "RfiWz4rah4_Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaGrad (Adaptive Gradient Descent)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "hJ7TyYPxiGOf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "TM9H8ctriMCu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam (Adaptive Moment Estimation)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "2SVuPHXFiRsh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  AdamW (Adam with Weight Decay)\n",
        "\n",
        "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "x-Ln1PdTiW68"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "ANS: sklearn.linear_model is a module in Scikit-Learn that provides linear models for regression and classification Problems.\n",
        "- It includes algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, and more.\n",
        "- It is useful for predicting values , binary/multi-class classification, and feature selection."
      ],
      "metadata": {
        "id": "Zvug4b3Dierx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "ANS: The fit() function trains a machine learning model by learning from input features (x) and target labels(y).\n",
        "- It finds the best parameters to minimize error.\n",
        "- Different models may require extra arguments like epochs and batch_size for neural networks.\n",
        "- x (features) and y (target labels) are required for argument. some models may take additional parameters for tuning."
      ],
      "metadata": {
        "id": "nzTHwhIpjLvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "ANS: model.predict() generates predictions using a trained machine learning model. It takes new input data(X_new) and returns predicted values based on the patterns the model learned during training.\n",
        "- For arguments, X_new must be provided.\n",
        "- The input must match the shape and format used during training."
      ],
      "metadata": {
        "id": "JGLWCcK-kYfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "ANS: **Continuous Variables:**\n",
        "- A continuous variables is a numerical variable that can take an infinite number of values within a given range.\n",
        "- It is usually measured rather than counted.\n",
        "- In this variable values can be ordered.\n",
        "e.g: Heigh, Weight, Temperature\n",
        "\n",
        "**Categorical Variables:**\n",
        "\n",
        "- A categorical variables represents the categorical data,\n",
        "- It represents a group or category.\n",
        "- It consist of discrete values that cannot be measured on a numerical scale but can be counted.\n",
        "e.g: Gender, Car Brand"
      ],
      "metadata": {
        "id": "o4uWnkXclCv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "ANS: Feature Scaling is the process of normalizing or standardizing numerical features so they have a consistent scale.\n",
        "- It helps machine learning models work efficiently by ensuring that all features contribute equally to the model.\n",
        "- Feature scaling tries to bring all the features on the same scale.\n",
        "- Many of the algorithms are distance based, that's why if high magnitude it becomes, computationally expensive.\n",
        "- All of the features are on same scale then interpretation becomes easier.\n"
      ],
      "metadata": {
        "id": "YL34_5Nlldma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "ANS: We use sklearn.preprocessing methods like MinMaxScaler (0-1 range), StandardScaler(Z_score), RobustScaler(outlier-resistant) and log transformation (for skewed data)."
      ],
      "metadata": {
        "id": "hYk2dPGqmxAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "ANS: sklearn.preprocessing is a module Scikit-Learn that provides various functions to transform and scale data before feeding it into machine learning models.\n",
        "- It helps improve model performance by normalizing, encoding, and standardizing data.\n",
        "- It ensures data is in a proper format before feeding it into machine learning models.\n",
        "- It convert categorical variables into numerical representations.\n",
        "- It reduce the impact of outliers."
      ],
      "metadata": {
        "id": "TcmkxtDZnYB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "ANS: To train a Machine Learning model, we need to split the dataset into:\n",
        "1) Training set(used to learn patterns)\n",
        "2) Validation set( Used hyperparameter tuning of model)\n",
        "3) Test data (Unseen data, test the model accuracy)\n",
        "- The common way to split data is using train_test_split from sklearn.model_selection.\n",
        "- For better model tuning, we add a validation set.\n",
        "- A structured approach ensures better model performance and efficiency."
      ],
      "metadata": {
        "id": "v7NdBCJGnjUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "ANS: It is the process of converting categorical or textual data into numeric formats that machine learning models can understand.\n",
        "- This is important because most machine learning only work with numerical data."
      ],
      "metadata": {
        "id": "wSGsUMAmnuTx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "avk95mv7jLIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ez5PXD4xjGQo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}