{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                          **Theoretical**\n",
        "\n",
        "\n",
        "1 What is Boosting in Machine Learning?\n",
        "-In machine learning, \"boosting\" is an ensemble learning technique that aims to improve the accuracy of predictive models. Here's a breakdown of what it entails:\n",
        "\n",
        "Core Idea:\n",
        "\n",
        "Boosting combines multiple \"weak learners\" into a single \"strong learner.\"\n",
        "A weak learner is a model that performs slightly better than random guessing.\n",
        "The process is sequential, meaning each subsequent model attempts to correct the errors made by the previous ones.\n",
        "\n",
        "\n",
        "\n",
        "2  How does Boosting differ from Bagging?\n",
        "-Bagging and boosting are both ensemble learning methods that combine multiple models to improve predictive accuracy, but they differ significantly in their approach:\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):**\n",
        "\n",
        "* **Parallel Training:**\n",
        "    * Bagging trains multiple independent models simultaneously.\n",
        "    * Each model is trained on a different random subset of the training data, created through bootstrap sampling (sampling with replacement).\n",
        "* **Variance Reduction:**\n",
        "    * The primary goal of bagging is to reduce variance, which helps to prevent overfitting.\n",
        "    * By averaging the predictions of multiple models, bagging smooths out the fluctuations and creates a more stable prediction.\n",
        "* **Independent Models:**\n",
        "    * The models in bagging are independent of each other.\n",
        "    * Each model is trained without considering the performance of the other models.\n",
        "* **Example:**\n",
        "    * Random Forest is a popular bagging algorithm.\n",
        "\n",
        "**Boosting:**\n",
        "\n",
        "* **Sequential Training:**\n",
        "    * Boosting trains models sequentially, where each subsequent model attempts to correct the errors made by the previous models.\n",
        "    * It focuses on the misclassified data points, giving them higher weights so that the next model pays more attention to them.\n",
        "* **Bias Reduction:**\n",
        "    * The primary goal of boosting is to reduce bias, which helps to improve accuracy.\n",
        "    * By iteratively correcting errors, boosting creates a strong model from a series of weak learners.\n",
        "* **Dependent Models:**\n",
        "    * The models in boosting are dependent on each other.\n",
        "    * Each model is built based on the performance of the previous models.\n",
        "* **Examples:**\n",
        "    * AdaBoost, Gradient Boosting, and XGBoost are popular boosting algorithms.\n",
        "\n",
        "**Key Differences Summarized:**\n",
        "\n",
        "* **Training:**\n",
        "    * Bagging: Parallel.\n",
        "    * Boosting: Sequential.\n",
        "* **Goal:**\n",
        "    * Bagging: Reduce variance.\n",
        "    * Boosting: Reduce bias.\n",
        "* **Model Dependence:**\n",
        "    * Bagging: Independent.\n",
        "    * Boosting: Dependent.\n",
        "\n",
        "\n",
        "\n",
        "3 What is the key idea behind AdaBoost?\n",
        "-The key idea behind AdaBoost (Adaptive Boosting) revolves around the concept of focusing on the mistakes made by previous models. Here's a breakdown:\n",
        "\n",
        "* **Sequential Learning and Adaptive Weighting:**\n",
        "    * AdaBoost works by training a series of \"weak learners\" sequentially.\n",
        "    * It adaptively adjusts the weights of the training data points.\n",
        "    * Specifically, it increases the weights of data points that were misclassified by previous weak learners.\n",
        "    * This forces subsequent weak learners to pay more attention to these difficult-to-classify examples.\n",
        "* **Combining Weak Learners into a Strong Learner:**\n",
        "    * The final prediction is made by combining the predictions of all the weak learners.\n",
        "    * Each weak learner is assigned a weight based on its accuracy.\n",
        "    * More accurate weak learners have a greater influence on the final prediction.\n",
        "* **Focus on Misclassified Data:**\n",
        "    * The core of AdaBoost's adaptivity is its focus on misclassified data.\n",
        "    * By giving more weight to these points, the algorithm ensures that subsequent models learn to correct the errors of their predecessors.\n",
        "\n",
        "In simpler terms, AdaBoost iteratively refines its predictions by:\n",
        "\n",
        "* Identifying where it went wrong.\n",
        "* Putting more emphasis on those errors in the next round.\n",
        "* Combining the results of each round into a final, more accurate prediction.\n",
        "\n",
        "\n",
        "\n",
        "4 Explain the working of AdaBoost with an example\n",
        "-To understand how AdaBoost works, let's break it down with a simplified, conceptual example. Imagine we want to classify data points as either \"positive\" or \"negative\" using a series of weak classifiers (in this case, simple decision \"stumps\" that make a single decision based on one feature).\n",
        "\n",
        "Here's a step-by-step illustration:\n",
        "\n",
        "**1. Initialization:**\n",
        "\n",
        "* We start with a dataset where each data point is assigned an equal weight. This means every point has the same importance in the initial learning phase.\n",
        "\n",
        "**2. First Weak Classifier:**\n",
        "\n",
        "* A weak classifier (e.g., a decision stump) is trained on the weighted data.\n",
        "* This classifier attempts to separate the positive and negative points.\n",
        "* Inevitably, some points will be misclassified.\n",
        "* The classifier's \"say\" (its influence in the final decision) is calculated based on its accuracy. More accurate classifiers get more \"say.\"\n",
        "\n",
        "**3. Updating Weights:**\n",
        "\n",
        "* The weights of the misclassified data points are increased. This means these points become more important in the next round.\n",
        "* The weights of correctly classified points are decreased.\n",
        "* This adjustment ensures that the next weak classifier focuses on the errors made by the previous one.\n",
        "\n",
        "**4. Second Weak Classifier:**\n",
        "\n",
        "* A new weak classifier is trained on the updated weighted data.\n",
        "* Because the weights have changed, this classifier will likely make different errors than the first one.\n",
        "* Again, its \"say\" is calculated based on its accuracy.\n",
        "\n",
        "**5. Iteration:**\n",
        "\n",
        "* Steps 3 and 4 are repeated for a set number of iterations or until a certain accuracy is achieved.\n",
        "* In each iteration, the algorithm focuses on the points that were difficult to classify in the previous rounds.\n",
        "\n",
        "**6. Final Prediction:**\n",
        "\n",
        "* The final prediction is made by combining the predictions of all the weak classifiers.\n",
        "* Each classifier's prediction is weighted according to its \"say.\"\n",
        "* The combined prediction is then used to classify the data point.\n",
        "\n",
        "**Simplified Example Scenario:**\n",
        "\n",
        "Imagine we want to classify points on a graph as red or blue.\n",
        "\n",
        "* **Round 1:** A simple vertical line is drawn, classifying some points correctly, but some are misclassified. The misclassified points weights are increased.\n",
        "* **Round 2:** Now the algorithm trys to correct the previous errors, so a horizontal line is drawn. Again, some errors are made, and those error's weights are increased.\n",
        "* **Final Prediction:** The results of the two lines, and the weight of each lines result, are used to make the final classification.\n",
        "\n",
        "**Key takeaways:**\n",
        "\n",
        "* AdaBoost is \"adaptive\" because it adjusts the weights of the data points.\n",
        "* It combines \"weak\" learners into a \"strong\" learner.\n",
        "* It focuses on the errors made by previous models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5 What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "-Gradient Boosting and AdaBoost are both powerful boosting algorithms, but they differ in their approach to minimizing errors. Here's a breakdown:\n",
        "\n",
        "**Gradient Boosting:**\n",
        "\n",
        "* **Focus on Residuals:**\n",
        "    * Gradient Boosting builds models by iteratively minimizing a loss function.\n",
        "    * Instead of adjusting data point weights, it focuses on the \"residuals\" (the differences between predicted and actual values).\n",
        "    * Each new model attempts to correct the errors made by the previous ones by predicting these residuals.\n",
        "* **Gradient Descent:**\n",
        "    * It uses gradient descent, an optimization algorithm, to find the best way to minimize the loss function. This is where the \"gradient\" in \"Gradient Boosting\" comes from.\n",
        "* **Flexibility:**\n",
        "    * Gradient Boosting is highly flexible and can work with various loss functions, making it suitable for both regression and classification problems.\n",
        "* **Generalization:**\n",
        "    * It is a more generalized form of boosting.\n",
        "\n",
        "**AdaBoost (Adaptive Boosting):**\n",
        "\n",
        "* **Focus on Data Weights:**\n",
        "    * AdaBoost adjusts the weights of data points, giving more importance to misclassified points.\n",
        "    * Subsequent models focus on these higher-weighted points.\n",
        "* **Weighted Models:**\n",
        "    * It assigns weights to the weak learners themselves, giving more influence to more accurate models.\n",
        "* **Specific Loss Function:**\n",
        "    * AdaBoost typically uses an exponential loss function.\n",
        "* **Weak Learners:**\n",
        "    * Historically, AdaBoost is often used with decision \"stumps\" (decision trees with a single split) as weak learners.\n",
        "\n",
        "**Key Differences Summarized:**\n",
        "\n",
        "* **Error Correction:**\n",
        "    * Gradient Boosting: Corrects errors by predicting residuals.\n",
        "    * AdaBoost: Corrects errors by adjusting data point weights.\n",
        "* **Optimization:**\n",
        "    * Gradient Boosting: Uses gradient descent.\n",
        "    * AdaBoost: Adjusts data point weights.\n",
        "* **Flexibility:**\n",
        "    * Gradient Boosting: More flexible, with various loss functions.\n",
        "    * AdaBoost: less flexible, typically using exponential loss.\n",
        "\n",
        "In essence, while both algorithms aim to improve prediction accuracy by combining weak learners, they employ different strategies for identifying and correcting errors. Gradient boosting uses the concept of gradients to minimize loss, while Adaboost, uses the concept of weighted data points.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6 What is the loss function in Gradient Boosting?\n",
        "-In Gradient Boosting, the \"loss function\" plays a crucial role. It's the metric that the algorithm aims to minimize during the training process. Here's a breakdown:\n",
        "\n",
        "**Purpose of the Loss Function:**\n",
        "\n",
        "* It quantifies the difference between the model's predictions and the actual target values.\n",
        "* The goal of Gradient Boosting is to iteratively build models that reduce this difference.\n",
        "\n",
        "**Key Aspects:**\n",
        "\n",
        "* **Flexibility:**\n",
        "    * A significant advantage of Gradient Boosting is its flexibility. It can accommodate various loss functions, making it adaptable to different types of problems (regression, classification, etc.).\n",
        "* **Common Loss Functions:**\n",
        "    * **Regression:**\n",
        "        * **Mean Squared Error (MSE):** This is a widely used loss function for regression tasks. It measures the average of the squared differences between predicted and actual values.\n",
        "    * **Classification:**\n",
        "        * **Log Loss (Cross-Entropy Loss):** This is commonly used for classification problems, particularly when dealing with probabilities. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "\n",
        "**How it Relates to Gradient Boosting:**\n",
        "\n",
        "* Gradient Boosting uses gradient descent to minimize the chosen loss function.\n",
        "* In each iteration, the algorithm calculates the \"gradient\" of the loss function with respect to the current predictions.\n",
        "* The weak learners (typically decision trees) are then trained to predict the negative gradient, which effectively corrects the errors made by the previous models.\n",
        "\n",
        "In essence, the loss function provides a measure of how \"wrong\" the model's predictions are, and Gradient Boosting uses this information to progressively improve its accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7 How does XGBoost improve over traditional Gradient Boosting?\n",
        "-XGBoost (Extreme Gradient Boosting) is indeed a very popular and powerful machine learning algorithm, and it builds upon the foundation of traditional Gradient Boosting. However, it incorporates several significant enhancements that contribute to its superior performance and efficiency. Here are some key ways XGBoost improves over traditional Gradient Boosting:\n",
        "\n",
        "**1. Regularization:**\n",
        "\n",
        "* XGBoost includes L1 and L2 regularization techniques, which help to prevent overfitting. This is a crucial improvement, as Gradient Boosting can be prone to overfitting, especially with complex datasets.\n",
        "* By penalizing overly complex models, XGBoost promotes better generalization to unseen data.\n",
        "\n",
        "**2. Speed and Efficiency:**\n",
        "\n",
        "* XGBoost is designed for computational efficiency. It utilizes parallel processing, allowing it to take advantage of multiple CPU cores, which significantly speeds up training.\n",
        "* It also employs optimized data structures and algorithms, leading to faster execution times and reduced memory usage.\n",
        "\n",
        "**3. Handling Missing Values:**\n",
        "\n",
        "* XGBoost has built-in capabilities for handling missing values. It can automatically learn the best way to handle missing data during training, eliminating the need for manual imputation.\n",
        "\n",
        "**4. Tree Pruning:**\n",
        "\n",
        "* XGBoost uses a more sophisticated tree pruning technique. Instead of stopping tree growth based on a simple threshold, it prunes trees based on a more comprehensive evaluation of loss reduction. This helps to create more accurate and robust models.\n",
        "\n",
        "**5. Cross-Validation:**\n",
        "\n",
        "* XGBoost has built-in cross-validation functionality, which simplifies the process of evaluating model performance and tuning hyperparameters.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* While both Gradient Boosting and XGBoost are based on the same fundamental principles, XGBoost incorporates key optimizations that enhance its speed, efficiency, and accuracy.\n",
        "* These optimizations, particularly regularization and parallel processing, have made XGBoost a popular choice for many machine learning tasks.\n",
        "\n",
        "Therefore, XGBoost can be viewed as an optimized and highly effective implementation of the Gradient Boosting framework.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8 What is the difference between XGBoost and CatBoost?\n",
        "-When comparing XGBoost and CatBoost, the key differences primarily revolve around how they handle categorical features and their underlying algorithms. Here's a breakdown:\n",
        "\n",
        "**CatBoost:**\n",
        "\n",
        "* **Handling Categorical Features:**\n",
        "    * CatBoost excels at handling categorical features natively. It uses a novel method called \"ordered boosting\" to deal with categorical variables, which reduces bias and overfitting.\n",
        "    * This eliminates the need for extensive preprocessing like one-hot encoding, especially for high-cardinality categorical features.\n",
        "* **Ordered Boosting:**\n",
        "    * CatBoost's \"ordered boosting\" helps prevent target leakage, which is a common problem when dealing with categorical variables.\n",
        "* **Symmetric Trees:**\n",
        "    * CatBoost often uses symmetric (balanced) trees, which can sometimes lead to faster prediction times.\n",
        "* **Reduced Overfitting:**\n",
        "    * The ordered boosting and other algorithmic features contribute to CatBoost's robustness against overfitting.\n",
        "\n",
        "**XGBoost:**\n",
        "\n",
        "* **Handling Categorical Features:**\n",
        "    * While XGBoost has gained some categorical feature handling abilities, traditionally, it requires categorical features to be encoded numerically (e.g., one-hot encoding). This can be cumbersome, especially with high-cardinality features.\n",
        "* **Flexibility and Customization:**\n",
        "    * XGBoost is known for its high degree of flexibility and customization. It offers a wide range of parameters for fine-tuning models.\n",
        "* **Speed and Performance:**\n",
        "    * XGBoost is renowned for its speed and efficiency, particularly on structured/tabular datasets.\n",
        "* **Regularization:**\n",
        "    * XGBoost has robust regularization techniques (L1 and L2) to prevent overfitting.\n",
        "\n",
        "**Key Differences Summarized:**\n",
        "\n",
        "* **Categorical Feature Handling:**\n",
        "    * CatBoost: Native handling, often superior.\n",
        "    * XGBoost: Requires more preprocessing.\n",
        "* **Overfitting:**\n",
        "    * CatBoost: Stronger emphasis on preventing overfitting through its algorithm.\n",
        "    * XGBoost: relies on strong regularization parameters.\n",
        "* **Flexibility and Speed:**\n",
        "    * XGBoost: very high flexibility, and often very fast.\n",
        "    * Catboost: very strong with categorical data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9 What are some real-world applications of Boosting techniques?\n",
        "-Boosting techniques, with their ability to significantly enhance predictive accuracy, have found widespread applications across various industries. Here are some real-world examples:\n",
        "\n",
        "**1. Finance:**\n",
        "\n",
        "* **Credit Risk Assessment:** Boosting models are used to predict the likelihood of loan defaults, helping financial institutions make informed lending decisions.\n",
        "* **Fraud Detection:** They play a crucial role in identifying fraudulent transactions by analyzing patterns and anomalies in financial data.\n",
        "* **Algorithmic Trading:** Boosting algorithms are employed to develop trading strategies by predicting market trends and optimizing investment decisions.\n",
        "\n",
        "**2. Healthcare:**\n",
        "\n",
        "* **Disease Diagnosis:** Boosting models can analyze medical images and patient data to assist in the early detection and diagnosis of diseases like cancer and diabetes.\n",
        "* **Drug Discovery:** They can be used to predict the efficacy of drug candidates and optimize drug development processes.\n",
        "* **Patient Risk Prediction:** Predicting patient readmission rates, or the likelyhood of developing certain conditions based on patient history.\n",
        "\n",
        "**3. E-commerce:**\n",
        "\n",
        "* **Recommendation Systems:** Boosting algorithms are used to personalize product recommendations based on user browsing and purchase history.\n",
        "* **Customer Churn Prediction:** They help identify customers who are likely to stop using a service, allowing businesses to implement retention strategies.\n",
        "* **Demand Forecasting:** Predicting future product demand to optimize inventory management and supply chain operations.\n",
        "\n",
        "**4. Marketing:**\n",
        "\n",
        "* **Customer Segmentation:** Boosting models can segment customers based on their behavior and preferences, enabling targeted marketing campaigns.\n",
        "* **Click-Through Rate (CTR) Prediction:** They are used to predict the likelihood of users clicking on online advertisements.\n",
        "* **Sentiment Analysis:** Analyzing customer reviews and social media posts to understand customer sentiment and brand perception.\n",
        "\n",
        "**5. Computer Vision:**\n",
        "\n",
        "* **Object Detection:** Boosting algorithms are used to detect and identify objects in images and videos, crucial for applications like autonomous vehicles and surveillance systems.\n",
        "* **Face Recognition:** They are employed in facial recognition systems for security and identification purposes.\n",
        "\n",
        "**6. Natural Language Processing (NLP):**\n",
        "\n",
        "* **Spam Detection:** Boosting models are used to filter spam emails and messages.\n",
        "* **Text Classification:** They can categorize text documents based on their content, useful for tasks like news categorization and sentiment analysis.\n",
        "* **Search Engine Ranking:** Boosting algorithms contribute to improving the relevance of search results.\n",
        "\n",
        "**7. Energy:**\n",
        "\n",
        "* **Predicting energy consumption:** boosting algorithms are used to predict energy consumption for better grid management.\n",
        "* **Fault detection in power systems:** detecting and predicting faults in power systems to improve reliability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10 How does regularization help in XGBoost?\n",
        "-Regularization is a crucial component of XGBoost, playing a significant role in preventing overfitting and improving the model's ability to generalize to unseen data. Here's how it works:\n",
        "\n",
        "**The Problem of Overfitting:**\n",
        "\n",
        "* Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations rather than underlying patterns.\n",
        "* This results in excellent performance on the training data but poor performance on new, unseen data.\n",
        "* Boosting algorithms, including Gradient Boosting, can be prone to overfitting because they iteratively build models that try to correct errors.\n",
        "\n",
        "**How Regularization Helps:**\n",
        "\n",
        "XGBoost incorporates regularization techniques to control model complexity and prevent overfitting:\n",
        "\n",
        "* **L1 and L2 Regularization:**\n",
        "    * XGBoost supports both L1 (Lasso) and L2 (Ridge) regularization.\n",
        "    * **L1 regularization (alpha):**\n",
        "        * Adds a penalty term to the loss function that is proportional to the absolute values of the leaf weights.\n",
        "        * This encourages sparsity, meaning it can drive some leaf weights to zero, effectively performing feature selection.\n",
        "    * **L2 regularization (lambda):**\n",
        "        * Adds a penalty term to the loss function that is proportional to the squared values of the leaf weights.\n",
        "        * This encourages smaller, more evenly distributed leaf weights, reducing the model's sensitivity to individual features.\n",
        "* **Tree-Specific Regularization:**\n",
        "    * **Gamma:**\n",
        "        * Specifies the minimum loss reduction required to make a further partition on a leaf node.\n",
        "        * Higher gamma values result in more conservative splits and simpler tree structures.\n",
        "    * **Min_child_weight:**\n",
        "        * Requires each leaf node to have a minimum sum of instance weights.\n",
        "        * This controls the depth and complexity of the trees, with higher values leading to simpler, more general trees.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "* Regularization adds constraints to the model's learning process, discouraging it from becoming too complex.\n",
        "* By penalizing overly complex models, XGBoost promotes simpler, more general models that are better able to handle unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11 What are some hyperparameters to tune in Gradient Boosting models?\n",
        "-Tuning hyperparameters is essential for optimizing the performance of Gradient Boosting models. Here are some of the most important hyperparameters to consider:\n",
        "\n",
        "**1. Number of Estimators (n_estimators):**\n",
        "\n",
        "* This determines the number of boosting stages or trees to build.\n",
        "* * **Tuning:** A higher number can improve accuracy but increases training time and risk of overfitting. Use cross-validation to find the optimal value.\n",
        "\n",
        "**2. Learning Rate (learning_rate or eta):**\n",
        "\n",
        "* This controls the contribution of each tree to the final prediction.\n",
        "* * **Tuning:** A smaller learning rate requires more trees but often leads to better generalization. Typically, values between 0.01 and 0.2 are used.\n",
        "\n",
        "**3. Tree Depth (max_depth):**\n",
        "\n",
        "* This limits the maximum depth of each tree, controlling its complexity.\n",
        "* * **Tuning:** Deeper trees can capture more complex relationships but are more prone to overfitting. Start with smaller values and increase gradually.\n",
        "\n",
        "**4. Minimum Samples Split (min_samples_split):**\n",
        "\n",
        "* This specifies the minimum number of samples required to split an internal node.\n",
        "* * **Tuning:** Higher values prevent the model from creating very specific splits, reducing overfitting.\n",
        "\n",
        "**5. Minimum Samples Leaf (min_samples_leaf):**\n",
        "\n",
        "* This specifies the minimum number of samples required to be at a leaf node.\n",
        "* * **Tuning:** Similar to `min_samples_split`, higher values prevent overfitting.\n",
        "\n",
        "**6. Subsample:**\n",
        "\n",
        "* This controls the fraction of samples used for training each tree.\n",
        "* * **Tuning:** Values less than 1 introduce randomness, reducing variance and preventing overfitting.\n",
        "\n",
        "**7. Feature Subsampling (max_features):**\n",
        "\n",
        "* This controls the fraction of features used for training each tree.\n",
        "* * **Tuning:** Similar to `subsample`, this introduces randomness and reduces variance.\n",
        "\n",
        "**8. Loss Function (loss):**\n",
        "\n",
        "* This defines the loss function to be minimized.\n",
        "* * **Tuning:** The choice depends on the problem type (regression or classification). Common options include 'ls' (least squares) for regression and 'deviance' (logistic regression) for classification.\n",
        "\n",
        "**9. Regularization Parameters (alpha, lambda):**\n",
        "\n",
        "* These are used to prevent overfitting, especially in XGBoost.\n",
        "* * **Tuning:** Experiment with different values to find the optimal balance between bias and variance.\n",
        "\n",
        "**General Tuning Strategies:**\n",
        "\n",
        "* **Cross-Validation:** Use k-fold cross-validation to evaluate model performance and avoid overfitting during hyperparameter tuning.\n",
        "* **Grid Search or Random Search:** These techniques can help you systematically explore the hyperparameter space.\n",
        "* **Bayesian Optimization:** a more advanced technique that can be more efficient than Grid or Random search.\n",
        "* **Start with Important Parameters:** Begin by tuning the most influential parameters, such as `n_estimators`, `learning_rate`, and `max_depth`.\n",
        "* **Monitor Performance:** Keep track of the model's performance on the validation set during tuning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12 What is the concept of Feature Importance in Boosting?\n",
        "-In the context of boosting algorithms, \"Feature Importance\" refers to the process of assigning scores to input features based on how useful they are for predicting the target variable. Essentially, it helps us understand which features contribute most to the model's predictive power.\n",
        "\n",
        "Here's a breakdown of the concept:\n",
        "\n",
        "**Why Feature Importance Matters:**\n",
        "\n",
        "* **Model Interpretation:** It provides insights into which features are most influential in the model's predictions, making the model more interpretable.\n",
        "* **Feature Selection:** It can be used to identify and remove irrelevant or redundant features, simplifying the model and improving its performance.\n",
        "* **Understanding Data:** It helps understand the relationships between features and the target variable, providing valuable insights into the underlying data.\n",
        "\n",
        "**How Boosting Algorithms Calculate Feature Importance:**\n",
        "\n",
        "* Boosting algorithms, particularly tree-based ones like Gradient Boosting, XGBoost, and CatBoost, calculate feature importance based on how often a feature is used to split nodes in the trees.\n",
        "* **Splitting Frequency:**\n",
        "    * The more a feature is used to split nodes, the more important it is considered.\n",
        "    * This is because splitting on a feature implies that it effectively separates data points with different target values.\n",
        "* **Information Gain:**\n",
        "    * Some algorithms also consider the information gain (or similar metrics) associated with each split.\n",
        "    * Features that lead to higher information gain are considered more important.\n",
        "* **Weighting:**\n",
        "    * The feature importance scores are often normalized, so they sum up to 1 or are represented as percentages.\n",
        "\n",
        "**Different Types of Feature Importance:**\n",
        "\n",
        "* **Gain:**\n",
        "    * Represents the total gain brought by that feature to the model.\n",
        "* **Cover:**\n",
        "    * Represents the number of observations related to this feature.\n",
        "* **Frequency:**\n",
        "    * Represents the relative number of times a particular feature's value is used in the trees of the model to make predictions.\n",
        "\n",
        "**Practical Applications:**\n",
        "\n",
        "* **Dimensionality Reduction:** Removing less important features to reduce the number of input variables.\n",
        "* **Feature Engineering:** Creating new features based on the most important ones.\n",
        "* **Business Insights:** Identifying the key factors that drive business outcomes.\n",
        "\n",
        "\n",
        "\n",
        "13 Why is CatBoost efficient for categorical data?\n",
        "-CatBoost is designed to handle categorical data efficiently due to its unique approach, which addresses the challenges associated with traditional encoding methods. Here's why it excels in this area:\n",
        "\n",
        "**1. Ordered Boosting:**\n",
        "\n",
        "* CatBoost uses an innovative algorithm called \"ordered boosting\" to deal with categorical features.\n",
        "* This method avoids target leakage, a common problem when encoding categorical variables.\n",
        "* Target leakage occurs when information from the target variable is inadvertently used to create the encoding, leading to overly optimistic performance on the training data and poor generalization.\n",
        "* Ordered boosting calculates the target statistics in a way that is less prone to this leakage.\n",
        "\n",
        "**2. Target Statistics Calculation:**\n",
        "\n",
        "* Traditional methods often use target statistics (e.g., the average target value for each category) to encode categorical features.\n",
        "* However, these methods can introduce noise and bias, especially with low-frequency categories.\n",
        "* CatBoost's ordered boosting calculates target statistics in a way that reduces noise and improves accuracy. It uses previous rows to calculate the target statistic for the current row, avoiding the use of the current rows target value in its own calculation.\n",
        "\n",
        "**3. Reduced Need for Preprocessing:**\n",
        "\n",
        "* Because CatBoost handles categorical features natively, it significantly reduces the need for extensive preprocessing steps like one-hot encoding.\n",
        "* One-hot encoding can create a large number of new features, especially with high-cardinality categorical variables, leading to increased dimensionality and computational cost.\n",
        "* CatBoost's approach simplifies the workflow and improves efficiency.\n",
        "\n",
        "**4. Handling High-Cardinality Features:**\n",
        "\n",
        "* CatBoost is particularly effective at handling high-cardinality categorical features (features with many unique categories).\n",
        "* Its ordered boosting algorithm can effectively capture the relationships between these features and the target variable, even when some categories have very few observations.\n",
        "\n",
        "**5. Symmetric Trees:**\n",
        "\n",
        "* CatBoost often uses symmetric (balanced) trees, which can sometimes lead to faster prediction times, and is also an efficiency gain.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "CatBoost's efficiency with categorical data stems from its ordered boosting algorithm, which mitigates target leakage and reduces noise. This allows it to handle categorical features directly, minimizing the need for extensive preprocessing and improving overall performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8r_n8ahFlCC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "**Practical**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14 Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
        "-from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost Classifier\n",
        "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42) #50 weak learners\n",
        "\n",
        "# Train the classifier\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Example to show feature importance.\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(adaboost_classifier.feature_importances_)\n",
        "\n",
        "\n",
        "\n",
        "15 Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "-from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost Regressor\n",
        "adaboost_regressor = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "adaboost_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the MAE\n",
        "print(f\"AdaBoost Regressor Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "# Example to show feature importance.\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(adaboost_regressor.feature_importances_)\n",
        "\n",
        "\n",
        "\n",
        "16 Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
        "-from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importance\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(data.feature_names, gb_classifier.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "17 Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "-from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")\n",
        "\n",
        "#Example of feature importance\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(gb_regressor.feature_importances_)\n",
        "\n",
        "\n",
        "\n",
        "18 Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "-import xgboost as xgb\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "xgb_y_pred = xgb_classifier.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "gb_y_pred = gb_classifier.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_y_pred)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"XGBoost Classifier Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(f\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\")\n",
        "\n",
        "#Example of feature importance for XGBoost.\n",
        "print(\"\\nXGBoost Feature Importances:\")\n",
        "print(xgb_classifier.feature_importances_)\n",
        "\n",
        "\n",
        "\n",
        "19 Train a CatBoost Classifier and evaluate using F1-Score\n",
        "-import catboost as cb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a CatBoost Classifier\n",
        "catboost_classifier = cb.CatBoostClassifier(\n",
        "    iterations=100,  # Number of boosting iterations\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    random_state=42,\n",
        "    verbose=0 # prevents the model from printing every iteration.\n",
        ")\n",
        "\n",
        "# Train the classifier\n",
        "catboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = catboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the F1-score\n",
        "print(f\"CatBoost Classifier F1-Score: {f1:.4f}\")\n",
        "\n",
        "#Example of feature importance.\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(catboost_classifier.feature_importances_)\n",
        "\n",
        "\n",
        "20 Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "-import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an XGBoost Regressor\n",
        "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "xgb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the MSE\n",
        "print(f\"XGBoost Regressor Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Example to show feature importance.\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(xgb_regressor.feature_importances_)\n",
        "\n",
        "\n",
        "\n",
        "21 Train an AdaBoost Classifier and visualize feature importance\n",
        "-import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost Classifier\n",
        "adaboost_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = adaboost_classifier.feature_importances_\n",
        "\n",
        "# Visualize feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(data.feature_names, feature_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"AdaBoost Classifier Feature Importance\")\n",
        "plt.tight_layout() #prevents labels from being cut off.\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "22 Train a Gradient Boosting Regressor and plot learning curves\n",
        "-import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "import numpy as np\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Calculate learning curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    gb_regressor, X_train, y_train, cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Calculate mean and standard deviation of training and test scores\n",
        "train_scores_mean = -np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = -np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, label='Training MSE')\n",
        "plt.plot(train_sizes, test_scores_mean, label='Cross-validation MSE')\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1)\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1)\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Gradient Boosting Regressor Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "23 Train an XGBoost Classifier and visualize feature importance\n",
        "-import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importances = xgb_classifier.feature_importances_\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(data.feature_names, feature_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"XGBoost Classifier Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "24 Train a CatBoost Classifier and plot the confusion matrix\n",
        "-import catboost as cb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a CatBoost Classifier\n",
        "catboost_classifier = cb.CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Train the classifier\n",
        "catboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = catboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('CatBoost Classifier Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "25 Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
        "-import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of number of estimators to try\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate AdaBoost for each number of estimators\n",
        "for n_estimators in n_estimators_list:\n",
        "    adaboost_classifier = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    adaboost_classifier.fit(X_train, y_train)\n",
        "    y_pred = adaboost_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"n_estimators={n_estimators}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot the accuracy vs. number of estimators\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_list, accuracies, marker='o')\n",
        "plt.xlabel(\"Number of Estimators (n_estimators)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AdaBoost Classifier Accuracy vs. Number of Estimators\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "26 Train a Gradient Boosting Classifier and visualize the ROC curve\n",
        "-import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the test set\n",
        "y_prob = gb_classifier.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate AUC (Area Under the ROC Curve)\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random guessing)\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Gradient Boosting Classifier ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "27 Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
        "-import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an XGBoost Regressor\n",
        "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(xgb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best learning rate and best model\n",
        "best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "best_regressor = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "print(f\"Best Model MSE: {mse:.4f}\")\n",
        "\n",
        "#Example of feature importance from the best model.\n",
        "print(\"\\nFeature Importances from Best Model:\")\n",
        "print(best_regressor.feature_importances_)\n",
        "\n",
        "\n",
        "28 Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "-import catboost as cb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # Imbalance: 90% class 0, 10% class 1\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CatBoost Classifier without class weights\n",
        "catboost_no_weights = cb.CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        ")\n",
        "catboost_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = catboost_no_weights.predict(X_test)\n",
        "\n",
        "# CatBoost Classifier with class weights\n",
        "catboost_with_weights = cb.CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        "    class_weights=[1, 9],  # Adjust weights to balance classes\n",
        ")\n",
        "catboost_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = catboost_with_weights.predict(X_test)\n",
        "\n",
        "# Print classification reports\n",
        "print(\"CatBoost without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"\\nCatBoost with class weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n",
        "\n",
        "\n",
        "29 Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "-import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of learning rates to try\n",
        "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate AdaBoost for each learning rate\n",
        "for learning_rate in learning_rates:\n",
        "    adaboost_classifier = AdaBoostClassifier(n_estimators=100, learning_rate=learning_rate, random_state=42)\n",
        "    adaboost_classifier.fit(X_train, y_train)\n",
        "    y_pred = adaboost_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Learning Rate: {learning_rate}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot the accuracy vs. learning rate\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(learning_rates, accuracies, marker='o')\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AdaBoost Classifier Accuracy vs. Learning Rate\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "30 Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "-import xgboost as xgb\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Iris dataset (multi-class)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an XGBoost Classifier for multi-class classification\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',  # Multi-class classification with probabilities\n",
        "    num_class=3,  # Number of classes in the Iris dataset\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss' #sets the metric that is evaluated during training.\n",
        ")\n",
        "\n",
        "# Train the classifier\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the test set\n",
        "y_prob = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Calculate the log loss\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "\n",
        "# Print the log loss\n",
        "print(f\"XGBoost Classifier Log Loss: {logloss:.4f}\")\n",
        "\n",
        "#Example of feature importance.\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(xgb_classifier.feature_importances_)"
      ],
      "metadata": {
        "id": "fNgkKqZAnC34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}