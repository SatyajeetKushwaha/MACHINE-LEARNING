{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axNJGtf8_ghQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "Assignment\n",
        "\n",
        "Theoretical Question\n",
        "1.What is a Support Vector Machine (SVM)?\n",
        "\n",
        "answ->A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
        "\n",
        "How SVM Works: Margin Maximization: SVM aims to maximize the margin (distance) between the closest data points (support vectors) of different classes and the decision boundary.\n",
        "\n",
        "Kernel Trick: If data is not linearly separable, SVM uses kernel functions (like polynomial or radial basis function (RBF)) to map data into a higher-dimensional space where it becomes separable.\n",
        "\n",
        "Soft Margin vs. Hard Margin:\n",
        "\n",
        "Hard margin: Used when data is perfectly separable.\n",
        "\n",
        "Soft margin: Allows some misclassification when data is noisy or not perfectly separable.\n",
        "\n",
        "Key Advantages of SVM: Effective for high-dimensional spaces.\n",
        "\n",
        "Works well with small datasets.\n",
        "\n",
        "Handles non-linearly separable data using kernels.\n",
        "\n",
        "2.What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "answ->The difference between Hard Margin SVM and Soft Margin SVM lies in how they handle data that is not perfectly separable.\n",
        "\n",
        "Hard Margin SVM Used when the data is perfectly separable.\n",
        "Tries to find a hyperplane that completely separates the classes without any misclassification.\n",
        "\n",
        "Strict‚Äîit does not allow any data points to be on the wrong side of the margin.\n",
        "\n",
        "Not suitable for noisy or overlapping data, as it may lead to overfitting.\n",
        "\n",
        "üí° Example: Works well when the data has a clear boundary without overlap.\n",
        "\n",
        "Soft Margin SVM Used when the data is not perfectly separable.\n",
        "Allows some misclassification by introducing a slack variable (Œæ) to permit some points inside the margin or on the wrong side.\n",
        "\n",
        "Introduces a regularization parameter (C) to balance margin width and misclassification penalty.\n",
        "\n",
        "More flexible and generalizes better for real-world, noisy data.\n",
        "\n",
        "3.What is the mathematical intuition behind SVM4\n",
        "\n",
        "answ->Mathematical Intuition Behind SVM The goal of Support Vector Machine (SVM) is to find an optimal hyperplane that maximizes the margin between two classes. This is achieved by solving an optimization problem.\n",
        "\n",
        "Defining the Hyperplane A hyperplane in an ùëõ n-dimensional space is given by:\n",
        "ùë§ ‚ãÖ ùë• + ùëè\n",
        "\n",
        "0 w‚ãÖx+b=0 where:\n",
        "\n",
        "ùë§ w is the weight vector (normal to the hyperplane).\n",
        "\n",
        "ùë• x is the input feature vector.\n",
        "\n",
        "ùëè b is the bias term.\n",
        "\n",
        "For a binary classification problem, we want to separate two classes:\n",
        "\n",
        "If ùë¶\n",
        "\n",
        "1 y=1, then ùë§ ‚ãÖ ùë• + ùëè ‚â• 1 w‚ãÖx+b‚â•1.\n",
        "\n",
        "If ùë¶\n",
        "\n",
        "‚àí 1 y=‚àí1, then ùë§ ‚ãÖ ùë• + ùëè ‚â§ ‚àí 1 w‚ãÖx+b‚â§‚àí1.\n",
        "\n",
        "Margin Maximization The margin is the distance between the support vectors (closest points from each class) and the hyperplane. The margin is given by:\n",
        "2 ‚à£ ‚à£ ùë§ ‚à£ ‚à£ ‚à£‚à£w‚à£‚à£ 2 ‚Äã\n",
        "\n",
        "To achieve the best classification, we want to maximize the margin while ensuring correct classification.\n",
        "\n",
        "Optimization Problem (Hard Margin SVM) We solve the following constrained optimization problem:\n",
        "\n",
        "min ‚Å° ùë§ , ùëè 1 2 ‚à£ ‚à£ ùë§ ‚à£ ‚à£ 2 w,b min ‚Äã\n",
        "\n",
        "2 1 ‚Äã ‚à£‚à£w‚à£‚à£ 2\n",
        "\n",
        "subject to:\n",
        "\n",
        "ùë¶ ùëñ ( ùë§ ‚ãÖ ùë• ùëñ + ùëè ) ‚â• 1 , ‚àÄ ùëñ y i ‚Äã (w‚ãÖx i ‚Äã +b)‚â•1,‚àÄi where ùë¶ ùëñ y i ‚Äã is the label of each data point.\n",
        "\n",
        "Soft Margin SVM (Handling Misclassification) For non-linearly separable data, we introduce slack variables ( ùúâ ùëñ Œæ i ‚Äã ) to allow some misclassification:\n",
        "ùë¶ ùëñ ( ùë§ ‚ãÖ ùë• ùëñ + ùëè ) ‚â• 1 ‚àí ùúâ ùëñ , ‚àÄ ùëñ y i ‚Äã (w‚ãÖx i ‚Äã +b)‚â•1‚àíŒæ i ‚Äã ,‚àÄi where ùúâ ùëñ ‚â• 0 Œæ i ‚Äã ‚â•0 represents how much a point violates the margin.\n",
        "\n",
        "The new optimization problem becomes:\n",
        "\n",
        "min ‚Å° ùë§ , ùëè 1 2 ‚à£ ‚à£ ùë§ ‚à£ ‚à£ 2 + ùê∂ ‚àë ùëñ\n",
        "\n",
        "1 ùëõ ùúâ ùëñ w,b min ‚Äã\n",
        "\n",
        "2 1 ‚Äã ‚à£‚à£w‚à£‚à£ 2 +C i=1 ‚àë n ‚Äã Œæ i ‚Äã\n",
        "\n",
        "where ùê∂ C is a regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
        "\n",
        "The Role of the Kernel Trick If the data is not linearly separable, SVM maps data to a higher-dimensional space using a kernel function ùêæ ( ùë• ùëñ , ùë• ùëó ) K(x i ‚Äã ,x j ‚Äã ), such as:\n",
        "Linear kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "ùë• ‚ãÖ ùë¶ K(x,y)=x‚ãÖy\n",
        "\n",
        "Polynomial kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "( ùë• ‚ãÖ ùë¶ + ùëê ) ùëë K(x,y)=(x‚ãÖy+c) d\n",
        "\n",
        "Radial Basis Function (RBF) kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "exp ‚Å° ( ‚àí ùõæ ‚à£ ‚à£ ùë• ‚àí ùë¶ ‚à£ ‚à£ 2 ) K(x,y)=exp(‚àíŒ≥‚à£‚à£x‚àíy‚à£‚à£ 2 )\n",
        "\n",
        "By using Lagrange multipliers and the dual problem, SVM only depends on dot products (kernel trick), allowing it to classify complex datasets.\n",
        "\n",
        "4.What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "answ->Role of Lagrange Multipliers in SVM Lagrange multipliers are used in Support Vector Machines (SVM) to transform the constrained optimization problem into an unconstrained one, making it easier to solve using calculus and optimization techniques.\n",
        "\n",
        "Why Do We Use Lagrange Multipliers? In SVM, we aim to find the optimal hyperplane that maximizes the margin while satisfying classification constraints. The optimization problem is:\n",
        "min ‚Å° ùë§ , ùëè 1 2 ‚à£ ‚à£ ùë§ ‚à£ ‚à£ 2 w,b min ‚Äã\n",
        "\n",
        "2 1 ‚Äã ‚à£‚à£w‚à£‚à£ 2\n",
        "\n",
        "subject to:\n",
        "\n",
        "ùë¶ ùëñ ( ùë§ ‚ãÖ ùë• ùëñ + ùëè ) ‚â• 1 , ‚àÄ ùëñ y i ‚Äã (w‚ãÖx i ‚Äã +b)‚â•1,‚àÄi However, this is a constrained optimization problem, which is difficult to solve directly. To handle this, we introduce Lagrange multipliers ( ùõº ùëñ Œ± i ‚Äã ).\n",
        "\n",
        "Lagrangian Function for SVM Using Lagrange multipliers, we convert the problem into a Lagrangian formulation:\n",
        "ùêø ( ùë§ , ùëè , ùõº )\n",
        "\n",
        "1 2 ‚à£ ‚à£ ùë§ ‚à£ ‚à£ 2 ‚àí ‚àë ùëñ\n",
        "\n",
        "1 ùëõ ùõº ùëñ [ ùë¶ ùëñ ( ùë§ ‚ãÖ ùë• ùëñ + ùëè ) ‚àí 1 ] L(w,b,Œ±)= 2 1 ‚Äã ‚à£‚à£w‚à£‚à£ 2 ‚àí i=1 ‚àë n ‚Äã Œ± i ‚Äã [y i ‚Äã (w‚ãÖx i ‚Äã +b)‚àí1] where:\n",
        "\n",
        "ùõº ùëñ Œ± i ‚Äã (Lagrange multipliers) are non-negative and enforce the constraints.\n",
        "\n",
        "If a data point is correctly classified with margin > 1, then its ùõº ùëñ Œ± i ‚Äã is 0.\n",
        "\n",
        "If a point is on the margin, ùõº ùëñ Œ± i ‚Äã is positive (support vectors).\n",
        "\n",
        "Converting to the Dual Problem Using Karush-Kuhn-Tucker (KKT) conditions, we transform this into a dual optimization problem, which depends only on dot products of data points:\n",
        "max ‚Å° ùõº ‚àë ùëñ\n",
        "\n",
        "1 ùëõ ùõº ùëñ ‚àí 1 2 ‚àë ùëñ\n",
        "\n",
        "1 ùëõ ‚àë ùëó\n",
        "\n",
        "1 ùëõ ùõº ùëñ ùõº ùëó ùë¶ ùëñ ùë¶ ùëó ( ùë• ùëñ ‚ãÖ ùë• ùëó ) Œ± max ‚Äã\n",
        "\n",
        "i=1 ‚àë n ‚Äã Œ± i ‚Äã ‚àí 2 1 ‚Äã\n",
        "\n",
        "i=1 ‚àë n ‚Äã\n",
        "\n",
        "j=1 ‚àë n ‚Äã Œ± i ‚Äã Œ± j ‚Äã y i ‚Äã y j ‚Äã (x i ‚Äã ‚ãÖx j ‚Äã ) subject to:\n",
        "\n",
        "‚àë ùëñ\n",
        "\n",
        "1 ùëõ ùõº ùëñ ùë¶ ùëñ\n",
        "\n",
        "0 , ùõº ùëñ ‚â• 0 , ‚àÄ ùëñ i=1 ‚àë n ‚Äã Œ± i ‚Äã y i ‚Äã =0,Œ± i ‚Äã ‚â•0,‚àÄi This dual problem is easier to solve and allows the use of the kernel trick to handle non-linearly separable data.\n",
        "\n",
        "Key Takeaways Lagrange multipliers help transform a constrained problem into an unconstrained one.\n",
        "They determine which points are support vectors (points with ùõº ùëñ\n",
        "\n",
        "0 Œ± i ‚Äã\n",
        "\n",
        "0).\n",
        "\n",
        "The dual form makes computation efficient and enables kernel methods.\n",
        "\n",
        "5.What are Support Vectors in SVM?\n",
        "\n",
        "answ->Support Vectors in SVM Support vectors are the data points that lie closest to the decision boundary (hyperplane) in a Support Vector Machine (SVM). These points are crucial because they define the margin and influence the placement of the hyperplane.\n",
        "\n",
        "Why Are Support Vectors Important? They are the critical points that determine the maximum-margin hyperplane.\n",
        "Removing any non-support vector does not affect the decision boundary, but removing a support vector changes it.\n",
        "\n",
        "In the dual optimization problem, support vectors have nonzero Lagrange multipliers ( ùõº ùëñ\n",
        "\n",
        "0 Œ± i ‚Äã\n",
        "\n",
        "0), while other points have ùõº ùëñ\n",
        "\n",
        "0 Œ± i ‚Äã =0.\n",
        "\n",
        "Identifying Support Vectors For a hard margin SVM (perfectly separable data), support vectors lie exactly on the margin, meaning:\n",
        "ùë¶ ùëñ ( ùë§ ‚ãÖ ùë• ùëñ + ùëè )\n",
        "\n",
        "1 y i ‚Äã (w‚ãÖx i ‚Äã +b)=1 For a soft margin SVM (with misclassification allowed), support vectors either lie on the margin or inside it:\n",
        "\n",
        "ùë¶ ùëñ ( ùë§ ‚ãÖ ùë• ùëñ + ùëè ) ‚â§ 1 y i ‚Äã (w‚ãÖx i ‚Äã +b)‚â§1 Here, slack variables ( ùúâ ùëñ Œæ i ‚Äã ) determine how much a point violates the margin.\n",
        "\n",
        "Visualization Imagine a 2D dataset with two classes (red and blue). The hyperplane separates them, and the margin is maximized. The support vectors are the closest red and blue points to the hyperplane.\n",
        "üìå Key insight: The hyperplane depends only on the support vectors, not on the entire dataset.\n",
        "\n",
        "Summary ‚úÖ Support vectors are the most important data points in SVM. ‚úÖ They define the margin and influence the decision boundary. ‚úÖ Only support vectors have nonzero Lagrange multipliers ( ùõº ùëñ\n",
        "0 Œ± i ‚Äã\n",
        "\n",
        "0). ‚úÖ They make SVM robust, as the model ignores non-support vectors.\n",
        "\n",
        "6.What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "answ->Support Vector Classifier (SVC) in SVM A Support Vector Classifier (SVC) is the classification version of Support Vector Machine (SVM). It is used to separate data into different classes by finding an optimal hyperplane that maximizes the margin between the classes.\n",
        "\n",
        "How SVC Works SVC works by: Finding the hyperplane that best separates the classes. Maximizing the margin (distance between support vectors and the hyperplane). Using support vectors (critical data points) to define the decision boundary. Using kernel functions when data is not linearly separable.\n",
        "\n",
        "Hard Margin vs. Soft Margin SVC Hard Margin SVC: Works when data is perfectly separable (strict boundary).\n",
        "\n",
        "Soft Margin SVC: Allows misclassification using a regularization parameter (C) to balance margin width and classification errors.\n",
        "\n",
        "SVC with Kernels If data is not linearly separable, SVC uses kernel functions to transform data into a higher-dimensional space where it becomes separable:\n",
        "üîπ Linear Kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "ùë• ‚ãÖ ùë¶ K(x,y)=x‚ãÖy üîπ Polynomial Kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "( ùë• ‚ãÖ ùë¶ + ùëê ) ùëë K(x,y)=(x‚ãÖy+c) d\n",
        "\n",
        "üîπ Radial Basis Function (RBF) Kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "ùëí ‚àí ùõæ ‚à£ ‚à£ ùë• ‚àí ùë¶ ‚à£ ‚à£ 2 K(x,y)=e ‚àíŒ≥‚à£‚à£x‚àíy‚à£‚à£ 2\n",
        "\n",
        "üîπ Sigmoid Kernel: ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "tanh ‚Å° ( ùõº ùë• ‚ãÖ ùë¶ + ùëê ) K(x,y)=tanh(Œ±x‚ãÖy+c)\n",
        "\n",
        "Key Parameters in SVC C (Regularization Parameter): Controls the trade-off between maximizing the margin and allowing some misclassifications.\n",
        "High C ‚Üí Small margin, fewer misclassifications (overfitting risk).\n",
        "\n",
        "Low C ‚Üí Larger margin, more misclassifications (better generalization).\n",
        "\n",
        "Kernel: Defines the transformation of data for non-linear problems.\n",
        "\n",
        "Gamma ( ùõæ Œ≥): Used in RBF and polynomial kernels to control the influence of each data point.\n",
        "\n",
        "Summary SVC is a classification model based on SVM. It finds the best hyperplane to separate data into classes. Supports both linear and non-linear classification using kernels. Support vectors are the key points defining the decision boundary.\n",
        "7.What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "answ->Support Vector Regressor (SVR) in SVM A Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. Instead of classifying data, SVR predicts continuous values while maintaining a margin of tolerance.\n",
        "\n",
        "How SVR Works It finds a function that best fits the data while allowing some error.\n",
        "A margin, called the epsilon-tube, is defined around the predicted function.\n",
        "\n",
        "Data points within this margin are ignored, while points outside it are penalized.\n",
        "\n",
        "Support vectors are the key data points that influence the final regression model.\n",
        "\n",
        "Key Parameters in SVR C: Controls the trade-off between margin width and prediction accuracy.\n",
        "Epsilon: Defines how much error is tolerated without penalty.\n",
        "\n",
        "Kernel: Helps handle non-linear relationships in data.\n",
        "\n",
        "Gamma: Used in some kernels to control how much influence each data point has.\n",
        "\n",
        "Kernels in SVR SVR supports different kernel functions:\n",
        "Linear Kernel: Best for simple relationships.\n",
        "\n",
        "Polynomial Kernel: Captures polynomial trends.\n",
        "\n",
        "RBF Kernel: Handles complex, non-linear relationships.\n",
        "\n",
        "Summary SVR predicts continuous values instead of class labels.\n",
        "It fits a function within a margin of tolerance.\n",
        "\n",
        "Only support vectors influence the regression model.\n",
        "\n",
        "It works well with both linear and non-linear data using kernels.\n",
        "\n",
        "8.What is the Kernel Trick in SVM?\n",
        "\n",
        "answ->Kernel Trick in SVM The Kernel Trick is a technique in Support Vector Machines (SVM) that allows the algorithm to handle non-linearly separable data by transforming it into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "Why is the Kernel Trick Needed? Some datasets cannot be separated by a straight line (hyperplane) in their original space.\n",
        "Instead of manually adding new features, the kernel trick maps data into a higher-dimensional space where a linear separator exists.\n",
        "\n",
        "How the Kernel Trick Works Without explicitly computing new features, a kernel function calculates the dot product of data points in a higher-dimensional space.\n",
        "This makes computation efficient and avoids the high cost of working in very large dimensions.\n",
        "\n",
        "Instead of transforming ùë• x into a high-dimensional space ùúô ( ùë• ) œï(x), we use a kernel function ùêæ ( ùë• ùëñ , ùë• ùëó ) K(x i ‚Äã ,x j ‚Äã ) that directly computes the result.\n",
        "\n",
        "Common Kernel Functions in SVM üîπ Linear Kernel: Used when data is already linearly separable.\n",
        "ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "ùë• ‚ãÖ ùë¶ K(x,y)=x‚ãÖy\n",
        "\n",
        "üîπ Polynomial Kernel: Useful for curved decision boundaries.\n",
        "\n",
        "ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "( ùë• ‚ãÖ ùë¶ + ùëê ) ùëë K(x,y)=(x‚ãÖy+c) d\n",
        "\n",
        "ùëë d controls the polynomial degree.\n",
        "\n",
        "üîπ Radial Basis Function (RBF) Kernel: Popular for complex, non-linear relationships.\n",
        "\n",
        "ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "ùëí ‚àí ùõæ ‚à£ ‚à£ ùë• ‚àí ùë¶ ‚à£ ‚à£ 2 K(x,y)=e ‚àíŒ≥‚à£‚à£x‚àíy‚à£‚à£ 2\n",
        "\n",
        "ùõæ Œ≥ controls how much influence each data point has.\n",
        "\n",
        "üîπ Sigmoid Kernel: Inspired by neural networks.\n",
        "\n",
        "ùêæ ( ùë• , ùë¶ )\n",
        "\n",
        "tanh ‚Å° ( ùõº ùë• ‚ãÖ ùë¶ + ùëê ) K(x,y)=tanh(Œ±x‚ãÖy+c)\n",
        "\n",
        "Summary The Kernel Trick allows SVM to classify non-linear data. It maps data into a higher-dimensional space without explicitly computing it. Different kernel functions help capture complex patterns. The RBF kernel is widely used for real-world problems.\n",
        "9.Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "\n",
        "answ->Comparison of Linear Kernel, Polynomial Kernel, and RBF Kernel in SVM\n",
        "\n",
        "Linear Kernel Formula: ùêæ ( ùë• , ùë¶ )\n",
        "ùë• ‚ãÖ ùë¶ K(x,y)=x‚ãÖy\n",
        "Use Case: Works best when data is linearly separable.\n",
        "\n",
        "Advantages: Simple, fast, and works well with high-dimensional data.\n",
        "\n",
        "Disadvantages: Cannot handle non-linear relationships.\n",
        "\n",
        "Polynomial Kernel Formula: ùêæ ( ùë• , ùë¶ )\n",
        "( ùë• ‚ãÖ ùë¶\n",
        "ùëê ) ùëë K(x,y)=(x‚ãÖy+c) d\n",
        "Use Case: Suitable for data with polynomial relationships.\n",
        "\n",
        "Advantages: Captures curved decision boundaries.\n",
        "\n",
        "Disadvantages: Higher degrees can lead to overfitting and slow computation.\n",
        "\n",
        "RBF (Radial Basis Function) Kernel Formula: ùêæ ( ùë• , ùë¶ )\n",
        "ùëí ‚àí ùõæ ‚à£ ‚à£ ùë• ‚àí ùë¶ ‚à£ ‚à£ 2 K(x,y)=e ‚àíŒ≥‚à£‚à£x‚àíy‚à£‚à£ 2\n",
        "Use Case: Best for complex, non-linear data.\n",
        "\n",
        "Advantages: Very powerful, adapts well to various data patterns.\n",
        "\n",
        "Disadvantages: Requires tuning of the ùõæ Œ≥ parameter, can overfit.\n",
        "\n",
        "When to Use Each Kernel? Kernel Best for Linear Linearly separable data, high dimensions Polynomial Data with polynomial relationships RBF Complex, non-linear data\n",
        "10.What is the effect of the C parameter in SVM?\n",
        "\n",
        "answ->Effect of the C Parameter in SVM The C parameter in Support Vector Machine (SVM) controls the trade-off between maximizing the margin and minimizing classification errors. It acts as a regularization parameter that influences the model‚Äôs complexity and flexibility.\n",
        "\n",
        "High C Value (Strict Model) The model gives more importance to correctly classifying all training points.\n",
        "It creates a smaller margin and tries to minimize misclassification.\n",
        "\n",
        "Effect: Leads to low bias but high variance (overfitting).\n",
        "\n",
        "Good for: Well-separated data with fewer outliers. Problem: Model becomes too sensitive to noise.\n",
        "\n",
        "Low C Value (Flexible Model) The model allows some misclassification to maintain a wider margin.\n",
        "Focuses on generalization rather than perfect training accuracy.\n",
        "\n",
        "Effect: Leads to higher bias but lower variance (better generalization).\n",
        "\n",
        "Good for: Noisy data with overlapping classes. Problem: May misclassify more training points.\n",
        "\n",
        "Summary C Value Margin Size Misclassification Risk Best For High C Small (strict) Less Overfitting Clean, well-separated data Low C Large (flexible) More Underfitting Noisy or overlapping data\n",
        "Choosing the Right C Value Start with a moderate C value and tune it using cross-validation.\n",
        "If the model overfits, decrease C to allow more flexibility.\n",
        "\n",
        "If the model underfits, increase C to focus more on training accuracy.\n",
        "\n",
        "11.What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "answ->Role of the Gamma Parameter in RBF Kernel SVM The Gamma (Œ≥) parameter in the Radial Basis Function (RBF) Kernel controls how much influence a single training example has. It determines the spread of the decision boundary by adjusting the influence of each data point.\n",
        "\n",
        "High Gamma Value (Œ≥ ‚Üí Large) Each data point has high influence over the decision boundary.\n",
        "The model creates very tight and complex decision regions around data points.\n",
        "\n",
        "Effect: High accuracy on training data but poor generalization (overfitting).\n",
        "\n",
        "Good for: Simple datasets where fine decision boundaries are needed. Problem: Can capture noise, leading to overfitting.\n",
        "\n",
        "Low Gamma Value (Œ≥ ‚Üí Small) Each data point has low influence, meaning more data points affect the decision boundary.\n",
        "The model creates smoother, more generalized decision boundaries.\n",
        "\n",
        "Effect: The model generalizes better but may fail to capture patterns (underfitting).\n",
        "\n",
        "Good for: Complex datasets where smooth decision boundaries work better. Problem: May not capture enough details, leading to underfitting.\n",
        "\n",
        "Summary Gamma Value Effect on Decision Boundary Risk Best For High Œ≥ Complex, narrow decision boundary Overfitting Simple datasets Low Œ≥ Smooth, wide decision boundary Underfitting Complex datasets\n",
        "Choosing the Right Gamma Value Start with a moderate Œ≥ value and tune it using cross-validation.\n",
        "If the model overfits, reduce Œ≥ to make the decision boundary smoother.\n",
        "\n",
        "If the model underfits, increase Œ≥ to make it more flexible.\n",
        "\n",
        "12.What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "answ->Na√Øve Bayes Classifier and Why It Is Called \"Na√Øve\"\n",
        "\n",
        "What is the Na√Øve Bayes Classifier? The Na√Øve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is commonly used for classification tasks, such as spam detection, sentiment analysis, and text classification.\n",
        "It assumes that features are independent given the class label, which simplifies computation.\n",
        "\n",
        "Why Is It Called \"Na√Øve\"? It is called \"Na√Øve\" because it assumes that all features are independent of each other, which is rarely true in real-world data. This assumption simplifies calculations but may not always be accurate.\n",
        "For example, in email spam classification, Na√Øve Bayes assumes that the presence of words like \"free\" and \"win\" are independent, even though they are often related in spam emails.\n",
        "\n",
        "Bayes‚Äô Theorem (Basic Concept) ùëÉ ( ùëå ‚à£ ùëã )\n",
        "ùëÉ ( ùëã ‚à£ ùëå ) ùëÉ ( ùëå ) ùëÉ ( ùëã ) P(Y‚à£X)= P(X) P(X‚à£Y)P(Y) ‚Äã\n",
        "Where:\n",
        "\n",
        "ùëÉ ( ùëå ‚à£ ùëã ) P(Y‚à£X) ‚Üí Probability of class ùëå Y given features ùëã X (Posterior Probability).\n",
        "\n",
        "ùëÉ ( ùëã ‚à£ ùëå ) P(X‚à£Y) ‚Üí Probability of features ùëã X given class ùëå Y (Likelihood).\n",
        "\n",
        "ùëÉ ( ùëå ) P(Y) ‚Üí Probability of class ùëå Y occurring (Prior Probability).\n",
        "\n",
        "ùëÉ ( ùëã ) P(X) ‚Üí Probability of features ùëã X occurring (Evidence).\n",
        "\n",
        "Types of Na√Øve Bayes Classifiers Gaussian Na√Øve Bayes ‚Üí Used when features follow a normal distribution.\n",
        "Multinomial Na√Øve Bayes ‚Üí Used for text classification (word frequencies).\n",
        "\n",
        "Bernoulli Na√Øve Bayes ‚Üí Used for binary feature data (presence/absence of words).\n",
        "\n",
        "13.What is Bayes Theorem?\n",
        "\n",
        "answ->Bayes‚Äô Theorem\n",
        "\n",
        "Definition Bayes‚Äô Theorem is a fundamental rule in probability theory that describes how to update our beliefs based on new evidence. It helps calculate the probability of an event occurring given prior knowledge of related events.\n",
        "\n",
        "Formula ùëÉ ( ùê¥ ‚à£ ùêµ )\n",
        "\n",
        "ùëÉ ( ùêµ ‚à£ ùê¥ ) ùëÉ ( ùê¥ ) ùëÉ ( ùêµ ) P(A‚à£B)= P(B) P(B‚à£A)P(A) ‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëÉ ( ùê¥ ‚à£ ùêµ ) P(A‚à£B) ‚Üí Probability of event A happening given that B has occurred (Posterior Probability).\n",
        "\n",
        "ùëÉ ( ùêµ ‚à£ ùê¥ ) P(B‚à£A) ‚Üí Probability of event B happening given that A has occurred (Likelihood).\n",
        "\n",
        "ùëÉ ( ùê¥ ) P(A) ‚Üí Prior probability of event A (Prior).\n",
        "\n",
        "ùëÉ ( ùêµ ) P(B) ‚Üí Total probability of event B occurring (Evidence).\n",
        "\n",
        "Example: Medical Diagnosis Imagine a test for a disease:\n",
        "1% of the population has the disease (P(Disease) = 0.01).\n",
        "\n",
        "If a person has the disease, the test is 90% accurate (P(Positive Test | Disease) = 0.9).\n",
        "\n",
        "If a person does not have the disease, the test gives a false positive 5% of the time (P(Positive Test | No Disease) = 0.05).\n",
        "\n",
        "Using Bayes' Theorem, we can find the probability that a person actually has the disease if they tested positive.\n",
        "\n",
        "Applications of Bayes‚Äô Theorem Spam Filtering ‚Üí Determines if an email is spam based on keywords. Medical Diagnosis ‚Üí Calculates disease probability based on symptoms. Machine Learning (Na√Øve Bayes) ‚Üí Used in classification algorithms.\n",
        "14.Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes:\n",
        "\n",
        "answ->Gaussian Na√Øve Bayes is used for continuous numerical data. It assumes that features follow a normal distribution. It is commonly used in predicting numerical outcomes like house prices or medical diagnosis based on measurements. It does not work well when data is not normally distributed.\n",
        "\n",
        "Multinomial Na√Øve Bayes is used for text classification problems where features represent word counts or frequencies. It works well for spam detection, sentiment analysis, and document classification. It cannot handle continuous numerical features.\n",
        "\n",
        "Bernoulli Na√Øve Bayes is used for binary data, where features indicate the presence or absence of a characteristic. It is useful for spam filtering, fraud detection, and text classification when using binary word occurrence data. It does not perform well with non-binary data.\n",
        "\n",
        "Gaussian is best for continuous numerical values, multinomial is best for word frequency data, and Bernoulli is best for binary feature data.\n",
        "\n",
        "15.When should you use Gaussian Na√Øve Bayes over other variants?\n",
        "\n",
        "answ->Gaussian Na√Øve Bayes should be used when the dataset contains continuous numerical features that follow a normal (Gaussian) distribution. It is ideal for cases where features like age, height, weight, temperature, or other real-valued attributes are important in classification.\n",
        "\n",
        "It is preferred over other Na√Øve Bayes variants when:\n",
        "\n",
        "The features are continuous rather than discrete counts or binary values.\n",
        "\n",
        "The data is approximately normally distributed for each class.\n",
        "\n",
        "You need a fast and simple probabilistic model for classification.\n",
        "\n",
        "It is not suitable when:\n",
        "\n",
        "The features are categorical or discrete, where Multinomial Na√Øve Bayes is better.\n",
        "\n",
        "The features are binary (0 or 1), where Bernoulli Na√Øve Bayes is more effective.\n",
        "\n",
        "16.What are the key assumptions made by Na√Øve Bayes?\n",
        "\n",
        "answ->Key Assumptions Made by Na√Øve Bayes Na√Øve Bayes is based on Bayes' Theorem and makes the following assumptions:\n",
        "\n",
        "Feature Independence Assumption\n",
        "\n",
        "It assumes that all features are independent of each other, given the class label.\n",
        "\n",
        "In reality, features are often correlated, but Na√Øve Bayes still works well in many cases.\n",
        "\n",
        "Equal Importance of Features\n",
        "\n",
        "Every feature contributes equally to the final classification.\n",
        "\n",
        "This may not always be true, especially in real-world datasets where some features are more important than others.\n",
        "\n",
        "Conditional Probability Follows a Specific Distribution\n",
        "\n",
        "Gaussian Na√Øve Bayes assumes a normal distribution for continuous data.\n",
        "\n",
        "Multinomial Na√Øve Bayes assumes feature values represent word counts or frequencies.\n",
        "\n",
        "Bernoulli Na√Øve Bayes assumes binary feature values (0 or 1).\n",
        "\n",
        "No Hidden or Missing Features\n",
        "\n",
        "Assumes all required features are available and correctly measured.\n",
        "\n",
        "Missing data or irrelevant features can affect performance.\n",
        "\n",
        "17.What are the advantages and disadvantages of Na√Øve Bayes?\n",
        "\n",
        "answ->Advantages and Disadvantages of Na√Øve Bayes Advantages Simple and Fast\n",
        "\n",
        "Na√Øve Bayes is easy to implement and computationally efficient.\n",
        "\n",
        "Works well with large datasets and high-dimensional data.\n",
        "\n",
        "Works Well with Small Data\n",
        "\n",
        "Requires less training data compared to other machine learning models.\n",
        "\n",
        "Performs Well in Text Classification\n",
        "\n",
        "Commonly used for spam filtering, sentiment analysis, and document classification.\n",
        "\n",
        "Handles Irrelevant Features Well\n",
        "\n",
        "Since it calculates probabilities independently, irrelevant features do not strongly affect performance.\n",
        "\n",
        "Can Work with Missing Data\n",
        "\n",
        "Unlike some models, Na√Øve Bayes can handle missing values by using probability estimates.\n",
        "\n",
        "Disadvantages Feature Independence Assumption is Unrealistic\n",
        "\n",
        "In real-world data, features are often correlated, which can reduce accuracy.\n",
        "\n",
        "Struggles with Continuous and Complex Data\n",
        "\n",
        "Gaussian Na√Øve Bayes assumes normally distributed data, which may not always be true.\n",
        "\n",
        "Zero Probability Problem\n",
        "\n",
        "If a category in test data is not present in training data, Na√Øve Bayes assigns it a probability of zero, causing issues.\n",
        "\n",
        "Solution: Use Laplace Smoothing to avoid zero probabilities.\n",
        "\n",
        "Not Always the Best Classifier\n",
        "\n",
        "Performs well for simple tasks but can be outperformed by more complex models like Random Forest, SVM, or Neural Networks in some cases.\n",
        "\n",
        "18.Why is Na√Øve Bayes a good choice for text classification?\n",
        "\n",
        "answ->Why is Na√Øve Bayes a Good Choice for Text Classification? Na√Øve Bayes is widely used for text classification because of its efficiency, accuracy, and ability to handle high-dimensional data. Here‚Äôs why it works well:\n",
        "\n",
        "Works Well with High-Dimensional Data Text data often has thousands of words (features), but Na√Øve Bayes can handle this efficiently.\n",
        "It does not require complex feature selection or reduction techniques.\n",
        "\n",
        "Fast and Scalable Training and prediction are very fast, even with large datasets.\n",
        "Works well in real-time applications like spam filtering and sentiment analysis.\n",
        "\n",
        "Handles Sparse Data Well Most words in a document are zero in a feature matrix (sparse data), but Na√Øve Bayes still performs well.\n",
        "Other models like SVM or Neural Networks often struggle with highly sparse data.\n",
        "\n",
        "Works Well with Probabilistic Word Occurrences Text classification relies on the frequency of words in different categories.\n",
        "Na√Øve Bayes calculates the probability of a document belonging to a class based on word occurrences, making it highly effective for natural language processing (NLP) tasks.\n",
        "\n",
        "19.Compare SVM and Na√Øve Bayes for classification tasks:\n",
        "\n",
        "answ->Comparison of SVM and Na√Øve Bayes for Classification Tasks SVM and Na√Øve Bayes are both widely used for classification, but they have different strengths and weaknesses.\n",
        "\n",
        "Basic Concept SVM (Support Vector Machine): Finds the optimal hyperplane that maximizes the margin between different classes.\n",
        "Na√Øve Bayes: Uses Bayes‚Äô Theorem to calculate the probability of a class given input features.\n",
        "\n",
        "Assumptions SVM: No strict assumption about data distribution, works well with non-linearly separable data.\n",
        "Na√Øve Bayes: Assumes feature independence, which may not always be true.\n",
        "\n",
        "Performance with Large Datasets SVM: Can be computationally expensive for large datasets.\n",
        "Na√Øve Bayes: Fast and efficient, even with large datasets.\n",
        "\n",
        "Performance with High-Dimensional Data SVM: Performs well with high-dimensional data but may require tuning of hyperparameters (kernel, C, gamma).\n",
        "Na√Øve Bayes: Naturally handles high-dimensional data, especially in text classification.\n",
        "\n",
        "20.How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        "answ->How Does Laplace Smoothing Help in Na√Øve Bayes? Problem: Zero Probability Issue In Na√Øve Bayes, probabilities are calculated using word frequencies (for text) or feature occurrences. If a word never appears in a particular class during training, its probability becomes zero, causing the entire classification result to be zero (since probabilities are multiplied).\n",
        "\n",
        "For example, in spam detection:\n",
        "\n",
        "If the word \"offer\" never appeared in a non-spam email, Na√Øve Bayes assigns P(offer | non-spam) = 0.\n",
        "\n",
        "This means that any new email containing \"offer\" will have a probability of being non-spam equal to zero, which is incorrect.\n",
        "\n",
        "Solution: Laplace Smoothing Laplace Smoothing adds a small constant (usually 1) to all word counts to ensure no probability is ever zero.\n",
        "\n",
        "Formula (for categorical features like text classification) ùëÉ ( ùë• ‚à£ ùë¶ )\n",
        "\n",
        "( ùëÅ ùë¶ , ùë• + ùõº ) ( ùëÅ ùë¶ + ùõº ùêæ ) P(x‚à£y)= (N y ‚Äã +Œ±K) (N y,x ‚Äã +Œ±) ‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëÅ ùë¶ , ùë• N y,x ‚Äã ‚Üí Number of times feature ùë• x appears in class ùë¶ y.\n",
        "\n",
        "ùëÅ ùë¶ N y ‚Äã ‚Üí Total number of words in class ùë¶ y.\n",
        "\n",
        "ùêæ K ‚Üí Total number of unique words.\n",
        "\n",
        "ùõº Œ± ‚Üí Smoothing parameter (usually 1, called Laplace Smoothing).\n",
        "\n",
        "How It Helps Prevents Zero Probability\n",
        "\n",
        "Ensures all words/features have a small probability, even if they were not seen in training.\n",
        "\n",
        "Improves Generalization\n",
        "\n",
        "Helps the model handle unseen words/features better during testing.\n",
        "\n",
        "Better Classification Results\n",
        "\n",
        "Reduces overfitting to training data by smoothing probability estimates.\n",
        "\n",
        "\n",
        "#Practcal Question\n",
        "\n",
        "\n",
        "#21.Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "Accuracy: 1.00\n",
        "\n",
        "#22.Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "#compare their accuracies\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data  # Features\n",
        "y = wine.target  # Target (wine class)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f'Linear Kernel Accuracy: {accuracy_linear:.2f}')\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f'RBF Kernel Accuracy: {accuracy_rbf:.2f}')\n",
        "\n",
        "Linear Kernel Accuracy: 1.00\n",
        "RBF Kernel Accuracy: 0.81\n",
        "\n",
        "#23.Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "#Squared Error (MSE)\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = datasets.fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target (house prices)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Regressor with RBF kernel\n",
        "svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "\n",
        "Mean Squared Error: 1.33\n",
        "\n",
        "#24.Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(\"SVM with Polynomial Kernel\")\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(svm_poly, X, y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#25.Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and\n",
        "#evaluate accuracy:\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#26.Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20\n",
        "#Newsgroups dataset\n",
        "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_counts, y_train)\n",
        "y_pred = clf.predict(X_test_counts)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#27.Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "#boundaries visually\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "C_values = [0.1, 1, 10]\n",
        "models = [SVC(kernel='linear', C=C).fit(X_train, y_train) for C in C_values]\n",
        "\n",
        "def plot_decision_boundary(model, X, y, title):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.show()\n",
        "\n",
        "for C, model in zip(C_values, models):\n",
        "    plot_decision_boundary(model, X, y, f\"Decision Boundary (C={C})\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#28.Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with\n",
        "#binary features\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.randint(2, size=(200, 10))\n",
        "y = np.random.randint(2, size=200)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = BernoulliNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "Accuracy: 0.53\n",
        "\n",
        "#29.Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "#unscaled data\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM without scaling\n",
        "svm_unscaled = SVC()\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "unscaled_accuracy = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with scaled features\n",
        "svm_scaled = SVC()\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "scaled_accuracy = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f'Accuracy without scaling: {unscaled_accuracy:.2f}')\n",
        "print(f'Accuracy with scaling: {scaled_accuracy:.2f}')\n",
        "\n",
        "Accuracy without scaling: 1.00\n",
        "Accuracy with scaling: 1.00\n",
        "\n",
        "#30.Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and\n",
        "#after Laplace Smoothing\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes without Laplace Smoothing\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=0)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes with Laplace Smoothing\n",
        "gnb_with_smoothing = GaussianNB()\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "print(f'Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.2f}')\n",
        "print(f'Accuracy with Laplace Smoothing: {accuracy_with_smoothing:.2f}')\n",
        "\n",
        "Accuracy without Laplace Smoothing: 1.00\n",
        "Accuracy with Laplace Smoothing: 1.00\n",
        "\n",
        "#31.Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "#gamma, kernel)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Train SVM Classifier with GridSearchCV\n",
        "svm = SVC()\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "Best Parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
        "Accuracy: 1.00\n",
        "\n",
        "#32.Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "#check it improve accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM Classifier without class weighting\n",
        "svm_no_weight = SVC()\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "\n",
        "# Train SVM Classifier with class weighting\n",
        "svm_weighted = SVC(class_weight='balanced')\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(f'Accuracy without class weighting: {accuracy_no_weight:.2f}')\n",
        "print(f'Accuracy with class weighting: {accuracy_weighted:.2f}')\n",
        "\n",
        "Accuracy without class weighting: 0.90\n",
        "Accuracy with class weighting: 0.90\n",
        "\n",
        "#33.Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load email dataset\n",
        "data = fetch_openml(name='spam', version=1, as_frame=True)\n",
        "X = data.data['text']\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Na√Øve Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_counts, y_train)\n",
        "y_pred = clf.predict(X_test_counts)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "#34.Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and\n",
        "#compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load email dataset\n",
        "data = fetch_openml(name='spam', version=1, as_frame=True)\n",
        "X = data.data['text']\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Na√Øve Bayes classifier\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_counts, y_train)\n",
        "y_pred_nb = nb_clf.predict(X_test_counts)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train_counts, y_train)\n",
        "y_pred_svm = svm_clf.predict(X_test_counts)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# Print accuracies\n",
        "print(f'Na√Øve Bayes Accuracy: {accuracy_nb:.2f}')\n",
        "print(f'SVM Accuracy: {accuracy_svm:.2f}')\n",
        "\n",
        "\n",
        "#35.Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare\n",
        "#results\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "data = fetch_openml(name='spam', version=1, as_frame=True)\n",
        "X = data.data['text']\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "selector = SelectKBest(chi2, k=1000)\n",
        "X_train_selected = selector.fit_transform(X_train_counts, y_train)\n",
        "X_test_selected = selector.transform(X_test_counts)\n",
        "\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_selected, y_train)\n",
        "y_pred_nb = nb_clf.predict(X_test_selected)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train_selected, y_train)\n",
        "y_pred_svm = svm_clf.predict(X_test_selected)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f'Na√Øve Bayes Accuracy with Feature Selection: {accuracy_nb:.2f}')\n",
        "print(f'SVM Accuracy with Feature Selection: {accuracy_svm:.2f}')\n",
        "\n",
        "\n",
        "#36.Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "#strategies on the Wine dataset and compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_ovr = SVC(kernel='linear', decision_function_shape='ovr')\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "svm_ovo = SVC(kernel='linear', decision_function_shape='ovo')\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "print(f'SVM Accuracy with One-vs-Rest: {accuracy_ovr:.2f}')\n",
        "print(f'SVM Accuracy with One-vs-One: {accuracy_ovo:.2f}')\n",
        "\n",
        "\n",
        "SVM Accuracy with One-vs-Rest: 1.00\n",
        "SVM Accuracy with One-vs-One: 1.00\n",
        "\n",
        "#37.Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "#Cancer dataset and compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f'SVM Accuracy with Linear Kernel: {accuracy_linear:.2f}')\n",
        "print(f'SVM Accuracy with Polynomial Kernel: {accuracy_poly:.2f}')\n",
        "print(f'SVM Accuracy with RBF Kernel: {accuracy_rbf:.2f}')\n",
        "\n",
        "SVM Accuracy with Linear Kernel: 0.96\n",
        "SVM Accuracy with Polynomial Kernel: 0.95\n",
        "SVM Accuracy with RBF Kernel: 0.95\n",
        "\n",
        "#38.Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "#average accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(svm, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "print(f'Cross-Validation Accuracies: {scores}')\n",
        "print(f'Average Accuracy: {np.mean(scores):.2f}')\n",
        "\n",
        "Cross-Validation Accuracies: [0.94736842 0.92982456 0.95614035 0.93859649 0.96460177]\n",
        "Average Accuracy: 0.95\n",
        "\n",
        "#39.Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare\n",
        "#performance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train)\n",
        "y_pred_default = nb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "nb_prior = GaussianNB(priors=[0.7, 0.3])\n",
        "nb_prior.fit(X_train, y_train)\n",
        "y_pred_prior = nb_prior.predict(X_test)\n",
        "accuracy_prior = accuracy_score(y_test, y_pred_prior)\n",
        "\n",
        "print(f'Na√Øve Bayes Accuracy with Default Priors: {accuracy_default:.2f}')\n",
        "print(f'Na√Øve Bayes Accuracy with Custom Priors: {accuracy_prior:.2f}')\n",
        "\n",
        "Na√Øve Bayes Accuracy with Default Priors: 0.97\n",
        "Na√Øve Bayes Accuracy with Custom Priors: 0.96\n",
        "\n",
        "#40.Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "#compare accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "rfe = RFE(estimator=svm, n_features_to_select=10)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_default = svm.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "svm.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "print(f'SVM Accuracy without RFE: {accuracy_default:.2f}')\n",
        "print(f'SVM Accuracy with RFE: {accuracy_rfe:.2f}')\n",
        "\n",
        "SVM Accuracy without RFE: 0.96\n",
        "SVM Accuracy with RFE: 0.97\n",
        "\n",
        "#41.Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "#F1-Score instead of accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "rfe = RFE(estimator=svm, n_features_to_select=10)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_default = svm.predict(X_test)\n",
        "precision_default = precision_score(y_test, y_pred_default)\n",
        "recall_default = recall_score(y_test, y_pred_default)\n",
        "f1_default = f1_score(y_test, y_pred_default)\n",
        "\n",
        "svm.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm.predict(X_test_rfe)\n",
        "precision_rfe = precision_score(y_test, y_pred_rfe)\n",
        "recall_rfe = recall_score(y_test, y_pred_rfe)\n",
        "f1_rfe = f1_score(y_test, y_pred_rfe)\n",
        "\n",
        "print(f'SVM Precision without RFE: {precision_default:.2f}')\n",
        "print(f'SVM Recall without RFE: {recall_default:.2f}')\n",
        "print(f'SVM F1-Score without RFE: {f1_default:.2f}')\n",
        "print(f'SVM Precision with RFE: {precision_rfe:.2f}')\n",
        "print(f'SVM Recall with RFE: {recall_rfe:.2f}')\n",
        "print(f'SVM F1-Score with RFE: {f1_rfe:.2f}')\n",
        "\n",
        "SVM Precision without RFE: 0.95\n",
        "SVM Recall without RFE: 0.99\n",
        "SVM F1-Score without RFE: 0.97\n",
        "SVM Precision with RFE: 0.99\n",
        "SVM Recall with RFE: 0.97\n",
        "SVM F1-Score with RFE: 0.98\n",
        "\n",
        "#42.Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss\n",
        "#(Cross-Entropy Loss)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_prob = nb.predict_proba(X_test)\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "\n",
        "print(f'Na√Øve Bayes Log Loss: {logloss:.4f}')\n",
        "\n",
        "\n",
        "Na√Øve Bayes Log Loss: 0.2037\n",
        "\n",
        "#43.Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#44.Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "#Error (MAE) instead of MSE\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f'SVR Mean Absolute Error: {mae:.2f}')\n",
        "\n",
        "\n",
        "#45.Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "#score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f'Na√Øve Bayes ROC-AUC Score: {roc_auc:.4f}')\n",
        "\n",
        "Na√Øve Bayes ROC-AUC Score: 0.9984\n",
        "\n",
        "#46.Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "y_prob = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(recall, precision, marker='.', label='SVM')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}