{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**\n",
        "\n",
        "\n",
        "1 What does R-squared represent in a regression model?\n",
        "-R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a regression model. It is also known as the coefficient of determination.\n",
        "\n",
        "\n",
        "2  What are the assumptions of linear regression?\n",
        "-Linear regression relies on several key assumptions to ensure the validity and reliability of its estimates. These assumptions are:\n",
        "\n",
        "### **1. Linearity**  \n",
        "- The relationship between the independent variables (predictors) and the dependent variable (outcome) must be **linear**.  \n",
        "- If the relationship is non-linear, linear regression may not be appropriate or could require transformation (e.g., logarithmic transformation).\n",
        "\n",
        "### **2. Independence (No Autocorrelation)**  \n",
        "- The residuals (errors) should be **independent** of each other.  \n",
        "- In time series data, this means no serial correlation (errors at one time point should not be correlated with errors at another time point).  \n",
        "- **Violation test**: Use the **Durbin-Watson test** to check for autocorrelation.\n",
        "\n",
        "### **3. Homoscedasticity (Constant Variance of Errors)**  \n",
        "- The variance of residuals should remain **constant** across all levels of the independent variable(s).  \n",
        "- If variance increases or decreases (heteroscedasticity), standard errors may be biased, leading to unreliable statistical tests.  \n",
        "- **Violation test**: Use a **scatter plot of residuals vs. predicted values** or **Breusch-Pagan test**.\n",
        "\n",
        "### **4. Normality of Residuals**  \n",
        "- The residuals (errors) should be **normally distributed**, especially for small sample sizes.  \n",
        "- This is important for hypothesis testing (e.g., t-tests, confidence intervals).  \n",
        "- **Violation test**: Use a **Q-Q plot**, **histogram of residuals**, or **Shapiro-Wilk test**.\n",
        "\n",
        "### **5. No Multicollinearity**  \n",
        "- Independent variables should not be highly correlated with each other.  \n",
        "- High multicollinearity makes it difficult to determine the effect of each predictor on the dependent variable.  \n",
        "- **Violation test**: Check **Variance Inflation Factor (VIF)**‚Äîa VIF > 5 or 10 suggests problematic multicollinearity.\n",
        "\n",
        "### **6. No Omitted Variable Bias**  \n",
        "- The model should include all relevant predictors; omitting important variables can lead to biased estimates.  \n",
        "- There should be no correlation between the independent variables and omitted factors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3 What is the difference between R-squared and Adjusted R-squared?\n",
        "-### **Difference Between \\(R^2\\) and Adjusted \\(R^2\\)**  \n",
        "\n",
        "| **Metric**          | **Definition** | **Formula** | **Key Difference** |\n",
        "|---------------------|---------------|------------|-------------------|\n",
        "| **\\(R^2\\) (R-Squared)** | Measures the proportion of variance in the dependent variable explained by the independent variables. | \\(\\ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\) | Increases when more predictors are added, even if they don‚Äôt improve the model. |\n",
        "| **Adjusted \\(R^2\\)** | Adjusts \\(R^2\\) by accounting for the number of predictors, preventing overfitting. | \\(\\ Adjusted\\ R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right) \\) | Increases only if a new predictor improves the model; decreases if it does not. |\n",
        "\n",
        "### **Key Differences**  \n",
        "1. **Adjusted \\(R^2\\) Penalizes Extra Variables**  \n",
        "   - \\(R^2\\) always increases when new independent variables are added, even if they don‚Äôt contribute to explaining variance.  \n",
        "   - Adjusted \\(R^2\\) **increases only when the added predictor actually improves the model**.  \n",
        "\n",
        "2. **Use Case**  \n",
        "   - **\\(R^2\\)** is useful for basic understanding of model fit.  \n",
        "   - **Adjusted \\(R^2\\)** is better for comparing models with different numbers of predictors.  \n",
        "\n",
        "3. **Formula Adjustment**  \n",
        "   - Adjusted \\(R^2\\) introduces a penalty based on sample size (\\(n\\)) and the number of predictors (\\(p\\)).  \n",
        "\n",
        "### **When to Use Which?**  \n",
        "- If you just want to see how well the model explains variance, use **\\(R^2\\)**.  \n",
        "- If you are comparing multiple models or avoiding overfitting, use **Adjusted \\(R^2\\)**.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4 Why do we use Mean Squared Error (MSE)?\n",
        "-### **Why Do We Use Mean Squared Error (MSE)?**  \n",
        "\n",
        "Mean Squared Error (MSE) is a widely used metric for evaluating the performance of regression models. It measures the **average squared difference** between actual and predicted values.\n",
        "\n",
        "### **1. Definition & Formula**  \n",
        "\\[\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "\\]\n",
        "Where:  \n",
        "- \\( y_i \\) = Actual value  \n",
        "- \\( \\hat{y}_i \\) = Predicted value  \n",
        "- \\( n \\) = Number of observations  \n",
        "\n",
        "### **2. Reasons for Using MSE**  \n",
        "#### **a) Penalizes Larger Errors More**  \n",
        "- Squaring the errors **amplifies larger deviations**, making the model more sensitive to outliers.\n",
        "- This ensures that models minimize large prediction errors, leading to better performance.\n",
        "\n",
        "#### **b) Differentiable for Optimization**  \n",
        "- The squared function is continuous and differentiable, making it easy to compute gradients for optimization (e.g., in gradient descent for machine learning models).\n",
        "\n",
        "#### **c) Easy to Interpret & Compare**  \n",
        "- MSE provides a simple way to measure how well a model fits the data.\n",
        "- It is widely used in model selection and hyperparameter tuning.\n",
        "\n",
        "### **3. Limitations of MSE**  \n",
        "#### **a) Not in the Same Units as the Target Variable**  \n",
        "- Since MSE squares the errors, its unit is **different** from the original variable.  \n",
        "- Solution: Use **Root Mean Squared Error (RMSE)** to convert it back to the original unit.\n",
        "\n",
        "#### **b) Sensitive to Outliers**  \n",
        "- Large errors get squared, making MSE **highly sensitive to outliers**.\n",
        "- Solution: Use **Mean Absolute Error (MAE)** if robustness to outliers is needed.\n",
        "\n",
        "### **4. Alternatives to MSE**  \n",
        "- **Mean Absolute Error (MAE):** Uses absolute values instead of squares, making it less sensitive to outliers.  \n",
        "- **Root Mean Squared Error (RMSE):** The square root of MSE, making the error units the same as the target variable.  \n",
        "- **Mean Absolute Percentage Error (MAPE):** Expresses error as a percentage of actual values.  \n",
        "\n",
        "\n",
        "\n",
        "5 What does an Adjusted R-squared value of 0.85 indicate?\n",
        "-An **Adjusted R-squared** value of **0.85** indicates that **85% of the variance in the dependent variable is explained by the independent variables** in the regression model, **adjusted for the number of predictors**.  \n",
        "\n",
        "### **Key Interpretations:**\n",
        "1. **Strong Model Fit**  \n",
        "   - Since **Adjusted \\(R^2\\)** accounts for the number of predictors, an **0.85 value suggests a strong fit**, meaning the independent variables explain most of the variability in the target variable.  \n",
        "\n",
        "2. **Better than Simple \\(R^2\\)**  \n",
        "   - Unlike **\\(R^2\\), which always increases with more variables**, **Adjusted \\(R^2\\) only increases if the new variables contribute meaningful explanatory power**.  \n",
        "   - If the Adjusted \\(R^2\\) remains high, it suggests that most of the predictors in the model are relevant.\n",
        "\n",
        "3. **Still Some Unexplained Variance**  \n",
        "   - Since Adjusted \\(R^2\\) is **not 1.0**, about **15% of the variance is still unexplained**, meaning there could be other factors affecting the dependent variable that are not included in the model.\n",
        "\n",
        "### **What Should You Do Next?**\n",
        "- **Check for overfitting**: Even though 0.85 is high, ensure the model is generalizable by testing on unseen data.  \n",
        "- **Evaluate residuals**: Ensure assumptions like normality and homoscedasticity are met.  \n",
        "- **Consider adding/exploring more variables**: If the domain knowledge suggests other relevant predictors, test their impact.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6 How do we check for normality of residuals in linear regression?\n",
        "-### **How to Check for Normality of Residuals in Linear Regression**  \n",
        "The assumption of **normality of residuals** means that the residuals (errors) should follow a normal distribution. This is especially important for valid hypothesis testing and confidence intervals in regression analysis.  \n",
        "\n",
        "### **Methods to Check Normality**  \n",
        "\n",
        "#### **1. Visual Inspection**  \n",
        "üìå *Best for an initial quick check.*  \n",
        "\n",
        "‚úÖ **Histogram of Residuals**  \n",
        "- Plot a histogram of the residuals.  \n",
        "- If the residuals are normally distributed, the histogram should resemble a **bell curve**.  \n",
        "\n",
        "‚úÖ **Q-Q Plot (Quantile-Quantile Plot)**  \n",
        "- Compares the residual quantiles to a normal distribution.  \n",
        "- If the residuals are normal, points should fall along the **45-degree reference line**.  \n",
        "\n",
        "‚úÖ **Box Plot**  \n",
        "- Identifies potential **outliers** that may affect normality.  \n",
        "- Extreme outliers can distort the assumption of normality.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Statistical Tests**  \n",
        "üìå *Useful when you need a formal test rather than a visual check.*  \n",
        "\n",
        "‚úÖ **Shapiro-Wilk Test**  \n",
        "- Null hypothesis (\\(H_0\\)): Residuals follow a normal distribution.  \n",
        "- If \\(p < 0.05\\), reject \\(H_0\\) (residuals are **not** normal).  \n",
        "\n",
        "‚úÖ **Kolmogorov-Smirnov (K-S) Test**  \n",
        "- Tests whether residuals deviate significantly from a normal distribution.  \n",
        "- Works better for **larger datasets**.  \n",
        "\n",
        "‚úÖ **Anderson-Darling Test**  \n",
        "- Similar to Shapiro-Wilk but places more emphasis on the tails of the distribution.  \n",
        "\n",
        "‚úÖ **Jarque-Bera Test**  \n",
        "- Checks for skewness and kurtosis in residuals.  \n",
        "- If the test statistic is **significant**, residuals deviate from normality.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Skewness and Kurtosis**  \n",
        "üìå *Numerical indicators of normality.*  \n",
        "- **Skewness**: Measures symmetry (should be close to 0 for normality).  \n",
        "- **Kurtosis**: Measures the \"tailedness\" of the distribution (should be around 3).  \n",
        "\n",
        "---\n",
        "\n",
        "### **What to Do If Residuals Are Not Normal?**  \n",
        "üîπ **Apply Transformations** (e.g., log, square root, Box-Cox transformation).  \n",
        "üîπ **Check for Outliers** and remove influential points if necessary.  \n",
        "üîπ **Use Robust Regression** if normality is severely violated.  \n",
        "üîπ **Increase Sample Size**, as normality issues may disappear with larger data.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7 What is multicollinearity, and how does it impact regression?\n",
        "-### **What is Multicollinearity?**  \n",
        "**Multicollinearity** occurs when two or more independent variables in a regression model are highly correlated, meaning they provide redundant information. This makes it difficult to determine the individual effect of each predictor on the dependent variable.\n",
        "\n",
        "### **Types of Multicollinearity**\n",
        "1. **Perfect Multicollinearity**  \n",
        "   - When one predictor is a perfect linear combination of another (e.g., \\( X_2 = 2X_1 \\)).  \n",
        "   - The model cannot be estimated in this case.  \n",
        "   \n",
        "2. **High (But Not Perfect) Multicollinearity**  \n",
        "   - When independent variables are strongly correlated but not exact duplicates.  \n",
        "   - This can cause instability in coefficient estimates.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Does Multicollinearity Impact Regression?**\n",
        "1. **Unstable Coefficients**  \n",
        "   - High correlation between predictors makes it difficult for the model to assign the correct contribution to each variable.  \n",
        "   - Small changes in data can lead to large fluctuations in coefficients.\n",
        "\n",
        "2. **Inflated Standard Errors**  \n",
        "   - Standard errors of regression coefficients increase, making them **statistically insignificant** even if they are actually important.\n",
        "\n",
        "3. **Misleading p-values**  \n",
        "   - Due to large standard errors, **p-values may be high**, leading to incorrect conclusions about variable significance.\n",
        "\n",
        "4. **Difficulty in Interpretation**  \n",
        "   - When predictors are highly correlated, it becomes hard to determine their independent effect on the outcome.\n",
        "\n",
        "5. **Overfitting Issues**  \n",
        "   - A model with highly correlated predictors may perform well on training data but poorly on new data.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Detect Multicollinearity?**\n",
        "‚úÖ **Variance Inflation Factor (VIF)**  \n",
        "   - Measures how much variance of a coefficient is inflated due to correlation with other predictors.  \n",
        "   - **Rule of thumb:**  \n",
        "     - **VIF > 5 or 10** indicates high multicollinearity.  \n",
        "     - **VIF ‚âà 1** means no multicollinearity.  \n",
        "\n",
        "‚úÖ **Correlation Matrix**  \n",
        "   - A heatmap or correlation table can reveal high correlations (above **0.7 or 0.8** is concerning).  \n",
        "\n",
        "‚úÖ **Condition Number**  \n",
        "   - A high condition number (\\( > 30 \\)) suggests multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Handle Multicollinearity?**\n",
        "üîπ **Remove One of the Correlated Variables**  \n",
        "   - If two variables convey similar information, keep the more interpretable one.  \n",
        "\n",
        "üîπ **Use Principal Component Analysis (PCA)**  \n",
        "   - Reduces correlated variables into uncorrelated principal components.  \n",
        "\n",
        "üîπ **Combine Variables**  \n",
        "   - Create a new feature that represents both correlated variables (e.g., average or ratio).  \n",
        "\n",
        "üîπ **Use Ridge Regression (L2 Regularization)**  \n",
        "   - Penalizes large coefficients, reducing multicollinearity impact.  \n",
        "\n",
        "\n",
        "\n",
        "8 What is Mean Absolute Error (MAE)?\n",
        "-ChatGPT said:\n",
        "What is Mean Absolute Error (MAE)?\n",
        "Mean Absolute Error (MAE) is a regression metric that measures the average absolute difference between actual and predicted values. It tells us how far predictions are, on average, from the actual values, making it a useful measure of model accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9 What are the benefits of using an ML pipeline?\n",
        "-### **Benefits of Using an ML Pipeline**  \n",
        "\n",
        "A **Machine Learning (ML) pipeline** is a structured way to automate and streamline the ML workflow, from data preprocessing to model deployment. Using an ML pipeline offers several key advantages:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Automation & Efficiency üöÄ**  \n",
        "- Automates repetitive tasks (e.g., data cleaning, feature engineering, model training).  \n",
        "- Reduces manual intervention, saving time and effort.  \n",
        "\n",
        "‚úÖ Example: Instead of manually preprocessing data each time, a pipeline ensures it happens automatically before model training.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Reproducibility üîÑ**  \n",
        "- Ensures consistent results when re-running experiments.  \n",
        "- Eliminates randomness in data transformations and model training.  \n",
        "\n",
        "‚úÖ Example: Using **Scikit-learn Pipelines**, you can ensure the same preprocessing steps are applied every time.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Scalability üìà**  \n",
        "- Easily handles large datasets and complex workflows.  \n",
        "- Can be deployed to production systems seamlessly.  \n",
        "\n",
        "‚úÖ Example: A pipeline can be used for batch processing or real-time inference without changing the core logic.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. Modular & Maintainable Code üèóÔ∏è**  \n",
        "- Breaks down the ML process into **separate, manageable steps** (e.g., data preprocessing, feature selection, model training).  \n",
        "- Makes debugging and updating models easier.  \n",
        "\n",
        "‚úÖ Example: If a feature engineering step needs updating, you can modify just that step without affecting the entire workflow.  \n",
        "\n",
        "---\n",
        "\n",
        "### **5. Hyperparameter Tuning & Experimentation üéØ**  \n",
        "- Pipelines can integrate **automated hyperparameter tuning** (e.g., GridSearchCV, RandomizedSearchCV).  \n",
        "- Ensures that model selection and optimization are done systematically.  \n",
        "\n",
        "‚úÖ Example: Instead of manually testing different parameters, a pipeline can automate hyperparameter tuning.  \n",
        "\n",
        "---\n",
        "\n",
        "### **6. Avoids Data Leakage üö´**  \n",
        "- Ensures that data transformation steps (like scaling or feature selection) are **only learned from training data** and **not influenced by test data**.  \n",
        "- Prevents overfitting and ensures fair evaluation.  \n",
        "\n",
        "‚úÖ Example: If you apply scaling to the entire dataset before splitting, you risk **data leakage**‚Äîa pipeline prevents this by applying transformations only to the training set.  \n",
        "\n",
        "---\n",
        "\n",
        "### **7. Easy Deployment & Integration üè≠**  \n",
        "- Pipelines can be deployed in **production environments** (e.g., cloud, APIs, edge devices).  \n",
        "- Can be integrated with tools like **MLOps**, **CI/CD**, and **monitoring systems**.  \n",
        "\n",
        "‚úÖ Example: A pipeline built with **TensorFlow or Scikit-learn** can be deployed in **AWS, Azure, or Google Cloud** for real-time predictions.  \n",
        "\n",
        "---\n",
        "\n",
        "### **8. Supports Parallel Processing ‚ö°**  \n",
        "- Many pipeline frameworks (e.g., Apache Airflow, Kubeflow) support parallel execution, reducing computation time.  \n",
        "\n",
        "‚úÖ Example: Data preprocessing, feature selection, and model training can run in **parallel**, speeding up ML workflows.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Popular ML Pipeline Tools & Frameworks**  \n",
        "‚úÖ **Scikit-learn Pipelines** (for structured ML workflows)  \n",
        "‚úÖ **TensorFlow Extended (TFX)** (for deep learning pipelines)  \n",
        "‚úÖ **Apache Airflow** (for scheduling and orchestrating ML tasks)  \n",
        "‚úÖ **Kubeflow** (for scalable ML pipelines in Kubernetes)  \n",
        "\n",
        "\n",
        "\n",
        "10 Why is RMSE considered more interpretable than MSE?\n",
        "-### **Why is RMSE More Interpretable Than MSE?**  \n",
        "\n",
        "**Root Mean Squared Error (RMSE)** is often preferred over **Mean Squared Error (MSE)** for interpretation because it is in the **same unit** as the target variable, making it easier to understand in practical terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences Between RMSE and MSE**  \n",
        "\n",
        "| Metric | Formula | Interpretation | Units |\n",
        "|--------|---------|----------------|-------|\n",
        "| **MSE** | \\( MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2 \\) | Measures average squared error but is not in the original scale of the target variable | Squared units (e.g., if target is in dollars, MSE is in dollars¬≤) |\n",
        "| **RMSE** | \\( RMSE = \\sqrt{MSE} \\) | Measures the average error in the same unit as the target variable | Same as the target variable (e.g., dollars) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is RMSE More Interpretable?**  \n",
        "1. **Same Unit as Target Variable**  \n",
        "   - RMSE takes the square root of MSE, bringing the error measurement **back to the original scale of the dependent variable**.  \n",
        "   - Example: If predicting house prices in **dollars**, RMSE will be in **dollars**, while MSE will be in **dollars¬≤**, which is harder to interpret.  \n",
        "\n",
        "2. **Easier to Relate to Actual Errors**  \n",
        "   - Since RMSE represents the **average deviation from actual values**, it directly tells us how far off our predictions are, on average.  \n",
        "   - Example: If RMSE = **5,000 dollars**, it means the model‚Äôs predictions are off by about **$5,000 on average**.  \n",
        "\n",
        "3. **More Comparable Across Models**  \n",
        "   - Since MSE is squared, its magnitude can vary widely, making it harder to compare across models.  \n",
        "   - RMSE provides a more intuitive comparison of model performance.  \n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use RMSE vs. MSE?**  \n",
        "- **Use RMSE** when interpretability is important (e.g., communicating results to stakeholders).  \n",
        "- **Use MSE** when emphasizing large errors (because it squares errors, making large deviations more significant).  \n",
        "\n",
        "\n",
        "\n",
        "11 What is pickling in Python, and how is it useful in ML?\n",
        "-### **What is Pickling in Python?**  \n",
        "**Pickling** is the process of **serializing** (converting) a Python object into a binary format so it can be saved to a file and later **deserialized** (loaded back into memory). This is done using Python‚Äôs built-in `pickle` module.  \n",
        "\n",
        "### **How to Pickle an Object?**\n",
        "```python\n",
        "import pickle\n",
        "\n",
        "# Example object (dictionary)\n",
        "data = {\"name\": \"Alice\", \"age\": 25, \"score\": 90}\n",
        "\n",
        "# Save to file\n",
        "with open(\"data.pkl\", \"wb\") as file:\n",
        "    pickle.dump(data, file)\n",
        "\n",
        "# Load from file\n",
        "with open(\"data.pkl\", \"rb\") as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "print(loaded_data)  # Output: {'name': 'Alice', 'age': 25, 'score': 90}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Pickling Useful in Machine Learning?**  \n",
        "\n",
        "1. **Saving and Loading Trained Models**  \n",
        "   - After training a machine learning model, you can pickle it and reuse it later without retraining.  \n",
        "   - **Example: Save a trained model**  \n",
        "     ```python\n",
        "     import pickle\n",
        "     from sklearn.linear_model import LinearRegression\n",
        "\n",
        "     model = LinearRegression()\n",
        "     model.fit(X_train, y_train)\n",
        "\n",
        "     with open(\"model.pkl\", \"wb\") as file:\n",
        "         pickle.dump(model, file)\n",
        "\n",
        "     # Load the model later\n",
        "     with open(\"model.pkl\", \"rb\") as file:\n",
        "         loaded_model = pickle.load(file)\n",
        "     ```\n",
        "\n",
        "2. **Speeding Up Workflows**  \n",
        "   - Instead of recalculating expensive feature transformations, pickle and reload them.  \n",
        "\n",
        "3. **Sharing and Deployment**  \n",
        "   - You can share trained models with other developers without sharing the raw training data.  \n",
        "   - Useful for **deploying ML models** in applications and APIs.  \n",
        "\n",
        "4. **Storing Preprocessed Data and Features**  \n",
        "   - Avoid reprocessing large datasets by pickling precomputed features.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations of Pickling**  \n",
        "‚ùå **Not Cross-Language Compatible** ‚Äì Pickled objects can only be loaded in Python.  \n",
        "‚ùå **Security Risk** ‚Äì Loading an untrusted pickle file can execute malicious code.  \n",
        "‚ùå **Version Issues** ‚Äì Pickled objects may not be compatible across different Python versions.  \n",
        "\n",
        "‚úÖ **Alternatives:** Use `joblib` for larger ML models or `JSON` for simple data serialization.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12 What does a high R-squared value mean?\n",
        "-### **What Does a High R-Squared Value Mean?**  \n",
        "\n",
        "A **high \\( R^2 \\) (R-squared) value** in a regression model indicates that **a large proportion of the variance in the dependent variable is explained by the independent variables**.  \n",
        "\n",
        "For example:  \n",
        "- \\( R^2 = 0.85 \\) means **85% of the variance** in the target variable is explained by the predictors.  \n",
        "- \\( R^2 = 0.95 \\) means the model explains **95% of the variance**, leaving only 5% unexplained.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation of a High \\( R^2 \\) Value**  \n",
        "\n",
        "‚úÖ **Good Fit (Generally)**  \n",
        "- If \\( R^2 \\) is high, the independent variables **effectively predict the dependent variable**.  \n",
        "- It suggests that the model captures the patterns in the data well.  \n",
        "\n",
        "‚ùó **BUT High \\( R^2 \\) Does NOT Always Mean a Good Model!**  \n",
        "- A high \\( R^2 \\) does not confirm that the model is **correct** or **useful**.  \n",
        "- There could still be **overfitting, multicollinearity, or missing variables**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Things to Watch Out For:**  \n",
        "\n",
        "1. **Overfitting üö®**  \n",
        "   - If \\( R^2 \\) is **too high (near 1.0)**, the model might be memorizing the training data rather than generalizing well.  \n",
        "   - Solution: Check **Adjusted \\( R^2 \\)**, Cross-validation, or Regularization.  \n",
        "\n",
        "2. **Multicollinearity üåÄ**  \n",
        "   - High \\( R^2 \\) with **high VIF (Variance Inflation Factor)** may indicate that predictors are highly correlated, reducing the reliability of coefficients.  \n",
        "\n",
        "3. **Non-Linearity üîÑ**  \n",
        "   - A high \\( R^2 \\) doesn‚Äôt confirm that a linear model is the best choice‚Äîthere could be **non-linear relationships** that the model fails to capture.  \n",
        "\n",
        "4. **Missing Important Variables üîç**  \n",
        "   - A model can have high \\( R^2 \\) but still **miss key predictors** if the right variables aren't included.  \n",
        "\n",
        "---\n",
        "\n",
        "### **When is a High \\( R^2 \\) Truly Good?**  \n",
        "‚úÖ **For Predictive Accuracy:** If the model performs well on both training and test data.  \n",
        "‚úÖ **For Business & Decision Making:** If the model provides useful insights and makes logical sense.  \n",
        "‚úÖ **When Adjusted \\( R^2 \\) is Also High:** This means the predictors are genuinely contributing to the model.  \n",
        "\n",
        "\n",
        "\n",
        "13 What happens if linear regression assumptions are violated?\n",
        "-### **What Happens If Linear Regression Assumptions Are Violated?**  \n",
        "\n",
        "Linear regression relies on several assumptions for accurate and reliable results. If these assumptions are violated, it can lead to **biased estimates, incorrect inferences, and poor model performance**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Linearity Violation üìà**  \n",
        "**Assumption:** The relationship between the independent and dependent variables should be **linear**.  \n",
        "\n",
        "‚ùå **Consequence:**  \n",
        "- The model may underfit the data, leading to poor predictions.  \n",
        "- Residuals (errors) will show a pattern, indicating a non-linear relationship.  \n",
        "\n",
        "‚úÖ **Solution:**  \n",
        "- Use polynomial regression or a non-linear model like decision trees or neural networks.  \n",
        "- Apply transformations (e.g., log, square root) to make relationships more linear.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Independence Violation (Autocorrelation) üîÑ**  \n",
        "**Assumption:** Errors (residuals) should be **independent** of each other.  \n",
        "\n",
        "‚ùå **Consequence:**  \n",
        "- If residuals are correlated (common in time-series data), the model may **underestimate standard errors**, making p-values unreliable.  \n",
        "- Predictions may not generalize well.  \n",
        "\n",
        "‚úÖ **Solution:**  \n",
        "- Check for autocorrelation using the **Durbin-Watson test**.  \n",
        "- If autocorrelation is present, use **time series models** like ARIMA instead of linear regression.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Normality Violation (Non-Normal Residuals) üìä**  \n",
        "**Assumption:** Residuals should be **normally distributed**.  \n",
        "\n",
        "‚ùå **Consequence:**  \n",
        "- Confidence intervals and hypothesis tests (e.g., p-values) may be inaccurate.  \n",
        "- The model may struggle with outliers or skewed distributions.  \n",
        "\n",
        "‚úÖ **Solution:**  \n",
        "- Check normality using a **QQ plot** or **Shapiro-Wilk test**.  \n",
        "- Apply transformations (e.g., log or Box-Cox).  \n",
        "- Use **robust regression** if extreme outliers are an issue.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. Homoscedasticity Violation (Heteroscedasticity) üåä**  \n",
        "**Assumption:** The variance of residuals should be **constant** across all levels of independent variables.  \n",
        "\n",
        "‚ùå **Consequence:**  \n",
        "- Unequal variance (heteroscedasticity) leads to **biased standard errors**, making p-values and confidence intervals unreliable.  \n",
        "- This can result in **incorrect significance tests**.  \n",
        "\n",
        "‚úÖ **Solution:**  \n",
        "- Check for heteroscedasticity using **residual plots** or the **Breusch-Pagan test**.  \n",
        "- Apply transformations (e.g., log transformation).  \n",
        "- Use **weighted least squares regression** instead of ordinary least squares (OLS).  \n",
        "\n",
        "---\n",
        "\n",
        "### **5. Multicollinearity Violation üîÑ**  \n",
        "**Assumption:** Independent variables should not be highly correlated with each other.  \n",
        "\n",
        "‚ùå **Consequence:**  \n",
        "- **Unstable coefficients**‚Äîsmall changes in data lead to large fluctuations in estimated coefficients.  \n",
        "- **Incorrect variable importance**‚Äîthe model struggles to distinguish the effect of each predictor.  \n",
        "\n",
        "‚úÖ **Solution:**  \n",
        "- Detect multicollinearity using the **Variance Inflation Factor (VIF)** (**VIF > 5** is a concern).  \n",
        "- Remove or combine correlated variables.  \n",
        "- Use **regularization techniques** like Ridge Regression (L2 regularization).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table: Consequences & Solutions**  \n",
        "\n",
        "| **Violation**        | **Consequence** | **Solution** |\n",
        "|----------------------|----------------|--------------|\n",
        "| **Non-linearity** | Poor model fit, biased predictions | Use polynomial regression, transformations, or non-linear models |\n",
        "| **Autocorrelation** | Underestimated errors, unreliable predictions | Use time-series models (e.g., ARIMA), check Durbin-Watson test |\n",
        "| **Non-normal residuals** | Invalid hypothesis tests, poor interpretability | Apply transformations (log, Box-Cox), use robust regression |\n",
        "| **Heteroscedasticity** | Biased standard errors, misleading significance tests | Use weighted least squares, check Breusch-Pagan test |\n",
        "| **Multicollinearity** | Unstable coefficients, incorrect variable importance | Check VIF, remove correlated variables, use Ridge Regression |\n",
        "\n",
        "\n",
        "\n",
        "14 How can we address multicollinearity in regression?\n",
        "-### **How to Address Multicollinearity in Regression?**  \n",
        "\n",
        "Multicollinearity occurs when **independent variables are highly correlated**, making it difficult to determine the individual effect of each predictor. It can lead to **unstable coefficients** and **incorrect interpretations**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **How to Detect Multicollinearity?**  \n",
        "1. **Variance Inflation Factor (VIF)** ‚Äì A VIF **greater than 5 or 10** indicates multicollinearity.  \n",
        "   ```python\n",
        "   from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "   import pandas as pd\n",
        "\n",
        "   # Example DataFrame with independent variables\n",
        "   X = df[['feature1', 'feature2', 'feature3']]\n",
        "   \n",
        "   # Calculate VIF for each feature\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"Feature\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "   print(vif)\n",
        "   ```\n",
        "2. **Correlation Matrix** ‚Äì A high correlation (e.g., > 0.8) between two predictors suggests multicollinearity.\n",
        "   ```python\n",
        "   print(df.corr())\n",
        "   ```\n",
        "3. **Eigenvalues of the Design Matrix** ‚Äì If some eigenvalues are very small, it indicates near collinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Fix Multicollinearity?**  \n",
        "\n",
        "#### **1. Remove Highly Correlated Predictors üîç**  \n",
        "- Drop one of the correlated variables.  \n",
        "- Choose the one that is less important based on business knowledge or feature importance.  \n",
        "\n",
        "#### **2. Combine Correlated Features (Feature Engineering) üèóÔ∏è**  \n",
        "- Create a new feature by averaging or applying PCA (Principal Component Analysis).  \n",
        "- Example: If \"Height\" and \"Weight\" are highly correlated, create **BMI = Weight / Height¬≤**.\n",
        "\n",
        "#### **3. Use Regularization (Ridge or Lasso Regression) üèÜ**  \n",
        "- **Ridge Regression (L2 regularization)** reduces multicollinearity by shrinking coefficients.  \n",
        "- **Lasso Regression (L1 regularization)** can **eliminate** less important predictors, reducing redundancy.  \n",
        "   ```python\n",
        "   from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "   ridge = Ridge(alpha=1.0)\n",
        "   ridge.fit(X_train, y_train)\n",
        "\n",
        "   lasso = Lasso(alpha=0.1)\n",
        "   lasso.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "#### **4. Use Principal Component Analysis (PCA) üîÑ**  \n",
        "- PCA reduces correlated features into **uncorrelated principal components**, which can be used in regression.  \n",
        "   ```python\n",
        "   from sklearn.decomposition import PCA\n",
        "\n",
        "   pca = PCA(n_components=2)\n",
        "   X_pca = pca.fit_transform(X)\n",
        "   ```\n",
        "\n",
        "#### **5. Increase Sample Size (If Possible) üìä**  \n",
        "- Sometimes, adding more data can help stabilize coefficient estimates.\n",
        "\n",
        "---\n",
        "\n",
        "### **Best Approach?**  \n",
        "- **If two variables are highly correlated**, remove one.  \n",
        "- **If multicollinearity is widespread**, use **Ridge Regression or PCA**.  \n",
        "- **If interpretability is key**, prefer **Lasso Regression** to remove redundant features.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15 How can feature selection improve model performance in regression analysis?\n",
        "-Feature selection is the process of choosing the most relevant and important features for a regression model while removing redundant or irrelevant ones. It improves accuracy, interpretability, and efficiency.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16  How is Adjusted R-squared calculated?\n",
        "-### **How is Adjusted R-Squared Calculated?**  \n",
        "\n",
        "**Adjusted R-squared (\\( R^2_{adj} \\))** is a modified version of **R-squared (\\( R^2 \\))** that accounts for the number of predictors in the model. Unlike regular \\( R^2 \\), which **always increases** when more variables are added (even if they are irrelevant), **Adjusted \\( R^2 \\) penalizes the inclusion of unnecessary predictors**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Formula for Adjusted R-Squared**  \n",
        "\n",
        "\\[\n",
        "R^2_{adj} = 1 - \\left( \\frac{(1 - R^2) \\times (n - 1)}{n - k - 1} \\right)\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- \\( R^2 \\) = Regular R-squared  \n",
        "- \\( n \\) = Number of observations (sample size)  \n",
        "- \\( k \\) = Number of independent variables (predictors)  \n",
        "\n",
        "---\n",
        "\n",
        "### **How Does It Work?**  \n",
        "‚úÖ **If a new predictor improves the model significantly**, Adjusted \\( R^2 \\) **increases**.  \n",
        "‚ùå **If a new predictor adds little to no value**, Adjusted \\( R^2 \\) **decreases**.  \n",
        "\n",
        "This makes **Adjusted \\( R^2 \\)** a **better metric than regular \\( R^2 \\)** for comparing models with different numbers of predictors.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation**  \n",
        "Suppose:  \n",
        "- \\( R^2 = 0.85 \\)  \n",
        "- \\( n = 100 \\) (observations)  \n",
        "- \\( k = 5 \\) (predictors)  \n",
        "\n",
        "\\[\n",
        "R^2_{adj} = 1 - \\left( \\frac{(1 - 0.85) \\times (100 - 1)}{100 - 5 - 1} \\right)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "R^2_{adj} = 1 - \\left( \\frac{(0.15 \\times 99)}{94} \\right)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "R^2_{adj} = 1 - \\left( \\frac{14.85}{94} \\right)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "R^2_{adj} = 1 - 0.158  \n",
        "\\]\n",
        "\n",
        "\\[\n",
        "R^2_{adj} = 0.842\n",
        "\\]\n",
        "\n",
        "So, the Adjusted \\( R^2 \\) value is **0.842**, which is slightly lower than the original \\( R^2 \\) of 0.85 due to the penalty for additional predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**  \n",
        "- **Adjusted \\( R^2 \\) is always ‚â§ Regular \\( R^2 \\)**.  \n",
        "- **If Adjusted \\( R^2 \\) increases**, adding the predictor **improves the model**.  \n",
        "- **If Adjusted \\( R^2 \\) decreases**, the new predictor **does not contribute much** and should be reconsidered.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "17 Why is MSE sensitive to outliers?\n",
        "-Mean Squared Error (MSE) is sensitive to outliers because it squares the error terms, giving more weight to larger errors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "18 What is the role of homoscedasticity in linear regression?\n",
        "-Homoscedasticity refers to the assumption that the variance of residuals (errors) remains constant across all levels of the independent variable(s). It is a key assumption in ordinary least squares (OLS) regression.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19 What is Root Mean Squared Error (RMSE)?\n",
        "-Root Mean Squared Error (RMSE) is a popular metric for evaluating the performance of a regression model. It measures the average magnitude of the prediction error, giving more weight to large errors due to squaring.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20 Why is pickling considered risky?\n",
        "-Pickling in Python is a process used to serialize and deserialize objects, commonly used in machine learning (ML) models to save and load trained models efficiently. However, pickling is risky due to security vulnerabilities and compatibility issues.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "21 What alternatives exist to pickling for saving ML models?\n",
        "-There are several alternatives to **pickling** for saving machine learning models, depending on your needs for **portability, security, efficiency, and compatibility**. Here are some of the most common alternatives:  \n",
        "\n",
        "### **1. Joblib**  \n",
        "- **Library**: `joblib`  \n",
        "- **Why Use It?**  \n",
        "  - Faster than `pickle` for large NumPy arrays (common in ML models)  \n",
        "  - More efficient **compression** (can use `zlib`, `gzip`, etc.)  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  from joblib import dump, load\n",
        "  dump(model, 'model.joblib')\n",
        "  model = load('model.joblib')\n",
        "  ```  \n",
        "\n",
        "### **2. ONNX (Open Neural Network Exchange)**  \n",
        "- **Library**: `onnx`  \n",
        "- **Why Use It?**  \n",
        "  - **Interoperability** across frameworks (TensorFlow, PyTorch, etc.)  \n",
        "  - Supports hardware acceleration (TensorRT, DirectML)  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  import torch  \n",
        "  import onnx  \n",
        "  torch.onnx.export(model, dummy_input, \"model.onnx\")  \n",
        "  ```  \n",
        "\n",
        "### **3. TensorFlow SavedModel (for TensorFlow/Keras models)**  \n",
        "- **Library**: `tensorflow`  \n",
        "- **Why Use It?**  \n",
        "  - Optimized for **TensorFlow Serving** and **deployment**  \n",
        "  - Saves entire computation graph (better than HDF5 for TF models)  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  model.save(\"model_directory\")  # Save\n",
        "  model = tf.keras.models.load_model(\"model_directory\")  # Load\n",
        "  ```  \n",
        "\n",
        "### **4. HDF5 (Hierarchical Data Format)**\n",
        "- **Library**: `h5py`, `keras`  \n",
        "- **Why Use It?**  \n",
        "  - Structured storage for large datasets  \n",
        "  - Commonly used for Keras models (`.h5` format)  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  model.save(\"model.h5\")  \n",
        "  from tensorflow.keras.models import load_model  \n",
        "  model = load_model(\"model.h5\")  \n",
        "  ```  \n",
        "\n",
        "### **5. PMML (Predictive Model Markup Language)**  \n",
        "- **Library**: `sklearn2pmml`  \n",
        "- **Why Use It?**  \n",
        "  - XML-based, used for **model deployment in enterprise systems**  \n",
        "  - Works with **Java-based scoring engines**  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  from sklearn2pmml import sklearn2pmml\n",
        "  sklearn2pmml(pipeline, \"model.pmml\")\n",
        "  ```  \n",
        "\n",
        "### **6. JSON (for lightweight models)**  \n",
        "- **Library**: `json`  \n",
        "- **Why Use It?**  \n",
        "  - Human-readable  \n",
        "  - Useful for **saving model parameters** but **not weights**  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  import json  \n",
        "  with open(\"model.json\", \"w\") as f:\n",
        "      json.dump(model.get_params(), f)\n",
        "  ```  \n",
        "\n",
        "### **7. TorchScript (for PyTorch models)**  \n",
        "- **Library**: `torch`  \n",
        "- **Why Use It?**  \n",
        "  - Converts PyTorch models into an **optimized, deployable format**  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  scripted_model = torch.jit.script(model)\n",
        "  scripted_model.save(\"model.pt\")\n",
        "  ```  \n",
        "\n",
        "### **8. MLflow (for Experiment Tracking and Model Storage)**  \n",
        "- **Library**: `mlflow`  \n",
        "- **Why Use It?**  \n",
        "  - Tracks **versions** of models  \n",
        "  - Easily deployable with **MLflow Serving**  \n",
        "- **Usage**:  \n",
        "  ```python\n",
        "  import mlflow  \n",
        "  mlflow.sklearn.save_model(model, \"model_path\")\n",
        "  model = mlflow.sklearn.load_model(\"model_path\")\n",
        "  ```  \n",
        "\n",
        "### **Which One to Choose?**  \n",
        "| **Use Case** | **Best Alternative** |  \n",
        "|-------------|----------------|  \n",
        "| Large NumPy models | Joblib |  \n",
        "| Cross-framework (TF, PyTorch) | ONNX |  \n",
        "| TensorFlow/Keras models | SavedModel / HDF5 |  \n",
        "| Enterprise deployment | PMML |  \n",
        "| PyTorch models | TorchScript |  \n",
        "| Lightweight model metadata | JSON |  \n",
        "| Experiment tracking | MLflow |  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "22 What is heteroscedasticity, and why is it a problem?\n",
        "-Heteroscedasticity refers to a situation in which the variance of the errors (residuals) in a regression model is not constant across all levels of an independent variable. In other words, some observations have much larger or smaller error terms than others, leading to an unequal spread of residuals\n",
        "\n",
        "\n",
        "Why is Heteroscedasticity a Problem?\n",
        "Violates OLS Assumption\n",
        "\n",
        "Ordinary Least Squares (OLS) regression assumes homoscedasticity (constant variance).\n",
        "When heteroscedasticity is present, OLS estimates remain unbiased, but they are no longer efficient (i.e., they don‚Äôt have the smallest variance).\n",
        "Affects Standard Errors\n",
        "\n",
        "Since variance is not constant, the standard errors of regression coefficients are misestimated.\n",
        "This can lead to incorrect p-values and misleading hypothesis tests (e.g., a coefficient might appear significant when it's not).\n",
        "Reduces Model Reliability\n",
        "\n",
        "Predictions from a model with heteroscedasticity may be unreliable.\n",
        "It indicates that the model is not capturing some important relationships in the data.\n",
        "Violates Assumption for Confidence Intervals\n",
        "\n",
        "Confidence intervals around estimates become incorrect, affecting decision-making.\n",
        "\n",
        "\n",
        "\n",
        "23  How can interaction terms enhance a regression model's predictive power?\n",
        "-### **How Interaction Terms Enhance a Regression Model's Predictive Power**  \n",
        "\n",
        "#### **What Are Interaction Terms?**  \n",
        "Interaction terms in regression **capture the combined effect of two or more independent variables on the dependent variable**. They allow the relationship between an independent variable and the dependent variable to change depending on the value of another independent variable.\n",
        "\n",
        "**Formula for a Basic Interaction Term in Multiple Linear Regression:**  \n",
        "\\[\n",
        "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\epsilon\n",
        "\\]\n",
        "Where:\n",
        "- \\( X_1 \\) and \\( X_2 \\) are independent variables  \n",
        "- \\( X_1 \\times X_2 \\) is the interaction term  \n",
        "- \\( \\beta_3 \\) measures how the effect of \\( X_1 \\) on \\( Y \\) changes based on \\( X_2 \\)  \n",
        "\n",
        "---\n",
        "\n",
        "### **How Do Interaction Terms Improve Predictive Power?**  \n",
        "\n",
        "1. **Capture Non-Additive Relationships**  \n",
        "   - Standard regression assumes **additive effects**, meaning that the effect of one variable on the outcome is the same regardless of the other variables.  \n",
        "   - Interaction terms allow for **synergistic or antagonistic effects** that would be missed in a simple linear model.  \n",
        "\n",
        "   **Example:**  \n",
        "   - Suppose you're studying **salary (\\$Y\\$)** based on **education (\\$X_1\\$)** and **experience (\\$X_2\\$)**.  \n",
        "   - The effect of education might be **larger for experienced workers** than for fresh graduates.  \n",
        "   - Adding an interaction term **(education √ó experience)** captures this relationship.  \n",
        "\n",
        "---\n",
        "\n",
        "2. **Improve Model Fit and Accuracy**  \n",
        "   - Including interactions **reduces bias** by better modeling real-world complexities.  \n",
        "   - Results in **lower residual variance** and better **R¬≤ (goodness of fit)**.  \n",
        "\n",
        "   **Example:**  \n",
        "   - If your residual plot shows systematic patterns (not just random scatter), an interaction term might help explain missing relationships.  \n",
        "\n",
        "---\n",
        "\n",
        "3. **Enable More Precise Interpretations**  \n",
        "   - Interaction terms **clarify conditional relationships**, making results more actionable.  \n",
        "   - Helps understand **when** and **under what conditions** an independent variable has a stronger/weaker effect.  \n",
        "\n",
        "   **Example:**  \n",
        "   - If you study **advertising (\\$X_1\\$)** and **brand loyalty (\\$X_2\\$)** on sales (\\$Y\\$), an interaction can reveal:  \n",
        "     - Advertising might work **better for new customers** but **worse for loyal customers**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **How to Create Interaction Terms in Python?**  \n",
        "You can create interaction terms manually or using `PolynomialFeatures` from `sklearn`.  \n",
        "\n",
        "#### **Method 1: Manually Adding Interaction Terms**  \n",
        "```python\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample dataset\n",
        "df = pd.DataFrame({\n",
        "    'X1': [1, 2, 3, 4, 5],  # Education (years)\n",
        "    'X2': [10, 12, 15, 18, 20],  # Experience (years)\n",
        "    'Y': [50, 60, 70, 85, 100]  # Salary ($K)\n",
        "})\n",
        "\n",
        "# Add interaction term\n",
        "df['X1_X2'] = df['X1'] * df['X2']\n",
        "\n",
        "# Fit regression model\n",
        "X = sm.add_constant(df[['X1', 'X2', 'X1_X2']])  # Adding constant for intercept\n",
        "y = df['Y']\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "print(model.summary())\n",
        "```\n",
        "\n",
        "#### **Method 2: Using `PolynomialFeatures` from `sklearn`**\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 10], [2, 12], [3, 15], [4, 18], [5, 20]])  # Education & Experience\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_interaction = poly.fit_transform(X)\n",
        "\n",
        "print(X_interaction)  # Columns: X1, X2, X1*X2\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Interaction Terms?**  \n",
        "‚úÖ When there‚Äôs a **logical reason** that one variable modifies another‚Äôs effect.  \n",
        "‚úÖ When residual plots show **non-random patterns** suggesting missing relationships.  \n",
        "‚úÖ When **domain knowledge** suggests a **multiplicative** or **conditional** effect.  \n",
        "\n",
        "‚ö† **Avoid Overfitting!**  \n",
        "- Too many interaction terms can make the model **complex and hard to interpret**.  \n",
        "- Use **feature selection techniques** (e.g., stepwise regression, Lasso) to choose relevant interactions.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**  \n",
        "‚úî Interaction terms **improve predictive accuracy** by capturing **real-world dependencies**.  \n",
        "‚úî They allow for **non-additive effects**, making models more **flexible and interpretable**.  \n",
        "‚úî Use interaction terms **judiciously** to avoid overfitting.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JyLSqyXd55rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "**Practical:**\n",
        "\n",
        "\n",
        "\n",
        "1 Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model\n",
        "using Seaborn's \"diamonds\" dataset\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Load the diamonds dataset\n",
        "df = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Select features and target variable\n",
        "X = df[['carat', 'depth', 'table', 'x', 'y', 'z']]  # Predictor variables\n",
        "y = df['price']  # Target variable\n",
        "\n",
        "# Handle any potential infinite values or missing data\n",
        "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y = y[X.index]  # Ensure y matches the cleaned X indices\n",
        "\n",
        "# Split data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for linear regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit the Multiple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Compute residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# --- PLOT 1: Histogram of Residuals ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(residuals, bins=50, kde=True, color=\"royalblue\")\n",
        "plt.axvline(x=0, color='red', linestyle='dashed', linewidth=2)  # Reference line at zero\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid()\n",
        "\n",
        "# --- PLOT 2: Q-Q Plot (Normality Check) ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "sm.qqplot(residuals, line=\"s\", fit=True)\n",
        "plt.title(\"Q-Q Plot of Residuals\")\n",
        "plt.grid()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2  Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root\n",
        "Mean Squared Error (RMSE) for a linear regression model.\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Select features and target variable\n",
        "X = df[['carat', 'depth', 'table', 'x', 'y', 'z']]  # Predictor variables\n",
        "y = df['price']  # Target variable\n",
        "\n",
        "# Handle missing or infinite values (if any)\n",
        "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y = y[X.index]  # Ensure y matches the cleaned X indices\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "3  Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check\n",
        "linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Select relevant features and target variable\n",
        "X = df[['carat', 'depth', 'table', 'x', 'y', 'z']]  # Predictor variables\n",
        "y = df['price']  # Target variable\n",
        "\n",
        "# Handle missing/infinite values\n",
        "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y = y[X.index]  # Ensure y matches cleaned X indices\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# --- 1. LINEARITY CHECK: Scatter plot of actual vs. predicted values ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5, color=\"blue\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\"red\", linestyle=\"dashed\")  # 45-degree line\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Linearity Check: Actual vs. Predicted Prices\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# --- 2. HOMOSCEDASTICITY CHECK: Residuals plot ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(x=y_pred, y=residuals, alpha=0.5, color=\"purple\")\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"dashed\")  # Reference line at zero\n",
        "plt.xlabel(\"Predicted Prices\")\n",
        "plt.ylabel(\"Residuals (Errors)\")\n",
        "plt.title(\"Homoscedasticity Check: Residuals vs. Predicted Values\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# --- 3. MULTICOLLINEARITY CHECK: Correlation Matrix ---\n",
        "corr_matrix = pd.DataFrame(X_train_scaled, columns=X.columns).corr()\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Multicollinearity Check: Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# --- 4. MULTICOLLINEARITY CHECK: Variance Inflation Factor (VIF) ---\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_scaled, i) for i in range(X_train_scaled.shape[1])]\n",
        "print(\"\\nVariance Inflation Factor (VIF) for Multicollinearity Check:\\n\", vif_data)\n",
        "\n",
        "\n",
        "\n",
        "4 Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the\n",
        "performance of different regression models\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Select features and target variable\n",
        "X = df[['carat', 'depth', 'table', 'x', 'y', 'z']]  # Predictor variables\n",
        "y = df['price']  # Target variable\n",
        "\n",
        "# Handle missing/infinite values\n",
        "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y = y[X.index]  # Ensure y matches cleaned X indices\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different regression models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
        "    \"Lasso Regression\": Lasso(alpha=0.1),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(max_depth=10),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluate each model\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Create a pipeline with feature scaling and model training\n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"regressor\", model)\n",
        "    ])\n",
        "\n",
        "    # Train model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results.append([name, r2, mse, rmse, mae])\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"R¬≤ Score\", \"MSE\", \"RMSE\", \"MAE\"])\n",
        "\n",
        "# Sort by R¬≤ Score\n",
        "results_df = results_df.sort_values(by=\"R¬≤ Score\", ascending=False)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nPerformance of Regression Models:\\n\")\n",
        "print(results_df)\n",
        "\n",
        "# Plot model comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=results_df, x=\"Model\", y=\"R¬≤ Score\", palette=\"coolwarm\")\n",
        "plt.title(\"Model Performance Comparison (Higher R¬≤ is Better)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5  Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and\n",
        "R-squared score\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Select one feature (carat) and target variable (price)\n",
        "X = df[['carat']]  # Predictor (independent variable)\n",
        "y = df['price']    # Target (dependent variable)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Simple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get model parameters\n",
        "slope = model.coef_[0]   # Coefficient (slope)\n",
        "intercept = model.intercept_  # Intercept\n",
        "r2 = model.score(X_test, y_test)  # R-squared score\n",
        "\n",
        "# Print results\n",
        "print(f\"Coefficient (Slope): {slope:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- Visualization: Regression Line ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=X_test['carat'], y=y_test, color=\"blue\", alpha=0.5, label=\"Actual\")\n",
        "sns.lineplot(x=X_test['carat'], y=y_pred, color=\"red\", label=\"Regression Line\")\n",
        "plt.xlabel(\"Carat\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Simple Linear Regression: Carat vs. Price\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6 Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using\n",
        "simple linear regression and visualizes the results.\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Select feature (total_bill) and target variable (tip)\n",
        "X = df[['total_bill']]  # Independent variable\n",
        "y = df['tip']  # Dependent variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Simple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get model parameters\n",
        "slope = model.coef_[0]  # Coefficient (slope)\n",
        "intercept = model.intercept_  # Intercept\n",
        "r2 = model.score(X_test, y_test)  # R-squared score\n",
        "\n",
        "# Print results\n",
        "print(f\"Coefficient (Slope): {slope:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- Visualization: Regression Line ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=X_test['total_bill'], y=y_test, color=\"blue\", alpha=0.5, label=\"Actual Tips\")\n",
        "sns.lineplot(x=X_test['total_bill'], y=y_pred, color=\"red\", label=\"Regression Line\")\n",
        "plt.xlabel(\"Total Bill ($)\")\n",
        "plt.ylabel(\"Tip ($)\")\n",
        "plt.title(\"Simple Linear Regression: Total Bill vs. Tip\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7 Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the\n",
        "model to predict new values and plot the data points along with the regression line\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Generate Synthetic Dataset ---\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # 100 random values between 0 and 2\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relation (y = 4 + 3X) with noise\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train Simple Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get model parameters\n",
        "slope = model.coef_[0][0]  # Coefficient (slope)\n",
        "intercept = model.intercept_[0]  # Intercept\n",
        "r2 = model.score(X_test, y_test)  # R¬≤ Score\n",
        "\n",
        "# Print results\n",
        "print(f\"Coefficient (Slope): {slope:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "\n",
        "# --- 3. Make Predictions ---\n",
        "X_new = np.array([[0], [2]])  # Predict for X = 0 and X = 2\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# --- 4. Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Scatter plot of actual data\n",
        "plt.plot(X_new, y_pred, color=\"red\", linewidth=2, label=\"Regression Line\")  # Regression line\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Simple Linear Regression on Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8 Write a Python script that pickles a trained linear regression model and saves it to a file.\n",
        "-import numpy as np\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train a Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 3. Pickle the Model ---\n",
        "filename = \"model.pkl\"\n",
        "with open(filename, \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"Model saved to {filename}\")\n",
        "\n",
        "# --- 4. Load the Pickled Model ---\n",
        "with open(filename, \"rb\") as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# --- 5. Make Predictions Using the Loaded Model ---\n",
        "X_new = np.array([[0], [2]])  # Predict for X = 0 and X = 2\n",
        "y_pred = loaded_model.predict(X_new)\n",
        "\n",
        "# Print predictions\n",
        "print(f\"Predictions: {y_pred.flatten()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9  Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the\n",
        "regression curve\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 6 * np.random.rand(100, 1) - 3  # Random values between -3 and 3\n",
        "y = 2 + X - 0.5 * X**2 + np.random.randn(100, 1)  # Quadratic relationship with noise\n",
        "\n",
        "# Sort X for better plotting\n",
        "X_sorted = np.sort(X, axis=0)\n",
        "y_sorted = y[np.argsort(X, axis=0).flatten()]\n",
        "\n",
        "# --- 2. Train a Polynomial Regression Model (Degree 2) ---\n",
        "degree = 2\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "model.fit(X, y)\n",
        "\n",
        "# --- 3. Make Predictions ---\n",
        "y_pred = model.predict(X_sorted)\n",
        "\n",
        "# --- 4. Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Original data\n",
        "plt.plot(X_sorted, y_pred, color=\"red\", linewidth=2, label=\"Polynomial Regression Curve\")  # Regression curve\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(f\"Polynomial Regression (Degree {degree})\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10 Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear\n",
        "regression model to the data. Print the model's coefficient and intercep\n",
        "-import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # 100 random values between 0 and 2\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relation (y = 4 + 3X) with some noise\n",
        "\n",
        "# --- 2. Train a Simple Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# --- 3. Print Model Parameters ---\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11  Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and\n",
        "compares their performance\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 6 * np.random.rand(100, 1) - 3  # Random values between -3 and 3\n",
        "y = 2 + X - 0.5 * X**2 + np.random.randn(100, 1)  # Quadratic relationship with noise\n",
        "\n",
        "# Sort X for better plotting\n",
        "X_sorted = np.sort(X, axis=0)\n",
        "y_sorted = y[np.argsort(X, axis=0).flatten()]\n",
        "\n",
        "# --- 2. Fit Polynomial Regression Models and Compare ---\n",
        "degrees = [1, 2, 3, 4, 5]  # Different polynomial degrees\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create a pipeline with PolynomialFeatures and LinearRegression\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predict values\n",
        "    y_pred = model.predict(X_sorted)\n",
        "\n",
        "    # Compute performance metrics\n",
        "    r2 = r2_score(y_sorted, y_pred)\n",
        "    mse = mean_squared_error(y_sorted, y_pred)\n",
        "    mae = mean_absolute_error(y_sorted, y_pred)\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(f\"Degree {degree}: R¬≤={r2:.3f}, MSE={mse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    # Plot regression curve\n",
        "    plt.plot(X_sorted, y_pred, label=f\"Degree {degree} (R¬≤={r2:.2f})\")\n",
        "\n",
        "# --- 3. Visualization ---\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Original data\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Polynomial Regression Comparison\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12 Write a Python script that fits a simple linear regression model with two features and prints the model's\n",
        "coefficients, intercept, and R-squared score\n",
        "-import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X1 = 2 * np.random.rand(100, 1)  # Feature 1\n",
        "X2 = 3 * np.random.rand(100, 1)  # Feature 2\n",
        "y = 5 + 2 * X1 + 3 * X2 + np.random.randn(100, 1)  # Linear relation with noise\n",
        "\n",
        "# Combine features into a single matrix\n",
        "X = np.hstack((X1, X2))\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train a Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 3. Print Model Parameters ---\n",
        "print(f\"Coefficients (Slopes): {model.coef_.flatten()}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n",
        "print(f\"R-squared Score: {model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "13  Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the\n",
        "regression line along with the data points\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # Feature values (100 random numbers between 0 and 2)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with noise\n",
        "\n",
        "# --- 2. Train a Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# --- 3. Make Predictions ---\n",
        "X_new = np.array([[0], [2]])  # Predict for X=0 and X=2\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# --- 4. Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Scatter plot of data\n",
        "plt.plot(X_new, y_pred, color=\"red\", linewidth=2, label=\"Regression Line\")  # Regression line\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# --- 5. Print Model Parameters ---\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "14 Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset\n",
        "with multiple features\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# --- 1. Generate Synthetic Data with Multiple Features ---\n",
        "np.random.seed(42)\n",
        "X1 = 2 * np.random.rand(100, 1)  # Feature 1\n",
        "X2 = 3 * X1 + np.random.randn(100, 1) * 0.1  # Feature 2 (highly correlated with X1)\n",
        "X3 = 5 * np.random.rand(100, 1)  # Feature 3 (less correlated)\n",
        "y = 4 + 2 * X1 + 3 * X2 + 1.5 * X3 + np.random.randn(100, 1)  # Target variable\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(np.hstack([X1, X2, X3]), columns=[\"X1\", \"X2\", \"X3\"])\n",
        "\n",
        "# --- 2. Compute VIF for Each Feature ---\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = df.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "\n",
        "# --- 3. Print VIF Results ---\n",
        "print(\"Variance Inflation Factor (VIF) for Each Feature:\")\n",
        "print(vif_data)\n",
        "\n",
        "\n",
        "\n",
        "15  Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a\n",
        "polynomial regression model, and plots the regression curve\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 6 * np.random.rand(100, 1) - 3  # Random values between -3 and 3\n",
        "y = 2 + 1.5 * X - 0.8 * X**2 + 0.5 * X**3 - 0.2 * X**4 + np.random.randn(100, 1) * 3  # Degree 4 polynomial with noise\n",
        "\n",
        "# Sort X for better plotting\n",
        "X_sorted = np.sort(X, axis=0)\n",
        "y_sorted = y[np.argsort(X, axis=0).flatten()]\n",
        "\n",
        "# --- 2. Fit a Polynomial Regression Model (Degree 4) ---\n",
        "degree = 4\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_sorted)\n",
        "\n",
        "# --- 3. Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Scatter plot of data\n",
        "plt.plot(X_sorted, y_pred, color=\"red\", linewidth=2, label=f\"Polynomial Regression (Degree {degree})\")  # Regression curve\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Polynomial Regression (Degree 4)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16  Write a Python script that creates a machine learning pipeline with data standardization and a multiple\n",
        "linear regression model, and prints the R-squared score\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X1 = 2 * np.random.rand(100, 1)  # Feature 1\n",
        "X2 = 3 * np.random.rand(100, 1)  # Feature 2\n",
        "X3 = 4 * np.random.rand(100, 1)  # Feature 3\n",
        "y = 5 + 2 * X1 + 3 * X2 + 1.5 * X3 + np.random.randn(100, 1)  # Linear relationship with noise\n",
        "\n",
        "# Combine features into a single matrix\n",
        "X = np.hstack([X1, X2, X3])\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Create a Machine Learning Pipeline ---\n",
        "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# --- 3. Print Model Performance ---\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "17 Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the\n",
        "regression curve\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 6 * np.random.rand(100, 1) - 3  # Random values between -3 and 3\n",
        "y = 2 + 1.5 * X - 0.8 * X**2 + 0.5 * X**3 + np.random.randn(100, 1) * 3  # Cubic relationship with noise\n",
        "\n",
        "# Sort X for better plotting\n",
        "X_sorted = np.sort(X, axis=0)\n",
        "y_sorted = y[np.argsort(X, axis=0).flatten()]\n",
        "\n",
        "# --- 2. Fit a Polynomial Regression Model (Degree 3) ---\n",
        "degree = 3\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_sorted)\n",
        "\n",
        "# --- 3. Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Scatter plot of data\n",
        "plt.plot(X_sorted, y_pred, color=\"red\", linewidth=2, label=f\"Polynomial Regression (Degree {degree})\")  # Regression curve\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Polynomial Regression (Degree 3)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "18 Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print\n",
        "the R-squared score and model coefficients\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "n_samples = 100  # Number of samples\n",
        "n_features = 5   # Number of features\n",
        "\n",
        "X = np.random.rand(n_samples, n_features) * 10  # Features scaled between 0 and 10\n",
        "true_coeffs = np.array([2.5, -1.2, 3.8, 0.5, -2.0])  # True coefficients\n",
        "y = 5 + X.dot(true_coeffs) + np.random.randn(n_samples) * 2  # Linear relationship with noise\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train a Multiple Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- 3. Print Model Performance ---\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "\n",
        "\n",
        "\n",
        "19 Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the\n",
        "data points along with the regression line.\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # Random feature values between 0 and 2\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with noise\n",
        "\n",
        "# --- 2. Train a Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# --- 3. Make Predictions ---\n",
        "X_new = np.array([[0], [2]])  # Predict for X=0 and X=2\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# --- 4. Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Data Points\")  # Scatter plot of data\n",
        "plt.plot(X_new, y_pred, color=\"red\", linewidth=2, label=\"Regression Line\")  # Regression line\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# --- 5. Print Model Parameters ---\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20 Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "n_samples = 100  # Number of samples\n",
        "\n",
        "X1 = 2 * np.random.rand(n_samples, 1)  # Feature 1\n",
        "X2 = 3 * np.random.rand(n_samples, 1)  # Feature 2\n",
        "X3 = 4 * np.random.rand(n_samples, 1)  # Feature 3\n",
        "\n",
        "# Create the target variable (linear combination of features + noise)\n",
        "true_coeffs = np.array([2.5, -1.2, 3.8])  # True coefficients\n",
        "y = 5 + (X1 * true_coeffs[0] + X2 * true_coeffs[1] + X3 * true_coeffs[2]).flatten() + np.random.randn(n_samples) * 2\n",
        "\n",
        "# Combine features into a single matrix\n",
        "X = np.hstack([X1, X2, X3])\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train a Multiple Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- 3. Print Model Performance ---\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "21 Write a Python script that demonstrates how to serialize and deserialize machine learning models using\n",
        "joblib instead of pickling\n",
        "-import numpy as np\n",
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # Random feature values\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with noise\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train a Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 3. Serialize (Save) the Model Using joblib ---\n",
        "joblib.dump(model, \"linear_regression_model.pkl\")\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# --- 4. Deserialize (Load) the Model ---\n",
        "loaded_model = joblib.load(\"linear_regression_model.pkl\")\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# --- 5. Make Predictions Using the Loaded Model ---\n",
        "sample_input = np.array([[1.5]])  # Example input\n",
        "predicted_value = loaded_model.predict(sample_input)\n",
        "print(f\"Prediction for input {sample_input.flatten()[0]}: {predicted_value.flatten()[0]:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "22 Write a Python script to perform linear regression with categorical features using one-hot encoding. Use\n",
        "the Seaborn 'tips' dataset.\n",
        "-import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# --- 1. Load the 'tips' Dataset ---\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# --- 2. Select Features and Target ---\n",
        "categorical_features = [\"sex\", \"smoker\", \"day\", \"time\"]  # Categorical columns\n",
        "numerical_features = [\"total_bill\", \"size\"]  # Numerical columns\n",
        "target = \"tip\"\n",
        "\n",
        "# Separate features and target variable\n",
        "X = tips[numerical_features + categorical_features]\n",
        "y = tips[target]\n",
        "\n",
        "# --- 3. Perform One-Hot Encoding for Categorical Features ---\n",
        "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)  # Avoid dummy variable trap\n",
        "\n",
        "# --- 4. Split Data into Training and Testing Sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 5. Train a Multiple Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 6. Make Predictions ---\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- 7. Print Model Performance ---\n",
        "r2 = model.score(X_test, y_test)  # R-squared score\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "print(f\"Coefficients: {dict(zip(X.columns, model.coef_))}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "23 Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and Rsquared score\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "n_samples = 100  # Number of samples\n",
        "n_features = 5   # Number of features\n",
        "\n",
        "X = np.random.rand(n_samples, n_features) * 10  # Features scaled between 0 and 10\n",
        "true_coeffs = np.array([3, -2, 1.5, 0, -1])  # True coefficients\n",
        "y = 5 + X.dot(true_coeffs) + np.random.randn(n_samples) * 5  # Linear relationship with noise\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 2. Train Linear Regression ---\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "y_pred_lin = lin_reg.predict(X_test)\n",
        "r2_lin = r2_score(y_test, y_pred_lin)\n",
        "\n",
        "# --- 3. Train Ridge Regression (L2 Regularization) ---\n",
        "ridge_reg = Ridge(alpha=1.0)  # Regularization strength\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_reg.predict(X_test)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "# --- 4. Print Model Performance ---\n",
        "print(\"Linear Regression Results:\")\n",
        "print(f\"R-squared Score: {r2_lin:.4f}\")\n",
        "print(f\"Coefficients: {lin_reg.coef_}\\n\")\n",
        "\n",
        "print(\"Ridge Regression Results:\")\n",
        "print(f\"R-squared Score: {r2_ridge:.4f}\")\n",
        "print(f\"Coefficients: {ridge_reg.coef_}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "24 Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic\n",
        "dataset.\n",
        "-import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "np.random.seed(42)\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=10)  # 200 samples, 5 features, noise added\n",
        "\n",
        "# --- 2. Initialize Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "\n",
        "# --- 3. Perform Cross-Validation (Using 5 Folds) ---\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')  # R-squared as evaluation metric\n",
        "\n",
        "# --- 4. Print Model Performance ---\n",
        "print(f\"Cross-Validation R-squared Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared Score: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "25 Write a Python script that compares polynomial regression models of different degrees and prints the Rsquared score for each.\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- 1. Generate Synthetic Data (Polynomial Relationship) ---\n",
        "np.random.seed(42)\n",
        "X = 6 * np.random.rand(100, 1) - 3  # Random values between -3 and 3\n",
        "y = 2 + 1.5 * X - 0.8 * X**2 + 0.3 * X**3 + np.random.randn(100, 1) * 2  # Cubic relationship with noise\n",
        "\n",
        "# --- 2. Compare Polynomial Regression Models ---\n",
        "degrees = [1, 2, 3, 4, 5]  # Degrees of polynomial to test\n",
        "r2_scores = []\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(X, y, color=\"gray\", alpha=0.5, label=\"Data Points\")  # Plot raw data\n",
        "\n",
        "X_test = np.linspace(-3, 3, 100).reshape(-1, 1)  # For plotting smooth curves\n",
        "\n",
        "for degree in degrees:\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())  # Pipeline with PolynomialFeatures\n",
        "    model.fit(X, y)\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "    r2 = r2_score(y, y_pred)  # Compute R-squared score\n",
        "    r2_scores.append((degree, r2))\n",
        "\n",
        "    # Plot regression curve\n",
        "    plt.plot(X_test, model.predict(X_test), label=f\"Degree {degree}\")\n",
        "\n",
        "# --- 3. Print R-squared Scores ---\n",
        "print(\"Polynomial Regression Performance:\")\n",
        "for degree, score in r2_scores:\n",
        "    print(f\"Degree {degree}: R-squared Score = {score:.4f}\")\n",
        "\n",
        "# --- 4. Final Plot Formatting ---\n",
        "plt.xlabel(\"Feature (X)\")\n",
        "plt.ylabel(\"Target (y)\")\n",
        "plt.title(\"Polynomial Regression Comparison\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WydE8mr7_0la"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}