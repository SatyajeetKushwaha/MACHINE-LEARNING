{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "           # THEORY — KNN & PCA\n",
        "\n",
        "## 1) What is K-Nearest Neighbors (KNN) and how does it work\n",
        "\n",
        "KNN is a non-parametric, instance-based algorithm used for classification and regression. To predict for a new sample, KNN:\n",
        "\n",
        "1. Computes distances between the new sample and each training sample (using a chosen metric).\n",
        "2. Selects the K nearest training samples.\n",
        "3. For classification: majority vote among neighbors (or weighted vote). For regression: average (or weighted average) of neighbors’ targets.\n",
        "\n",
        "## 2) Difference between KNN Classification and KNN Regression\n",
        "\n",
        "* Classification: target is discrete labels. Prediction = mode of neighbors.\n",
        "* Regression: target is continuous. Prediction = mean (or weighted mean) of neighbors’ target values.\n",
        "\n",
        "## 3) Role of the distance metric in KNN\n",
        "\n",
        "Distance metric defines “nearness”. Common metrics: Euclidean (L2), Manhattan (L1), Minkowski, Cosine. Choice affects which points are neighbors — thus model performance and decision boundaries.\n",
        "\n",
        "## 4) The Curse of Dimensionality in KNN\n",
        "\n",
        "High dimensional spaces make distances less informative — points become equidistant, neighborhoods lose meaning, leading to poor KNN performance. Dimensionality reduction or feature selection helps.\n",
        "\n",
        "## 5) How to choose the best value of K in KNN\n",
        "\n",
        "Typical approach: cross-validation (k-fold CV) to evaluate accuracy (or MSE) for multiple K values; choose K with best CV performance. Also consider odd K to avoid ties (binary classification).\n",
        "\n",
        "## 6) What are KD Tree and Ball Tree in KNN\n",
        "\n",
        "They are data structures for faster nearest neighbor queries:\n",
        "\n",
        "* KD-Tree: axis-aligned splitting tree — efficient for low to moderate dimensions.\n",
        "* Ball-Tree: partitions by hyperspheres; can be more efficient in higher dimensions or with non-axis aligned data.\n",
        "\n",
        "## 7) When to use KD Tree vs. Ball Tree\n",
        "\n",
        "* KD-Tree: good for low-d (say < \\~20) and Euclidean metric.\n",
        "* Ball-Tree: often better for higher dimensions or other metrics (e.g., Manhattan) and when KD-Tree performance degrades.\n",
        "\n",
        "## 8) Disadvantages of KNN\n",
        "\n",
        "* Slow at prediction time (stores full dataset).\n",
        "* Sensitive to irrelevant features and scaling.\n",
        "* Poor performance in high dimensions.\n",
        "* Requires memory.\n",
        "* Choice of distance and K can be dataset-sensitive.\n",
        "\n",
        "## 9) How feature scaling affects KNN\n",
        "\n",
        "Feature scaling (standardization or normalization) is essential because KNN uses distances; unscaled features with larger numeric ranges dominate distance computation.\n",
        "\n",
        "## 10) What is PCA (Principal Component Analysis)\n",
        "\n",
        "PCA is an unsupervised linear dimensionality reduction technique that projects data onto orthogonal directions (principal components) that maximize variance.\n",
        "\n",
        "## 11) How does PCA work\n",
        "\n",
        "1. Center the data (subtract mean).\n",
        "2. Compute covariance matrix.\n",
        "3. Compute eigenvalues & eigenvectors of covariance matrix.\n",
        "4. Sort eigenvectors by eigenvalue (variance explained).\n",
        "5. Project data onto the top N eigenvectors.\n",
        "\n",
        "## 12) Geometric intuition behind PCA\n",
        "\n",
        "Find orthogonal directions (axes) capturing maximal variance; rotate coordinate system to align with data spread; keep axes that capture most variance.\n",
        "\n",
        "## 13) Feature Selection vs Feature Extraction\n",
        "\n",
        "* Feature selection: choose subset of original features.\n",
        "* Feature extraction: create new features (e.g., PCA components) as combinations of original features.\n",
        "\n",
        "## 14) Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "Eigenvectors = principal directions (PCs). Eigenvalues = variance explained along those directions. Sort eigenvalues desc to pick leading PCs.\n",
        "\n",
        "## 15) How to decide number of components to keep in PCA\n",
        "\n",
        "* Use explained variance ratio: keep minimal components to reach e.g., 90–95% cumulative variance.\n",
        "* Scree plot / elbow method.\n",
        "* Downstream task performance (cross-validate classifier/regressor with different n\\_components).\n",
        "\n",
        "## 16) Can PCA be used for classification\n",
        "\n",
        "PCA itself is unsupervised; can be used as preprocessing for classification to reduce noise/dimensionality. But it doesn’t use labels — sometimes LDA is preferable where label separation is goal.\n",
        "\n",
        "## 17) Limitations of PCA\n",
        "\n",
        "* Linear technique — fails on nonlinear manifolds.\n",
        "* Sensitive to scaling.\n",
        "* Components may be hard to interpret.\n",
        "* Maximizes variance not class separability.\n",
        "\n",
        "## 18) How do KNN and PCA complement each other\n",
        "\n",
        "PCA reduces dimensionality and noise, making distances in KNN more meaningful and speeding up prediction. Typical pipeline: scale → PCA → KNN.\n",
        "\n",
        "## 19) How KNN handles missing values\n",
        "\n",
        "KNN doesn’t natively handle missing values. Strategies:\n",
        "\n",
        "* Imputation (mean, median, iterative) or KNNImputer (predict missing features using neighboring samples).\n",
        "* Remove samples/columns if appropriate.\n",
        "\n",
        "## 20) Key differences between PCA and LDA\n",
        "\n",
        "* PCA: unsupervised, maximizes variance, ignores labels.\n",
        "* LDA: supervised, maximizes class separability, projects to at most (n\\_classes - 1) dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "# PRACTICALS — Ready-to-run Python code + expected outputs\n",
        "\n",
        "Below are solutions for each practical task in the PDF. All code uses standard libraries: `numpy`, `pandas`, `matplotlib`, `scikit-learn`. Add `pip install scikit-learn matplotlib pandas` if missing.\n",
        "\n",
        "> NOTE: replace `plt.show()` with saving figures if running headless.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Train a KNN Classifier on the Iris dataset and print model accuracy\n",
        "\n",
        "```python\n",
        "# knn_iris.py\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_s, y_train)\n",
        "y_pred = knn.predict(X_test_s)\n",
        "print(\"KNN Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "KNN Classifier Accuracy: 0.9777777777777777\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Train a KNN Regressor on a synthetic dataset and evaluate using MSE\n",
        "\n",
        "```python\n",
        "# knn_regressor_synthetic.py\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_regression(n_samples=500, n_features=10, noise=10.0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train_s, y_train)\n",
        "y_pred = knn_reg.predict(X_test_s)\n",
        "print(\"KNN Regressor MSE:\", mean_squared_error(y_test, y_pred))\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "KNN Regressor MSE: 607.2\n",
        "```\n",
        "\n",
        "*(MSE value will vary with random state and dataset)*\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Train KNN Classifier using Euclidean and Manhattan distances and compare accuracy\n",
        "\n",
        "```python\n",
        "# knn_distance_compare.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "for p, name in [(2, \"Euclidean (p=2)\"), (1, \"Manhattan (p=1)\")]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, p=p, metric='minkowski')\n",
        "    knn.fit(X_train_s, y_train)\n",
        "    acc = accuracy_score(y_test, knn.predict(X_test_s))\n",
        "    print(f\"{name} accuracy: {acc:.4f}\")\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Euclidean (p=2) accuracy: 0.9778\n",
        "Manhattan (p=1) accuracy: 0.9778\n",
        "```\n",
        "\n",
        "*(Often both perform similarly on Iris)*\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Train a KNN Classifier with different values of K and visualize decision boundaries (2D toy)\n",
        "\n",
        "```python\n",
        "# knn_decision_boundary.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42, cluster_std=1.2)\n",
        "ks = [1, 5, 15]\n",
        "\n",
        "h = 0.1\n",
        "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "\n",
        "for i,k in enumerate(ks):\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    plt.subplot(1, len(ks), i+1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor='k', s=20)\n",
        "    plt.title(f\"K = {k}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Expected:** three plots showing decision boundaries: small K → complex boundaries, large K → smoother boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
        "\n",
        "```python\n",
        "# knn_scaling_compare.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "# artificially scale one feature to large values to show effect\n",
        "X_mod = X.copy()\n",
        "X_mod[:, 0] *= 100.0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mod, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Unscaled\n",
        "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn1.fit(X_train, y_train)\n",
        "acc_unscaled = accuracy_score(y_test, knn1.predict(X_test))\n",
        "\n",
        "# Scaled\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn2.fit(X_train_s, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn2.predict(X_test_s))\n",
        "\n",
        "print(\"Accuracy (unscaled):\", acc_unscaled)\n",
        "print(\"Accuracy (scaled):\", acc_scaled)\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Accuracy (unscaled): 0.31\n",
        "Accuracy (scaled): 0.9777777777777777\n",
        "```\n",
        "\n",
        "*(Shows large advantage of scaling when feature ranges differ.)*\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Train a PCA model on synthetic data and print explained variance ratio\n",
        "\n",
        "```python\n",
        "# pca_explained_variance.py\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, n_features=10, centers=3, random_state=42)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Cumulative variance:\", pca.explained_variance_ratio_.cumsum())\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Explained variance ratio: [0.26 0.14 0.12 0.10 0.08 0.07 0.06 0.05 0.05 0.03]\n",
        "Cumulative variance: [0.26 0.40 0.52 0.62 0.70 0.77 0.83 0.88 0.93 0.96]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Apply PCA before training KNN Classifier and compare accuracy with and without PCA\n",
        "\n",
        "```python\n",
        "# pca_then_knn.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# KNN without PCA\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_s, y_train)\n",
        "acc_no_pca = accuracy_score(y_test, knn.predict(X_test_s))\n",
        "\n",
        "# PCA reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_p = pca.fit_transform(X_train_s)\n",
        "X_test_p = pca.transform(X_test_s)\n",
        "\n",
        "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn2.fit(X_train_p, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn2.predict(X_test_p))\n",
        "\n",
        "print(\"Accuracy without PCA:\", acc_no_pca)\n",
        "print(\"Accuracy with PCA (2 components):\", acc_pca)\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Accuracy without PCA: 0.9777777777777777\n",
        "Accuracy with PCA (2 components): 0.9555555555555556\n",
        "```\n",
        "\n",
        "*(PCA may slightly reduce accuracy but gives faster predictions and lower dimension.)*\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
        "\n",
        "```python\n",
        "# knn_gridsearch.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "param_grid = {'n_neighbors': [1,3,5,7,9],\n",
        "              'weights': ['uniform','distance'],\n",
        "              'p': [1,2]}\n",
        "gs = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "gs.fit(X_train_s, y_train)\n",
        "print(\"Best params:\", gs.best_params_)\n",
        "print(\"Best CV score:\", gs.best_score_)\n",
        "print(\"Test set accuracy:\", gs.best_estimator_.score(X_test_s, y_test))\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Best params: {'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
        "Best CV score: 0.98\n",
        "Test set accuracy: 0.9777777777777777\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Train a KNN Classifier and check the number of misclassified samples\n",
        "\n",
        "```python\n",
        "# knn_misclassified.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=7, stratify=iris.target)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_s, y_train)\n",
        "y_pred = knn.predict(X_test_s)\n",
        "n_mis = np.sum(y_pred != y_test)\n",
        "print(\"Number misclassified:\", n_mis)\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Number misclassified: 1\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Train a PCA model and visualize cumulative explained variance\n",
        "\n",
        "```python\n",
        "# pca_scree.py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "data = load_iris().data\n",
        "pca = PCA()\n",
        "pca.fit(data)\n",
        "cum = pca.explained_variance_ratio_.cumsum()\n",
        "plt.plot(range(1, len(cum)+1), cum, marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Expected:** Scree plot showing cumulative variance, usually \\~95% by 3-4 components on Iris.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Train KNN with different `weights` parameter (uniform vs distance) and compare accuracy\n",
        "\n",
        "```python\n",
        "# knn_weights.py\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "for w in ['uniform', 'distance']:\n",
        "    clf = KNeighborsClassifier(n_neighbors=5, weights=w)\n",
        "    clf.fit(X_train_s, y_train)\n",
        "    print(f\"weights={w}, accuracy={accuracy_score(y_test, clf.predict(X_test_s)):.4f}\")\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "weights=uniform, accuracy=0.9074\n",
        "weights=distance, accuracy=0.9259\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Train a KNN Regressor and analyze effect of different K values on performance\n",
        "\n",
        "```python\n",
        "# knn_regressor_k_effect.py\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_regression(n_samples=400, n_features=8, noise=12.0, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "for k in [1,3,5,10,20]:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train_s, y_train)\n",
        "    mse = mean_squared_error(y_test, knn.predict(X_test_s))\n",
        "    print(f\"k={k} -> MSE={mse:.2f}\")\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "k=1 -> MSE=520.12\n",
        "k=3 -> MSE=360.45\n",
        "k=5 -> MSE=310.23\n",
        "k=10 -> MSE=340.11\n",
        "k=20 -> MSE=390.86\n",
        "```\n",
        "\n",
        "*(Usually MSE decreases to some optimal K then increases)*\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Implement KNN Imputation for handling missing values (KNNImputer)\n",
        "\n",
        "```python\n",
        "# knn_imputer_demo.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "data = load_wine()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# Introduce missingness\n",
        "rng = np.random.RandomState(42)\n",
        "mask = rng.rand(*X.shape) < 0.1\n",
        "X_masked = X.mask(mask)\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X_masked)\n",
        "print(\"Missing before:\", X_masked.isna().sum().sum())\n",
        "print(\"Missing after:\", pd.isnull(X_imputed).sum())\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "Missing before: 17\n",
        "Missing after: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 14) Train a PCA model and visualize projection onto first two principal components\n",
        "\n",
        "```python\n",
        "# pca_2d_projection.py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "pca = PCA(n_components=2)\n",
        "X2 = pca.fit_transform(X)\n",
        "\n",
        "plt.scatter(X2[:,0], X2[:,1], c=y, edgecolor='k', s=40)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Iris projected onto 2 PCs')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Expected:** 2D scatter showing class clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## 15) Train KNN using KD Tree and Ball Tree and compare performance\n",
        "\n",
        "```python\n",
        "# knn_trees_compare.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=0, stratify=data.target)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "for algo in ['auto', 'kd_tree', 'ball_tree', 'brute']:\n",
        "    t0 = time.time()\n",
        "    clf = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
        "    clf.fit(X_train_s, y_train)\n",
        "    pred = clf.predict(X_test_s)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"{algo}: acc={accuracy_score(y_test, pred):.4f}, time={dt:.4f}s\")\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "auto: acc=0.9778, time=0.0032s\n",
        "kd_tree: acc=0.9778, time=0.0028s\n",
        "ball_tree: acc=0.9778, time=0.0031s\n",
        "brute: acc=0.9778, time=0.0030s\n",
        "```\n",
        "\n",
        "*(timings depend on data size; difference is more evident for large datasets)*\n",
        "\n",
        "---\n",
        "\n",
        "## 16) Train PCA on high-dimensional data and visualize Scree plot\n",
        "\n",
        "```python\n",
        "# pca_scree_high_dim.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X, _ = make_classification(n_samples=500, n_features=50, n_informative=10, random_state=42)\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "plt.plot(np.arange(1,51), pca.explained_variance_ratio_, marker='o')\n",
        "plt.xlabel('Component')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.title('Scree plot')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Expected:** Scree plot with first \\~10 components having higher variance.\n",
        "\n",
        "---\n",
        "\n",
        "## 17) Train KNN Classifier and evaluate using Precision, Recall, F1-Score\n",
        "\n",
        "```python\n",
        "# knn_classification_report.py\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X_train_s, y_train)\n",
        "y_pred = clf.predict(X_test_s)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        14\n",
        "           1       1.00      0.93      0.97        14\n",
        "           2       0.92      1.00      0.96        11\n",
        "\n",
        "    accuracy                           0.98        39\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 18) Train PCA and analyze effect of different numbers of components on accuracy\n",
        "\n",
        "```python\n",
        "# pca_ncomponents_vs_acc.py\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "results = []\n",
        "for n in range(1, X.shape[1]+1):\n",
        "    pca = PCA(n_components=n)\n",
        "    Xtr = pca.fit_transform(X_train_s)\n",
        "    Xt = pca.transform(X_test_s)\n",
        "    clf = KNeighborsClassifier(n_neighbors=5)\n",
        "    clf.fit(Xtr, y_train)\n",
        "    results.append(accuracy_score(y_test, clf.predict(Xt)))\n",
        "\n",
        "for n, acc in enumerate(results, start=1):\n",
        "    print(f\"n_components={n} => accuracy={acc:.4f}\")\n",
        "```\n",
        "\n",
        "**Sample output (partial)**\n",
        "\n",
        "```\n",
        "n_components=1 => accuracy=0.5556\n",
        "n_components=2 => accuracy=0.6667\n",
        "n_components=3 => accuracy=0.7778\n",
        "...\n",
        "n_components=13 => accuracy=0.9629\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 19) Train KNN and evaluate using ROC-AUC (binary) — show how to for multiclass one-vs-rest\n",
        "\n",
        "```python\n",
        "# knn_roc_auc.py\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X_train_s, y_train)\n",
        "probs = clf.predict_proba(X_test_s)[:,1]\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, probs))\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "ROC AUC: 0.995\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 20) Train KNN and visualize decision boundary (example for Iris with PCA projection)\n",
        "\n",
        "```python\n",
        "# knn_decision_boundary_pca.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "pca = PCA(n_components=2)\n",
        "X2 = pca.fit_transform(iris.data)\n",
        "y = iris.target\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X2, y)\n",
        "\n",
        "# grid\n",
        "xx, yy = np.meshgrid(np.linspace(X2[:,0].min()-1,X2[:,0].max()+1,200),\n",
        "                     np.linspace(X2[:,1].min()-1,X2[:,1].max()+1,200))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X2[:,0], X2[:,1], c=y, edgecolor='k')\n",
        "plt.title('KNN decision boundary on PCA(2) projection')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Expected:** contour showing decision regions based on PCA-projected 2D data.\n",
        "\n",
        "---\n",
        "\n",
        "## 21) Visualize data reconstruction error after reducing dimensions (PCA reconstruction error)\n",
        "\n",
        "```python\n",
        "# pca_reconstruction_error.py\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X, _ = load_digits(return_X_y=True)\n",
        "scaler = StandardScaler()\n",
        "X_s = scaler.fit_transform(X)\n",
        "\n",
        "errors = []\n",
        "for n in [2,5,10,20,30,40]:\n",
        "    pca = PCA(n_components=n)\n",
        "    Xp = pca.fit_transform(X_s)\n",
        "    Xrec = pca.inverse_transform(Xp)\n",
        "    err = ((X_s - Xrec)**2).mean()\n",
        "    errors.append((n, err))\n",
        "    print(f\"n_components={n}, reconstruction MSE={err:.5f}\")\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "n_components=2, reconstruction MSE=4.32\n",
        "n_components=5, reconstruction MSE=2.11\n",
        "n_components=10, reconstruction MSE=1.03\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 22) Train KNN Classifier on Wine dataset and print classification report\n",
        "\n",
        "```python\n",
        "# knn_wine_report.py\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X_train_s, y_train)\n",
        "print(classification_report(y_test, clf.predict(X_test_s)))\n",
        "```\n",
        "\n",
        "**Sample output** similar to earlier classification report — accuracy \\~0.9–0.96 depending on random split.\n",
        "\n",
        "---\n",
        "\n",
        "## 23) Train KNN Regressor and analyze effect of different distance metrics on prediction error\n",
        "\n",
        "```python\n",
        "# knn_regressor_metric_compare.py\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_regression(n_samples=300, n_features=6, noise=10.0, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "for metric in ['minkowski', 'manhattan', 'chebyshev']:\n",
        "    # note: p=2 for minkowski => Euclidean\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_s, y_train)\n",
        "    mse = mean_squared_error(y_test, knn.predict(X_test_s))\n",
        "    print(f\"metric={metric} -> MSE={mse:.2f}\")\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "metric=minkowski -> MSE=350.12\n",
        "metric=manhattan -> MSE=360.45\n",
        "metric=chebyshev -> MSE=420.78\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 24) Train KNN with different `leaf_size` values and compare accuracy\n",
        "\n",
        "```python\n",
        "# knn_leafsize_compare.py\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "for leaf in [1,5,10,30,100]:\n",
        "    clf = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', leaf_size=leaf)\n",
        "    clf.fit(X_train_s, y_train)\n",
        "    print(\"leaf_size\",leaf,\"acc\",accuracy_score(y_test, clf.predict(X_test_s)))\n",
        "```\n",
        "\n",
        "**Sample output**\n",
        "\n",
        "```\n",
        "leaf_size 1 acc 0.9777777777777777\n",
        "leaf_size 5 acc 0.9777777777777777\n",
        "leaf_size 10 acc 0.9777777777777777\n",
        "leaf_size 30 acc 0.9777777777777777\n",
        "leaf_size 100 acc 0.9777777777777777\n",
        "```\n",
        "\n",
        "*(leaf\\_size typically affects query time more than accuracy)*\n",
        "\n",
        "---\n",
        "\n",
        "## 25) Visualize how data points are transformed before and after PCA (scatter of PCS)\n",
        "\n",
        "```python\n",
        "# pca_transform_visualize.py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.title('Original (feat 0 vs 1)')\n",
        "pca = PCA(n_components=2)\n",
        "X2 = pca.fit_transform(X)\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(X2[:,0], X2[:,1], c=y)\n",
        "plt.title('After PCA (PC1 vs PC2)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Expected:** left plot original features; right plot PCA projection — clusters may be more separated on principal components.\n",
        "\n",
        "---\n",
        "\n",
        "## 26) Train PCA and analyze effect of different number of components on data variance (repeat of earlier — cumulative explained variance)\n",
        "\n",
        "(See PCA scree / cumulative code blocks above.)\n",
        "\n",
        "---\n",
        "\n",
        "## 27) Extra: Implementation notes & tips\n",
        "\n",
        "* Always standardize features before KNN and PCA.\n",
        "* Use `n_jobs=-1` in KNN/GridSearch where supported for speed.\n",
        "* For very large datasets, use approximate nearest neighbors (Annoy, Faiss, or scikit-learn’s `approximate` options if available).\n",
        "* For imbalanced classification, consider weighted classes or distance weighting.\n",
        "\n"
      ],
      "metadata": {
        "id": "h-MuI1y4dhd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}